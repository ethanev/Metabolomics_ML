{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict\n",
    "import glob, os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.utils import shuffle\n",
    "pd.options.display.max_columns = 100\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feng\n"
     ]
    }
   ],
   "source": [
    "##### this commented out part is for playing around with group / individual data sets\n",
    "# multi_to_single = ['ST000284', 'ST000046', 'ST000045', 'ST000763', 'ST000329', 'MTBLS358', 'MTBLS352']\n",
    "# multi_to_single = ['Feng']\n",
    "# datasets = OrderedDict()\n",
    "# for fn in sorted(glob.glob('./pickles/*.pkl')):\n",
    "# #     print(fn)\n",
    "#     if fn[10:-8] in multi_to_single:\n",
    "#         data = pd.read_pickle(open(fn,'rb'))\n",
    "#         print(data[0]['study'])\n",
    "#         datasets[data[0]['study']] = data\n",
    "\n",
    "#### for general processing:\n",
    "datasets = OrderedDict()\n",
    "for fn in sorted(glob.glob('./pickles/*.pkl')):\n",
    "    data = pd.read_pickle(open(fn,'rb'))\n",
    "    datasets[data[0]['study']] = data\n",
    "    \n",
    "pre_norm_ds = [ 'plasmaall_author',\n",
    "                'urineall_author',\n",
    "                'm_oxylipin_chronic_hep_b',\n",
    "                'm_chronic_hep_b_POS',\n",
    "                'm_chronic_hep_b_NEG',\n",
    "                'm_CER_mass_spectrometry_v4',\n",
    "                'm_CER_mass_spectrometry_v4_3_CS',\n",
    "                'm_CER_mass_spectrometry_v4_0_NS',\n",
    "                'm_CER_mass_spectrometry_v4_2_FS',\n",
    "                'm_CER_mass_spectrometry_v4_1_COPD',\n",
    "                'm_EICO_mass_spectrometry_v4',\n",
    "                'm_EICO_mass_spectrometry_v4_3_CS',\n",
    "                'm_EICO_mass_spectrometry_v4_0_NS',\n",
    "                'm_EICO_mass_spectrometry_v4_2_FS',\n",
    "                'm_EICO_mass_spectrometry_v4_1_COPD',\n",
    "                'AN000580',\n",
    "                'AN000581',\n",
    "                'AN001503',\n",
    "                'ulsam_author']\n",
    "\n",
    "def check_pre_norm(ds):\n",
    "    if ds['data_set'] in pre_norm_ds:\n",
    "        ds['pre_norm'] = 'Yes'\n",
    "    else:\n",
    "        ds['pre_norm'] = 'No'\n",
    "    return ds\n",
    "\n",
    "for k, v in datasets.items():\n",
    "    for ds in v:\n",
    "        ds = check_pre_norm(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_nan_to_val(data, value=0):\n",
    "    data[pd.isnull(data)] = value\n",
    "    return data\n",
    "\n",
    "# percentile normalization - Sean / Claire's method - code copied, thanks you two!\n",
    "def percentile_normalization(X, control_indices, all_indices):\n",
    "    norm_x = np.array(\n",
    "        [[sp.percentileofscore(X[control_indices, i], X[j, i], kind='mean') for j in all_indices] for i in range(X.shape[1])]\n",
    "        ).T\n",
    "    return norm_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define functions to perform the batch normalization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_trans_ds_aware(ds, X):\n",
    "    X[np.isinf(X)] = 0\n",
    "    X[X<0] = 0\n",
    "    if ds['pre_norm'] == 'Yes':\n",
    "        X = convert_nan_to_val(X, value=0)\n",
    "    else:\n",
    "        X = convert_nan_to_val(X, value=1)\n",
    "        X[X<1] = 1\n",
    "        X = np.log2(X)\n",
    "    return X\n",
    "        \n",
    "def no_batch_ds_normalization(ds):\n",
    "    y = ds['labels'].values.copy()\n",
    "    X = ds['features'].values.copy()\n",
    "    x_labels = list(ds['features'].index)\n",
    "    control = [i for i in range(y.shape[0]) if (y[i]==0 or y[i]==False)]\n",
    "    case = [i for i in range(y.shape[0]) if (y[i]!=0 or y[i]==True)]\n",
    "    all_ind = [i for i in range(y.shape[0])]\n",
    "    X_random = np.random.uniform(0.0,10**-9,size=(X.shape[0],X.shape[1]))\n",
    "    X = log_trans_ds_aware(ds, X)\n",
    "    X = X+X_random\n",
    "    X = percentile_normalization(X, control, all_ind)\n",
    "    ds['features'] = pd.DataFrame(X, index=x_labels)\n",
    "    return ds\n",
    "\n",
    "def batch_data(ds,k):\n",
    "    '''\n",
    "    return the batches specific to the 'batch effected' datasets \n",
    "    '''\n",
    "    # want to return two lists, first is a list of X matricies for individual batches, second is y matricies for individual batches\n",
    "    X = ds['features'].values.copy()\n",
    "    y = ds['labels'].values.copy()\n",
    "    x_labels = list(ds['features'].index)\n",
    "    y_labels = list(ds['labels'].index)\n",
    "    x_batch_labels = []\n",
    "    y_batch_labels = []\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    \n",
    "    if k == 'Feng':\n",
    "        batches = list(ds['features'].index)\n",
    "        if batches[0][-6:] == '.mzXML':\n",
    "            batches = [name[:-6] for name in batches if name[-6:]=='.mzXML' ]\n",
    "        batches = [int(ele[-1]) for ele in batches]\n",
    "    \n",
    "    if k == 'MTBLS17':\n",
    "        batches = list(ds['features'].index)\n",
    "        batches = [int(ele[3]) for ele in batches]\n",
    "        \n",
    "    if k == 'MTBLS19':\n",
    "        batches = list(ds['features'].index)\n",
    "        batches = [int(ele[3]) for ele in batches]\n",
    "        \n",
    "    if k == 'MTBLS72':\n",
    "        g1 = ['X20120829_Howard_NEG_129.mzML', 'X20120829_Howard_NEG_130.mzML', 'X20120829_Howard_NEG_131.mzML', 'X20120829_Howard_NEG_132.mzML', 'X20120829_Howard_NEG_134.mzML', 'X20120829_Howard_NEG_135.mzML', 'X20120829_Howard_NEG_137.mzML', 'X20120829_Howard_NEG_142.mzML', 'X20120829_Howard_NEG_158.mzML', 'X20120829_Howard_NEG_160.mzML', 'X20120829_Howard_NEG_163.mzML', 'X20120829_Howard_NEG_164.mzML', 'X20120829_Howard_NEG_165.mzML', 'X20120829_Howard_NEG_166.mzML', 'X20120829_Howard_NEG_167.mzML', 'X20120829_Howard_NEG_170.mzML', 'X20120829_Howard_NEG_173.mzML', 'X20120829_Howard_NEG_174.mzML', 'X20120829_Howard_NEG_175.mzML', 'X20120829_Howard_NEG_177.mzML', 'X20120829_Howard_NEG_179.mzML', 'X20120829_Howard_NEG_180.mzML', 'X20120829_Howard_NEG_181.mzML', 'X20120829_Howard_NEG_133.mzML', 'X20120829_Howard_NEG_136.mzML', 'X20120829_Howard_NEG_138.mzML', 'X20120829_Howard_NEG_140.mzML', 'X20120829_Howard_NEG_141.mzML', 'X20120829_Howard_NEG_143.mzML', 'X20120829_Howard_NEG_144.mzML', 'X20120829_Howard_NEG_146.mzML', 'X20120829_Howard_NEG_147.mzML', 'X20120829_Howard_NEG_148.mzML', 'X20120829_Howard_NEG_149.mzML', 'X20120829_Howard_NEG_151.mzML', 'X20120829_Howard_NEG_152.mzML', 'X20120829_Howard_NEG_153.mzML', 'X20120829_Howard_NEG_159.mzML', 'X20120829_Howard_NEG_168.mzML', 'X20120829_Howard_NEG_169.mzML', 'X20120829_Howard_NEG_176.mzML', 'X20120829_Howard_NEG_178.mzML', 'X20120829_Howard_NEG_182.mzML']\n",
    "        g2 = ['X20120829_Howard_NEG_07.mzML', 'X20120829_Howard_NEG_08.mzML', 'X20120829_Howard_NEG_09.mzML', 'X20120829_Howard_NEG_10.mzML', 'X20120829_Howard_NEG_11.mzML', 'X20120829_Howard_NEG_12.mzML', 'X20120829_Howard_NEG_13.mzML', 'X20120829_Howard_NEG_14.mzML', 'X20120829_Howard_NEG_15.mzML', 'X20120829_Howard_NEG_16.mzML', 'X20120829_Howard_NEG_18.mzML', 'X20120829_Howard_NEG_19.mzML', 'X20120829_Howard_NEG_20.mzML', 'X20120829_Howard_NEG_21.mzML', 'X20120829_Howard_NEG_22.mzML', 'X20120829_Howard_NEG_23.mzML', 'X20120829_Howard_NEG_24.mzML', 'X20120829_Howard_NEG_25.mzML', 'X20120829_Howard_NEG_26.mzML', 'X20120829_Howard_NEG_27.mzML', 'X20120829_Howard_NEG_29.mzML', 'X20120829_Howard_NEG_30.mzML', 'X20120829_Howard_NEG_31.mzML', 'X20120829_Howard_NEG_32.mzML', 'X20120829_Howard_NEG_33.mzML', 'X20120829_Howard_NEG_34.mzML', 'X20120829_Howard_NEG_35.mzML', 'X20120829_Howard_NEG_36.mzML', 'X20120829_Howard_NEG_37.mzML', 'X20120829_Howard_NEG_38.mzML', 'X20120829_Howard_NEG_40.mzML', 'X20120829_Howard_NEG_41.mzML', 'X20120829_Howard_NEG_42.mzML', 'X20120829_Howard_NEG_43.mzML', 'X20120829_Howard_NEG_44.mzML', 'X20120829_Howard_NEG_45.mzML', 'X20120829_Howard_NEG_46.mzML', 'X20120829_Howard_NEG_47.mzML', 'X20120829_Howard_NEG_48.mzML', 'X20120829_Howard_NEG_49.mzML', 'X20120829_Howard_NEG_51.mzML', 'X20120829_Howard_NEG_52.mzML', 'X20120829_Howard_NEG_53.mzML', 'X20120829_Howard_NEG_54.mzML', 'X20120829_Howard_NEG_55.mzML', 'X20120829_Howard_NEG_56.mzML', 'X20120829_Howard_NEG_57.mzML', 'X20120829_Howard_NEG_58.mzML', 'X20120829_Howard_NEG_59.mzML', 'X20120829_Howard_NEG_60.mzML']\n",
    "        g3 = ['X20120829_Howard_NEG_253.mzML', 'X20120829_Howard_NEG_254.mzML', 'X20120829_Howard_NEG_258.mzML', 'X20120829_Howard_NEG_259.mzML', 'X20120829_Howard_NEG_264.mzML', 'X20120829_Howard_NEG_267.mzML', 'X20120829_Howard_NEG_270.mzML', 'X20120829_Howard_NEG_271.mzML', 'X20120829_Howard_NEG_273.mzML', 'X20120829_Howard_NEG_274.mzML', 'X20120829_Howard_NEG_275.mzML', 'X20120829_Howard_NEG_277.mzML', 'X20120829_Howard_NEG_278.mzML', 'X20120829_Howard_NEG_284.mzML', 'X20120829_Howard_NEG_285.mzML', 'X20120829_Howard_NEG_286.mzML', 'X20120829_Howard_NEG_289.mzML', 'X20120829_Howard_NEG_295.mzML', 'X20120829_Howard_NEG_297.mzML', 'X20120829_Howard_NEG_299.mzML', 'X20120829_Howard_NEG_302.mzML', 'X20120829_Howard_NEG_251.mzML', 'X20120829_Howard_NEG_255.mzML', 'X20120829_Howard_NEG_256.mzML', 'X20120829_Howard_NEG_257.mzML', 'X20120829_Howard_NEG_265.mzML', 'X20120829_Howard_NEG_276.mzML', 'X20120829_Howard_NEG_279.mzML', 'X20120829_Howard_NEG_281.mzML', 'X20120829_Howard_NEG_282.mzML', 'X20120829_Howard_NEG_292.mzML', 'X20120829_Howard_NEG_296.mzML', 'X20120829_Howard_NEG_298.mzML', 'X20120829_Howard_NEG_300.mzML']   \n",
    "        groups = [g1,g2,g3]\n",
    "\n",
    "        labels = list(ds['features'].index)\n",
    "        batches = []\n",
    "        if labels[0][17:20] == 'POS':\n",
    "            for i, g in enumerate(groups):\n",
    "                groups[i] = [ele.replace('NEG','POS') for ele in g]\n",
    "        for f in labels:\n",
    "            for i,g in enumerate(groups):\n",
    "                if f in g:\n",
    "                    batches.append(i+1)                    \n",
    "                    \n",
    "    if k == 'MTBLS92':\n",
    "        batch = list(ds['features'].index)\n",
    "        if batch[0][0] == 'X':\n",
    "            batches = [int(strin[1])-1 for strin in batch]\n",
    "        else:\n",
    "            batches = [strin[0] for strin in batch]\n",
    "            mapper = {'A':0, 'B':1}\n",
    "            batches = [mapper[s] for s in batches]\n",
    "\n",
    "    if k == 'MTBLS105':\n",
    "        batches = ds['samples']['Factor Value[Batch]'].values\n",
    "        \n",
    "    if k == 'MTBLS146':\n",
    "        names = ds['samples'].set_index('Raw Spectral Data File')['Factor Value[Batch number]']\n",
    "        name_order = list(ds['features'].index)\n",
    "        names = names.loc[name_order]\n",
    "        batches = names.values\n",
    "        \n",
    "    if k == 'MTBLS404':\n",
    "        names = list(ds['features'].index)\n",
    "        batches = []\n",
    "        for n in names:\n",
    "            if 'b2' in n:\n",
    "                batches.append(1)\n",
    "            else: \n",
    "                batches.append(0)\n",
    "                \n",
    "    if k == 'ST000063':\n",
    "        batches = list(ds['labels'].index)\n",
    "        batches = [int(i[-1]) for i in batches]\n",
    "        \n",
    "    if k == 'ST000062':\n",
    "        batches = list(ds['labels'].index)\n",
    "        batches = [int(i[-1]) for i in batches]\n",
    "            \n",
    "    if k == 'ST000763':\n",
    "        names = list(ds['features'].index)\n",
    "        batches = pd.read_csv('batches.csv').set_index('SAMPLE_ID')\n",
    "        batches = batches.loc[names].values.flatten()\n",
    "   \n",
    "    if k == 'ST000865':\n",
    "        batches_ = list(ds['features'].index)\n",
    "        batches_ = [int(ele.split('_')[2]) for ele in batches_]\n",
    "        batches = []\n",
    "        for ele in batches_:\n",
    "            if ele > 87:\n",
    "                batches.append(1)\n",
    "            else:\n",
    "                batches.append(0) \n",
    "                \n",
    "    if k == 'ST000385':\n",
    "        batches = [0 if ele[2] == '3' else 1 for ele in x_labels]\n",
    "\n",
    "    # now break up the X and Y datasets based on the batch info which is all in list form!\n",
    "    set_batches = set(batches)\n",
    "    batches = np.asarray(batches)\n",
    "    for b in set_batches:\n",
    "        mask = batches==b\n",
    "        x_batches.append(X[batches==b])\n",
    "        x_batch_labels.append([x_labels[i] for i in range(len(x_labels)) if mask[i]])\n",
    "        y_batches.append(y[batches==b])  \n",
    "        y_batch_labels.append([y_labels[i] for i in range(len(y_labels)) if mask[i]])\n",
    "    single_x_batch_labels = [y for x in x_batch_labels for y in x]\n",
    "    single_y_batch_labels = [y for x in x_batch_labels for y in x]\n",
    "    return x_batches, y_batches, single_x_batch_labels, single_y_batch_labels \n",
    "\n",
    "def batch_ds_normalization(ds,k):\n",
    "    '''\n",
    "    need to extract the different batches, then on the batches get the indicies for each of the labels \n",
    "    issue: some datasets have multiple batches...so want to loop over batches to apply batch effect\n",
    "    '''\n",
    "    batch_x, batch_y, x_batch_labels, y_batch_labels = batch_data(ds, k) # these are lists of X and Ys for each of the batches\n",
    "    norm_x = []\n",
    "    reform_y = []\n",
    "    for X, y in zip(batch_x, batch_y):\n",
    "        control_ind = [i for i in range(y.shape[0]) if (y[i]==0 or y[i]==False)]\n",
    "        all_ind  = [i for i in range(y.shape[0])]\n",
    "        X_random = np.random.uniform(0.0,10**-7,size=(X.shape[0],X.shape[1]))\n",
    "        X = log_trans_ds_aware(ds, X)\n",
    "        X = X+X_random\n",
    "        # give the BN the X data JUST for that batch, the indicies that are cases and all the indicies\n",
    "        X = percentile_normalization(X,control_ind, all_ind)\n",
    "        norm_x.append(X)\n",
    "        reform_y.append(y)\n",
    "    full_X = np.concatenate(norm_x, axis=0)\n",
    "    full_y = np.concatenate(reform_y, axis=0)\n",
    "    ds['features'] = pd.DataFrame(full_X, index=x_batch_labels)\n",
    "    ds['labels'] = pd.DataFrame(full_y, index=y_batch_labels)    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell will perform the percentile normalization on all of the datasets and save!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feng\n",
      "batch - 2 part Feng plasmaall_author\n",
      "batch - 2 part Feng urineall_author\n",
      "no batch, single batch serum_IPO_aligned_Feng_serum_batch1 Feng\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in less\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no batch, single batch serum_IPO_aligned_Feng_serum_batch2 Feng\n",
      "no batch, single batch urine_IPO_aligned_Feng_urine_batch1 Feng\n",
      "no batch, single batch urine_IPO_aligned_Feng_urine_batch2 Feng\n",
      "batch - 2 part Feng serum_onebatch_IPO_aligned_Feng_serum_all_MSMS\n",
      "batch - 2 part Feng urine_onebatch_IPO_aligned_Feng_urine_all_MSMS\n"
     ]
    }
   ],
   "source": [
    "true_batch = ['MTBLS72', 'MTBLS92', 'MTBLS105', 'MTBLS146', 'MTBLS404', 'ST000063', 'ST000062','ST000763']\n",
    "part_batch = ['MTBLS17', 'MTBLS19', 'Feng', 'ST000865', 'ST000385']\n",
    "skip_ds = ['MTBLS148','MTBLS200', 'MTBLS20', 'ST000397', 'MTBLS264'] # no real labels here so cant do this...unless its all controls...\n",
    "for k, v in datasets.items():\n",
    "    new_combined_ds = []\n",
    "    print(k)\n",
    "    for ds in v:\n",
    "        if k in skip_ds:\n",
    "            new_combined_ds.append(ds)\n",
    "            continue\n",
    "        #### Now try the normalization \n",
    "        if k in true_batch:\n",
    "            print('batch - all batch', k, ds['data_set'])\n",
    "            ds = batch_ds_normalization(ds, k)\n",
    "        elif k in part_batch and ('onebatch' in ds['data_set'] or 'all' in ds['data_set']):\n",
    "            print('batch - 2 part', k, ds['data_set'])\n",
    "            ds = batch_ds_normalization(ds, k)\n",
    "        else:\n",
    "            print('no batch, single batch', ds['data_set'], k)\n",
    "            ds = no_batch_ds_normalization(ds)\n",
    "        new_combined_ds.append(ds)\n",
    "    pickle.dump(new_combined_ds, open('./bn_pickles/{}.pkl'.format(k), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
