{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elements in the code are inspired by Michael Murphy - Thanks!\n",
    "import glob, re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.multiclass import type_of_target # used to check the Y labels are appropriate for classification\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy import interp\n",
    "from scipy.stats import kruskal, mannwhitneyu\n",
    "pd.set_option('display.max_rows', 500) \n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = True # use the percentile normalized data? All of the paper does\n",
    "log = False\n",
    "stand_scaler = False\n",
    "model = 'log_reg' #log_reg, rf, svm or plsda\n",
    "remove_sig = True\n",
    "stat_sig = False\n",
    "top_sig = False\n",
    "top_sig_num = 0\n",
    "\n",
    "pre_norm_ds = [ 'plasmaall_author',\n",
    "                'urineall_author',\n",
    "                'm_oxylipin_chronic_hep_b',\n",
    "                'm_chronic_hep_b_POS',\n",
    "                'm_chronic_hep_b_NEG',\n",
    "                'm_CER_mass_spectrometry_v4',\n",
    "                'm_CER_mass_spectrometry_v4_3_CS',\n",
    "                'm_CER_mass_spectrometry_v4_0_NS',\n",
    "                'm_CER_mass_spectrometry_v4_2_FS',\n",
    "                'm_CER_mass_spectrometry_v4_1_COPD',\n",
    "                'm_EICO_mass_spectrometry_v4',\n",
    "                'm_EICO_mass_spectrometry_v4_3_CS',\n",
    "                'm_EICO_mass_spectrometry_v4_0_NS',\n",
    "                'm_EICO_mass_spectrometry_v4_2_FS',\n",
    "                'm_EICO_mass_spectrometry_v4_1_COPD',\n",
    "                'AN000580',\n",
    "                'AN000581',\n",
    "                'AN001503',\n",
    "                'ulsam_author']\n",
    "if bn:\n",
    "    path = './bn_pickles_paper/*.pkl'\n",
    "else:\n",
    "    path = './pickles/*.pkl'\n",
    "\n",
    "datasets = OrderedDict()\n",
    "for fn in sorted(glob.glob(path)):\n",
    "    data = pd.read_pickle(open(fn,'rb'), compression=None)\n",
    "    datasets[data[0]['study']] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_labels(ds):\n",
    "    ds['labels'] = ds['labels']*1\n",
    "    vals = ds['labels'].values\n",
    "    try:\n",
    "        vals = [item for sublist in vals for item in sublist]\n",
    "    except:\n",
    "        pass\n",
    "    labels = set(vals)\n",
    "    ds['num_labels'] = len(labels)\n",
    "    ds['label_set'] = labels\n",
    "    return ds\n",
    "\n",
    "def check_pre_norm(ds):\n",
    "    if ds['data_set'] in pre_norm_ds:\n",
    "        ds['pre_norm'] = 'Yes'\n",
    "    else:\n",
    "        ds['pre_norm'] = 'No'\n",
    "    return ds\n",
    "\n",
    "def convert_nan_to_val(data, value=0):\n",
    "    data[pd.isnull(data)] = value\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdr_corrected_p(dataset, fill_nan=False, log=False):\n",
    "    X = dataset['features'].values.copy()\n",
    "    # tuning the data with nan filling to 0, changing 0s to a diff number and or applying log transform\n",
    "    X[np.isinf(X)] = 0\n",
    "    X[X<0] = 0\n",
    "    if fill_nan:\n",
    "        X = convert_nan_to_val(X, value=0)\n",
    "    if log and dataset['pre_norm'] == 'No':\n",
    "        X[X<1] = 1\n",
    "        X = np.log2(X)\n",
    "    \n",
    "    y = dataset['labels'].values.ravel()\n",
    "    y = np.asarray([int(i) for i in y])\n",
    "    p = np.zeros(X.shape[1]) + np.nan\n",
    "    \n",
    "    for i in range(X.shape[1]):\n",
    "        feat_data = []\n",
    "        for j in ds['label_set']:\n",
    "            try:\n",
    "                X_0 = X[y==j,i]\n",
    "                X_0 = X_0[~np.isnan(X_0)]\n",
    "                feat_data.append(X_0)\n",
    "            except:\n",
    "                pass \n",
    "        ### do MW-U or kruskal but first find places were vectors all the same. \n",
    "        if ds['num_labels'] == 2:\n",
    "            if set(feat_data[0]) == set(feat_data[1]):\n",
    "                p[i] = 1\n",
    "                continue\n",
    "            else:\n",
    "                _, p[i] = mannwhitneyu(feat_data[0],feat_data[1], alternative='two-sided')\n",
    "        else:\n",
    "            if set(feat_data[0]) == set(feat_data[1]) == set(feat_data[2]):\n",
    "                p[i] = 1\n",
    "                continue\n",
    "            else:\n",
    "                _, p[i] = kruskal(*feat_data)                       \n",
    "    try:\n",
    "        _, p[~np.isnan(p)], _, _ = multipletests(p[~np.isnan(p)], alpha=0.05, method='fdr_bh')\n",
    "    except:\n",
    "        pass\n",
    "    dataset['pvalues'] = p\n",
    "    dataset['significant'] = (dataset['pvalues'] < 0.05).sum() if (~np.isnan(p)).any() else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X,y,ds,model):\n",
    "    X,y = shuffle(X,y)\n",
    "    if model == 'log_reg':\n",
    "        if ds['num_labels'] != 2:\n",
    "            clf = LogisticRegressionCV(scoring='accuracy', penalty='l1', solver='liblinear', tol=1e-4, intercept_scaling=1, max_iter=500, multi_class='ovr')\n",
    "        else:\n",
    "            clf = LogisticRegressionCV(scoring='roc_auc', penalty='l1', solver='liblinear', tol=1e-4, intercept_scaling=1, max_iter=500)\n",
    "    elif model == 'rf':\n",
    "        clf = RandomForestClassifier(n_estimators=1000)\n",
    "    elif model == 'svm':\n",
    "        param_grid = {'gamma': [1e-3, 0.01, 0.1, 1], 'C': [0.01, 0.1, 1, 10, 100]}\n",
    "        clf = GridSearchCV(SVC(kernel='linear', probability=True), param_grid, cv=3)\n",
    "    elif model == 'plsda':\n",
    "        param_grid = {'n_components': [2,5,20,50,100]}\n",
    "        clf = GridSearchCV(PLSRegression(), param_grid, cv=3)\n",
    "    else:\n",
    "        print('no valid classifier input, please try again with one of: log_reg, rf or svm')\n",
    "        exit(0)\n",
    "        \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True) # so this will probably give rather high - at the end you just get the last model...\n",
    "    aucs = []\n",
    "    num_stat = []\n",
    "    coefs = np.zeros(X.shape[1])\n",
    "    for train, test in cv.split(X,y):\n",
    "        x_train, y_train = X[train], y[train]\n",
    "        x_test, y_test = X[test], y[test]\n",
    "        \n",
    "        if stat_sig:\n",
    "            p = np.zeros(x_train.shape[1]) + np.nan\n",
    "            for i in range(x_train.shape[1]):\n",
    "                feat_data = []\n",
    "                for j in set(y_train):\n",
    "                    try:\n",
    "                        X_0 = x_train[y_train==j,i]\n",
    "                        X_0 = X_0[~np.isnan(X_0)]\n",
    "                        feat_data.append(X_0)\n",
    "                    except:\n",
    "                        pass \n",
    "                if set(feat_data[0]) == set(feat_data[1]):\n",
    "                    p[i] = 1\n",
    "                    continue\n",
    "                else:\n",
    "                    _, p[i] = mannwhitneyu(feat_data[0],feat_data[1], alternative='two-sided')                     \n",
    "            try:\n",
    "                _, p[~np.isnan(p)], _, _ = multipletests(p[~np.isnan(p)], alpha=0.05, method='fdr_bh')\n",
    "            except:\n",
    "                pass\n",
    "            # get the top X 'most' significant \n",
    "            if top_sig:\n",
    "                p_copy = p.copy()\n",
    "                sorted_ps = sorted(list(p_copy))\n",
    "                top_ps = sorted_ps[:top_sig_num] \n",
    "                to_keep = []\n",
    "                for p_val in top_ps:\n",
    "                    new_ps = np.where(p==p_val)[0].tolist()\n",
    "                    dup_removed_ps = []\n",
    "                    for ele in new_ps:\n",
    "                        if ele not in to_keep:\n",
    "                            dup_removed_ps.append(ele)\n",
    "                    new_ps = dup_removed_ps\n",
    "                    if len(to_keep) + len(new_ps) > top_sig_num:\n",
    "                        oversum =  len(to_keep) + len(new_ps) - top_sig_num\n",
    "                        new_ps = random.sample(new_ps, len(new_ps)-oversum)\n",
    "                        to_keep += new_ps\n",
    "                    else:\n",
    "                        to_keep += new_ps\n",
    "                ### trying to set the non important values to be greater than 0.05...set to 1\n",
    "                n_p = np.ones(p.shape[0])\n",
    "                n_p[to_keep] = p[to_keep]\n",
    "                p = n_p\n",
    "                x_train = x_train[:,p<0.05]\n",
    "                x_test = x_test[:,p<0.05]\n",
    "            else:\n",
    "                x_train = x_train[:,p<0.05]\n",
    "                x_test = x_test[:,p<0.05]\n",
    "            num_stat.append(x_train.shape[1])\n",
    "        elif remove_sig:\n",
    "            p = ds['pvalues'][ds['pvalues']>=0.05]\n",
    "            num_stat.append(0)\n",
    "        else:\n",
    "            num_stat.append(ds['significant'])\n",
    "            p = ds['pvalues']\n",
    "            \n",
    "        if model == 'plsda':\n",
    "            # massage the y valeus into the format for pls-da\n",
    "            y_train_new = np.zeros((y_train.shape[0],2))\n",
    "            y_train_new[np.arange(y_train.shape[0]), y_train] = 1\n",
    "            y_train = y_train_new\n",
    "        if stand_scaler:\n",
    "            scaler = StandardScaler()\n",
    "            x_train = scaler.fit_transform(x_train)\n",
    "            x_test = scaler.transform(x_test)   \n",
    "        if x_train.shape[1] == 0:\n",
    "            aucs.append(0.5) \n",
    "            continue\n",
    "        try:\n",
    "            ### the try-except is just for the plsda models, which error when n_comp > # features\n",
    "            ### this then just trains a model with the max number of features\n",
    "            clf.fit(x_train, y_train)\n",
    "        except:\n",
    "            if model == 'plsda':\n",
    "                comp = x_train.shape[1]\n",
    "                clf = PLSRegression(n_components = comp)\n",
    "                clf.fit(x_train, y_train)\n",
    "            else:\n",
    "                print('erroring, cannot train model')\n",
    "                break\n",
    "        if model == 'plsda':\n",
    "            try:\n",
    "                ds['indiv_split_model_coefs'].append(clf.best_estimator_.coef_[:,0])\n",
    "            except:\n",
    "                ds['indiv_split_model_coefs'].append(clf.coef_[:,0])\n",
    "        else:\n",
    "            ds['indiv_split_model_coefs'].append(clf.coef_[0])\n",
    "        ds['indiv_split_p_vals'].append(p)\n",
    "    \n",
    "        if ds['num_labels'] != 2:\n",
    "            ### if problem is set up as a multiclass problem and you are doing one v the rest training or true multiclass predictions\n",
    "            if ovr_auc:\n",
    "                # to do one v the rest AUCs:\n",
    "                y_pred = clf.predict_proba(x_test)\n",
    "                num_labels = y_pred.shape[1]\n",
    "                set_to = num_labels+10\n",
    "                indiv_aucs = []\n",
    "                for ind in range(y_pred.shape[1]):\n",
    "                    y_mut = y_test.copy()\n",
    "                    y_mut[y_mut==ind] = set_to\n",
    "                    y_mut[y_mut!=set_to] = 0\n",
    "                    y_mut[y_mut==set_to] = 1\n",
    "                    fpr, tpr, _ = roc_curve(y_mut, y_pred[:,ind])\n",
    "                    auc_value = metrics.auc(fpr, tpr)\n",
    "                    indiv_aucs.append(auc_value)\n",
    "                aucs.append(indiv_aucs)\n",
    "            else: aucs.append(clf.score(x_test, y_test))\n",
    "        else:\n",
    "            if model == 'plsda':\n",
    "                y_pred = clf.predict(x_test)\n",
    "                y_pred = np.absolute(y_pred)\n",
    "                row_sum = np.repeat(y_pred.sum(axis=1),2).reshape((-1,2))\n",
    "                y_pred = np.divide(y_pred, row_sum)\n",
    "            else:\n",
    "                y_pred = clf.predict_proba(x_test)\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_pred[:,1])\n",
    "            auc_value = metrics.auc(fpr, tpr)\n",
    "            aucs.append(auc_value) \n",
    "    auc = np.asarray(aucs)\n",
    "    return auc.mean(), auc.std(), clf, y_train.shape, y_test.shape, np.asarray(num_stat).mean()\n",
    "\n",
    "def fit_model(X,y,ds,model):\n",
    "    mean, std, clf, train_size, test_size, avg_num_stat_feat =  train_model(X,y,ds,model)\n",
    "    return mean, std, train_size[0], test_size[0], clf, avg_num_stat_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:205: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is just to combine datasets and make a new dictionary. \n",
    "original_multi_class = ['ST000284', 'ST000046', 'ST000045', 'ST000763', 'ST000329', 'MTBLS358', 'MTBLS352']\n",
    "\n",
    "combinable_ds = {'MTBLS17':[['IPO_aligned_MTBLS17_neg_exp1', 'IPO_aligned_MTBLS17_pos_exp1'],\n",
    "                            ['IPO_aligned_MTBLS17_neg_exp2', 'IPO_aligned_MTBLS17_pos_exp2'],\n",
    "                            ['IPO_aligned_MTBLS17_neg_exp3', 'IPO_aligned_MTBLS17_pos_exp3'],\n",
    "                            ['IPO_aligned_MTBLS17_neg_onebatch','IPO_aligned_MTBLS17_pos_onebatch'],\n",
    "                            ['Peaklist_EXP1_POS','Peaklist_EXP1_NEG'],\n",
    "                            ['Peaklist_EXP2_POS','Peaklist_EXP2_NEG'],\n",
    "                            ['Peaklist_EXP3_POS','Peaklist_EXP3_NEG']],\n",
    "                 'MTBLS19': [['Exp1F_POS', 'Exp1F_NEG'], ['Exp2F_POS', 'Exp2F_NEG'], \n",
    "                             ['IPO_aligned_MTBLS19_neg_exp1_F', 'IPO_aligned_MTBLS19_pos_exp1_F'],\n",
    "                             ['IPO_aligned_MTBLS19_neg_exp2_F','IPO_aligned_MTBLS19_pos_exp2_F'],\n",
    "                             ['IPO_aligned_MTBLS19_neg_all_F_R', 'IPO_aligned_MTBLS19_pos_all_F_R']],\n",
    "                 'MTBLS19_data': [['Exp1F_POS', 'Exp1F_NEG'], ['Exp2F_POS', 'Exp2F_NEG'], \n",
    "                             ['IPO_aligned_MTBLS19_neg_exp1_F', 'IPO_aligned_MTBLS19_pos_exp1_F'],\n",
    "                             ['IPO_aligned_MTBLS19_neg_exp2_F','IPO_aligned_MTBLS19_pos_exp2_F'],\n",
    "                             ['IPO_aligned_MTBLS19_neg_all_F_R', 'IPO_aligned_MTBLS19_pos_all_F_R']],\n",
    "                 'MTBLS28': [['m_mtbls28_NEG_v2_maf', 'm_mtbls28_POS_v2_maf'],\n",
    "                             ['IPO_aligned_MTBLS28_neg', 'IPO_aligned_MTBLS28_pos']],\n",
    "                 'MTBLS72': [['IPO_aligned_MTBLS72_neg', 'IPO_aligned_MTBLS72_pos']],\n",
    "                 'MTBLS105': [['IPO_aligned_MTBLS105_qMS', 'IPO_aligned_MTBLS105_SIM-MS']],\n",
    "                 'MTBLS266': [['m_mtbls266_NEG_mass_spectrometry_v2_maf', 'm_mtbls266_POS_mass_spectrometry_v2_maf'],\n",
    "                              ['IPO_aligned_MTBLS266_neg', 'IPO_aligned_MTBLS266_pos']],\n",
    "                 'MTBLS315': [['m_GC_nmfi_and_bsi_diagnosis_v2_maf', 'm_LC_nmfi_and_bsi_diagnosis_v2_maf', \n",
    "                               'm_UPLC_NEG_nmfi_and_bsi_diagnosis_v2_maf', 'm_UPLC_POS_nmfi_and_bsi_diagnosis_v2_maf'],\n",
    "                              ['IPO_aligned_MTBLS315_mzData', 'IPO_aligned_MTBLS315_mzXML', \n",
    "                               'IPO_aligned_MTBLS315_n_mzML', 'IPO_aligned_MTBLS315_p_mzML']],\n",
    "                 'MTBLS354': [['m_cap_metabolite_profiling_mass_spectrometry_v2_maf', 'm_cap_metabolite_profiling_mass_spectrometry-1_v2_maf'],\n",
    "                              ['IPO_aligned_MTBLS354_neg', 'IPO_aligned_MTBLS354_pos']],\n",
    "                 'MTBLS364': [['IPO_aligned_MTBLS364_hil_neg', 'IPO_aligned_MTBLS364_hil_pos',\n",
    "                               'IPO_aligned_MTBLS364_lip_neg', 'IPO_aligned_MTBLS364_lip_pos']],\n",
    "                 'ST000045': [['02Feb10-21-r0_ND_II','11Feb10-21-r0_ND_II', '11March10-21-_ND_II','17March10-21-_ND_II'],\n",
    "                              ['02Feb10-21-r0_ND_IW', '11Feb10-21-r0_ND_IW', '11March10-21-_ND_IW', '17March10-21-_ND_IW'],\n",
    "                              ['02Feb10-21-r0_II_IW', '11Feb10-21-r0_II_IW', '11March10-21-_II_IW', '17March10-21-_II_IW'],\n",
    "                              ['IPO_aligned_ST000045_2feb_pos_ND_II', 'IPO_aligned_ST000045_11feb_neg_ND_II',\n",
    "                               'IPO_aligned_ST000045_11mar_pos_ND_II', 'IPO_aligned_ST000045_17mar_neg_ND_II'],\n",
    "                              ['IPO_aligned_ST000045_2feb_pos_ND_IW', 'IPO_aligned_ST000045_11feb_neg_ND_IW',\n",
    "                               'IPO_aligned_ST000045_11mar_pos_ND_IW', 'IPO_aligned_ST000045_17mar_neg_ND_IW'],\n",
    "                              ['IPO_aligned_ST000045_2feb_pos_II_IW', 'IPO_aligned_ST000045_11feb_neg_II_IW',\n",
    "                               'IPO_aligned_ST000045_11mar_pos_II_IW', 'IPO_aligned_ST000045_17mar_neg_II_IW']],\n",
    "                 'ST000329': [['AN000525_MCD_FSGS','AN000526_MCD_FSGS'],\n",
    "                              ['AN000525_MCD_Control','AN000526_MCD_Control'],\n",
    "                              ['AN000525_FSGS_Control', 'AN000526_FSGS_Control'],\n",
    "                              ['IPO_aligned_ST000329_pos_MCD_FSGS', 'IPO_aligned_ST000329_neg_MCD_FSGS'],\n",
    "                              ['IPO_aligned_ST000329_pos_MCD_Control', 'IPO_aligned_ST000329_neg_MCD_Control'],\n",
    "                              ['IPO_aligned_ST000329_pos_FSGS_Control', 'IPO_aligned_ST000329_neg_FSGS_Control']],\n",
    "                 'ST000385': [['AN000603_plasma', 'AN000603_serum'],\n",
    "                              ['AN000620_plasma', 'AN000620_serum'],\n",
    "                              ['IPO_aligned_ST000385_adc2_plasma', 'IPO_aligned_ST000385_adc2_serum'],\n",
    "                              ['IPO_aligned_ST000385_adc1_plasma', 'IPO_aligned_ST000385_adc1_serum'],\n",
    "                              ['IPO_aligned_ST000385_onebatch_plasma','IPO_aligned_ST000385_onebatch_serum']],\n",
    "                 'ST000392': [['AN000628_plasma', 'AN000628_serum'], \n",
    "                              ['IPO_aligned_ST000392_plasma', 'IPO_aligned_ST000392_serum']],\n",
    "                 'ST000578': [['AN000888', 'AN000889'],\n",
    "                              ['IPO_aligned_ST000578_AE', 'IPO_aligned_ST000578_C18']],\n",
    "                 'ST000763': [['AN001201_Healthy_PAH','AN001202_Healthy_PAH'],\n",
    "                              ['AN001201_Healthy_Normal Pressures', 'AN001202_Healthy_Normal Pressures'],\n",
    "                              ['AN001201_Healthy_Borderline Pressures','AN001202_Healthy_Borderline Pressures'],\n",
    "                              ['AN001201_Healthy_LowRisk','AN001202_Healthy_LowRisk'],\n",
    "                              ['AN001201_PAH_Normal Pressures','AN001202_PAH_Normal Pressures'],\n",
    "                              ['AN001201_PAH_Borderline Pressures','AN001202_PAH_Borderline Pressures'],\n",
    "                              ['AN001201_PAH_LowRisk','AN001202_PAH_LowRisk'],\n",
    "                              ['AN001201_Normal Pressures_Borderline Pressures','AN001202_Normal Pressures_Borderline Pressures'],\n",
    "                              ['AN001201_Normal Pressures_LowRisk','AN001202_Normal Pressures_LowRisk'],\n",
    "                              ['AN001201_Borderline Pressures_LowRisk','AN001202_Borderline Pressures_LowRisk'],\n",
    "                              ['IPO_aligned_ST000763_untar_pos_Healthy_PAH','IPO_aligned_ST000763_untar_neg_Healthy_PAH'],\n",
    "                              ['IPO_aligned_ST000763_untar_pos_Healthy_Normal Pressures','IPO_aligned_ST000763_untar_neg_Healthy_Normal Pressures'],\n",
    "                              ['IPO_aligned_ST000763_untar_pos_Healthy_Borderline Pressures','IPO_aligned_ST000763_untar_neg_Healthy_Borderline Pressures'],\n",
    "                              ['IPO_aligned_ST000763_untar_pos_Healthy_LowRisk','IPO_aligned_ST000763_untar_neg_Healthy_LowRisk'],\n",
    "                              ['IPO_aligned_ST000763_untar_pos_PAH_Normal Pressures','IPO_aligned_ST000763_untar_neg_PAH_Normal Pressures'],\n",
    "                              ['IPO_aligned_ST000763_untar_pos_PAH_Borderline Pressures','IPO_aligned_ST000763_untar_neg_PAH_Borderline Pressures'],\n",
    "                              ['IPO_aligned_ST000763_untar_pos_PAH_LowRisk','IPO_aligned_ST000763_untar_neg_PAH_LowRisk'],\n",
    "                              ['IPO_aligned_ST000763_untar_pos_Normal Pressures_Borderline Pressures','IPO_aligned_ST000763_untar_neg_Normal Pressures_Borderline Pressures'],\n",
    "                              ['IPO_aligned_ST000763_untar_pos_Normal Pressures_LowRisk','IPO_aligned_ST000763_untar_neg_Normal Pressures_LowRisk'],\n",
    "                              ['IPO_aligned_ST000763_untar_pos_Borderline Pressures_LowRisk','IPO_aligned_ST000763_untar_neg_Borderline Pressures_LowRisk']],\n",
    "                 'ST000046': [['AN000076_CN_MCI', 'AN000077_CN_MCI', 'AN000078_CN_MCI','AN000079_CN_MCI'],\n",
    "                              ['AN000076_CN_AD', 'AN000077_CN_AD', 'AN000078_CN_AD', 'AN000079_CN_AD'],\n",
    "                              ['AN000076_MCI_AD','AN000077_MCI_AD','AN000078_MCI_AD','AN000079_MCI_AD'],\n",
    "                              ['IPO_aligned_ST000046_20120606_neg_hilic_CN_MCI','IPO_aligned_ST000046_20120618_pos_c18_CN_MCI',\n",
    "                               'IPO_aligned_ST000046_20120620_neg_c18_CN_MCI','XCMS-Report-annotated-SingleClass.04jun12_CN_MCI'],\n",
    "                              ['IPO_aligned_ST000046_20120606_neg_hilic_CN_AD','IPO_aligned_ST000046_20120618_pos_c18_CN_AD',\n",
    "                               'IPO_aligned_ST000046_20120620_neg_c18_CN_AD','XCMS-Report-annotated-SingleClass.04jun12_CN_AD'],\n",
    "                              ['IPO_aligned_ST000046_20120606_neg_hilic_MCI_AD','IPO_aligned_ST000046_20120618_pos_c18_MCI_AD',\n",
    "                               'IPO_aligned_ST000046_20120620_neg_c18_MCI_AD','XCMS-Report-annotated-SingleClass.04jun12_MCI_AD'],\n",
    "                              ['IPO_aligned_ST000046_20120613_neg_hilic_CN_MCI','IPO_aligned_ST000046_20120625_pos_c18_CN_MCI',\n",
    "                               'XCMS-Report-annotated-SingleClass.11jun12_CN_MCI','XCMS-Report-annotated-SingleClass.27jun12_CN_MCI'],\n",
    "                              ['IPO_aligned_ST000046_20120613_neg_hilic_CN_AD','IPO_aligned_ST000046_20120625_pos_c18_CN_AD',\n",
    "                               'XCMS-Report-annotated-SingleClass.11jun12_CN_AD','XCMS-Report-annotated-SingleClass.27jun12_CN_AD'],\n",
    "                              ['IPO_aligned_ST000046_20120613_neg_hilic_MCI_AD','IPO_aligned_ST000046_20120625_pos_c18_MCI_AD',\n",
    "                               'XCMS-Report-annotated-SingleClass.11jun12_MCI_AD','XCMS-Report-annotated-SingleClass.27jun12_MCI_AD']],\n",
    "                 'MTBLS408': [['IPO_aligned_MTBLS408_neg', 'IPO_aligned_MTBLS408_pos']],\n",
    "                 'MTBLS352': [['DEMO_neg-norm-metaboAnalystInput_T2D_NGT', 'DEMO_pos-norm-metaboAnalystInput_T2D_NGT'],\n",
    "                              ['DEMO_neg-norm-metaboAnalystInput_T2D_Pre-DM', 'DEMO_pos-norm-metaboAnalystInput_T2D_Pre-DM'],\n",
    "                              ['DEMO_neg-norm-metaboAnalystInput_NGT_Pre-DM', 'DEMO_pos-norm-metaboAnalystInput_NGT_Pre-DM']],\n",
    "                 'MTBLS358': [['m_CER_mass_spectrometry_v4_COPD_FS', 'm_EICO_mass_spectrometry_v4_COPD_FS',\n",
    "                               'm_SHOT_mass_spectrometry_v4_COPD_FS', 'm_TAG_mass_spectrometry_v4_COPD_FS'],\n",
    "                              ['m_CER_mass_spectrometry_v4_COPD_CS', 'm_EICO_mass_spectrometry_v4_COPD_CS',\n",
    "                               'm_SHOT_mass_spectrometry_v4_COPD_CS', 'm_TAG_mass_spectrometry_v4_COPD_CS'],\n",
    "                              ['m_CER_mass_spectrometry_v4_COPD_NS', 'm_EICO_mass_spectrometry_v4_COPD_NS',\n",
    "                               'm_SHOT_mass_spectrometry_v4_COPD_NS', 'm_TAG_mass_spectrometry_v4_COPD_NS'],\n",
    "                              ['m_CER_mass_spectrometry_v4_FS_CS', 'm_EICO_mass_spectrometry_v4_FS_CS',\n",
    "                               'm_SHOT_mass_spectrometry_v4_FS_CS', 'm_TAG_mass_spectrometry_v4_FS_CS'],\n",
    "                              ['m_CER_mass_spectrometry_v4_FS_NS', 'm_EICO_mass_spectrometry_v4_FS_NS',\n",
    "                               'm_SHOT_mass_spectrometry_v4_FS_NS', 'm_TAG_mass_spectrometry_v4_FS_NS'],\n",
    "                              ['m_CER_mass_spectrometry_v4_CS_NS', 'm_EICO_mass_spectrometry_v4_CS_NS',\n",
    "                               'm_SHOT_mass_spectrometry_v4_CS_NS', 'm_TAG_mass_spectrometry_v4_CS_NS']],\n",
    "                 'MTBLS279': [['m_chronic_hep_b_POS', 'm_chronic_hep_b_NEG']],\n",
    "                 'ST000608': [['AN000929', 'AN000930', 'AN000931']],\n",
    "                 'ST000450': [['AN000705', 'AN000706']],\n",
    "                 'ST000356': [['AN000582', 'AN000583']],\n",
    "                 'ST000355': [['AN000580', 'AN000581']]}\n",
    "\n",
    "# cannot do on Feng or ST000381; MTBLS148, 264, 665 not included in datasets(?), ST000421, 726\n",
    "#['peaks', 'data_set', 'study', 'labels', 'disease', 'samples', 'features', 'pre_norm']\n",
    "\n",
    "combined_ds = {}\n",
    "ds_names = []\n",
    "for k, v in datasets.items():\n",
    "    for ds in v:\n",
    "        ds_names.append(ds['data_set'])\n",
    "        \n",
    "for k, v in datasets.items(): \n",
    "    try:\n",
    "        to_combine = combinable_ds[k]\n",
    "    except:\n",
    "        continue\n",
    "    combined_ds[k] = []\n",
    "    for combo in to_combine:\n",
    "        if combo[0] not in ds_names:\n",
    "            continue\n",
    "        aucs = []\n",
    "        combined_feat = []\n",
    "        combined_peaks = []\n",
    "        combined_samples = []\n",
    "        combined_labels = []\n",
    "        combined_feat_names = []\n",
    "        study = k  \n",
    "        disease = v[0]['disease']\n",
    "        a_or_r = 'reprocessed' if 'XCMS' in combo[0] or 'IPO' in combo[0] else 'author'\n",
    "        combined_name = a_or_r + '_' + combo[0]\n",
    "        for ds_name in combo:\n",
    "            for ds in v:\n",
    "                if ds['data_set'] == ds_name:\n",
    "                    combined_feat.append(ds['features'].values)\n",
    "                    combined_feat_names.append(list(ds['features'].index))\n",
    "                    combined_peaks.append(ds['peaks'])\n",
    "                    combined_samples.append(ds['samples'])\n",
    "                    combined_labels.append(ds['labels'])\n",
    "        combined_feat = np.hstack(tuple(combined_feat))\n",
    "        combined_feat = pd.DataFrame(combined_feat, index=combined_feat_names[0])\n",
    "        combined_samples = pd.concat(combined_samples, axis=1)\n",
    "        ds = {'peaks': combined_peaks,\n",
    "              'data_set': combined_name,\n",
    "              'study': k,\n",
    "              'labels':combined_labels[0],\n",
    "              'disease': disease,\n",
    "              'samples': combined_samples,\n",
    "              'features': combined_feat,\n",
    "              'single_ds_aucs':aucs}\n",
    "        combined_ds[k].append(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in csv file which will be used to sort the indicies of the datasets and labels\n",
    "ds388_9_combine = pd.read_csv('./ST000388_9_combined.csv')\n",
    "\n",
    "for k, v in datasets.items(): \n",
    "    if k not in ['ST000388', 'ST000389']:\n",
    "        continue\n",
    "    for ds in v:\n",
    "        data_name = ds['features'].index\n",
    "        for data in ['lc', 'gc', 'min_lc']:\n",
    "            not_shared = []\n",
    "            for ele in ds388_9_combine[data]:\n",
    "                if ele not in data_name:\n",
    "                    not_shared.append(ele)\n",
    "            if len(not_shared) < 50:\n",
    "                for ele in not_shared:\n",
    "                    ds388_9_combine = ds388_9_combine[ds388_9_combine[data] != ele]\n",
    "                    \n",
    "for k, v in datasets.items(): \n",
    "    if k not in ['ST000388', 'ST000389']:\n",
    "        continue\n",
    "    for ds in v:\n",
    "        for ele in ['lc', 'gc', 'min_lc']:\n",
    "            try:\n",
    "                ds['features'] = ds['features'].loc[ds388_9_combine[ele]]\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "for k, v in datasets.items(): \n",
    "    if k not in ['ST000388', 'ST000389']:\n",
    "        continue\n",
    "    for ds in v:\n",
    "        ind_names = list(ds['features'].index)\n",
    "        if ds['data_set'] == 'IPO_aligned_ST000388_LC':   \n",
    "            ind_names = list(ds388_9_combine['min_lc'])\n",
    "#             ind_names = [ele.split('_')[0]+'_'+ele.split('_')[-1][:-5] for ele in ind_names]\n",
    "        ds['labels'] = ds['labels'].loc[ind_names]\n",
    "# ok now i need to go through and combine the author data and the ipo data\n",
    "rep_ds = {'data_set': 'reprocessed_ST000388',\n",
    "          'study': 'ST000388'}\n",
    "auth_ds = {'data_set': 'author_ST000388',\n",
    "          'study': 'ST000388'}\n",
    "rep_feat = []\n",
    "rep_feat_names = []\n",
    "auth_feat = []\n",
    "auth_feat_names = []\n",
    "\n",
    "for k, v in datasets.items(): \n",
    "    if k not in ['ST000388', 'ST000389']:\n",
    "        continue\n",
    "    for ds in v:\n",
    "        if 'IPO' in ds['data_set']:\n",
    "            rep_ds['disease'] = ds['disease']\n",
    "            rep_ds['samples'] = ds['samples']\n",
    "            rep_ds['labels'] = ds['labels']\n",
    "            rep_ds['peaks'] = ds['peaks']\n",
    "            rep_feat.append(ds['features'].values)\n",
    "            rep_feat_names.append(list(ds['features'].index))\n",
    "        else:\n",
    "            auth_ds['disease'] = ds['disease']\n",
    "            auth_ds['samples'] = ds['samples']\n",
    "            auth_ds['labels'] = ds['labels']\n",
    "            auth_ds['peaks'] = ds['peaks']\n",
    "            auth_feat.append(ds['features'].values)\n",
    "            auth_feat_names.append(list(ds['features'].index))\n",
    "# for combined_feat, comb_ds, inds in zip([rep_feat, auth_feat], [rep_ds,auth_ds], [rep_feat_names, auth_feat_names]):\n",
    "for combined_feat, comb_ds, inds in zip([rep_feat], [rep_ds], [rep_feat_names]):\n",
    "    combined_feat = np.hstack(tuple(combined_feat))\n",
    "    combined_feat = pd.DataFrame(combined_feat, index=inds[0])\n",
    "    comb_ds['features'] = combined_feat\n",
    "            \n",
    "combined_ds['ST000388'] = []\n",
    "combined_ds['ST000388'].append(rep_ds)\n",
    "# combined_ds['ST000388'].append(auth_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTBLS105 reprocessed_IPO_aligned_MTBLS105_qMS\n",
      "MTBLS17 reprocessed_IPO_aligned_MTBLS17_neg_onebatch\n",
      "MTBLS19_data reprocessed_IPO_aligned_MTBLS19_neg_all_F_R\n",
      "MTBLS266 reprocessed_IPO_aligned_MTBLS266_neg\n",
      "MTBLS279 author_m_chronic_hep_b_POS\n",
      "MTBLS28 reprocessed_IPO_aligned_MTBLS28_neg\n",
      "MTBLS315 reprocessed_IPO_aligned_MTBLS315_mzData\n",
      "MTBLS354 reprocessed_IPO_aligned_MTBLS354_neg\n",
      "MTBLS358 author_m_CER_mass_spectrometry_v4_COPD_FS\n",
      "MTBLS358 author_m_CER_mass_spectrometry_v4_COPD_CS\n",
      "MTBLS358 author_m_CER_mass_spectrometry_v4_COPD_NS\n",
      "MTBLS358 author_m_CER_mass_spectrometry_v4_FS_CS\n",
      "MTBLS358 author_m_CER_mass_spectrometry_v4_FS_NS\n",
      "MTBLS358 author_m_CER_mass_spectrometry_v4_CS_NS\n",
      "MTBLS364 reprocessed_IPO_aligned_MTBLS364_hil_neg\n",
      "MTBLS408 reprocessed_IPO_aligned_MTBLS408_neg\n",
      "MTBLS72 reprocessed_IPO_aligned_MTBLS72_neg\n",
      "ST000045 reprocessed_IPO_aligned_ST000045_2feb_pos_ND_II\n"
     ]
    }
   ],
   "source": [
    "for k, v in combined_ds.items():\n",
    "    for ds in v:\n",
    "        ds = get_num_labels(ds)\n",
    "        ds = check_pre_norm(ds)\n",
    "        print(k, ds['data_set'])\n",
    "        if log:\n",
    "            fdr_corrected_p(ds, fill_nan=True, log=True)  \n",
    "        else:\n",
    "            fdr_corrected_p(ds, fill_nan=True, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if remove_sig:\n",
    "    for k, v in combined_ds.items():  \n",
    "        for ds in v: \n",
    "            ds['features'] = ds['features'].iloc[:,ds['pvalues']>=0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_indiv_ps_coefs(ds):\n",
    "    if stat_sig:\n",
    "        if ds['indiv_split_model_coefs'] != []:\n",
    "            combined_coefs = []\n",
    "            for coefs, p in zip(ds['indiv_split_model_coefs'],ds['indiv_split_p_vals']):\n",
    "                to_combine_coefs = np.zeros(p.shape[0])\n",
    "                to_combine_coefs[p<0.05] = coefs\n",
    "                combined_coefs.append(to_combine_coefs)\n",
    "            combined_coefs = np.asarray(combined_coefs)\n",
    "            combined_coefs = combined_coefs.mean(axis=0)\n",
    "        else:\n",
    "            combined_coefs = np.zeros(ds['indiv_split_p_vals'][0].shape[0])\n",
    "        ps = np.asarray(ds['indiv_split_p_vals'])\n",
    "        combined_ps = ps.mean(axis=0)\n",
    "    else:\n",
    "        combined_coefs = []\n",
    "        if remove_sig:\n",
    "            for coefs in ds['indiv_split_model_coefs']:\n",
    "                to_combine_coefs = np.zeros(ds['pvalues'].shape[0])\n",
    "                to_combine_coefs[ds['pvalues']>=0.05] = coefs\n",
    "                combined_coefs.append(coefs)\n",
    "            combined_coefs = np.asarray(combined_coefs)\n",
    "            combined_coefs = combined_coefs.mean(axis=0)\n",
    "            combined_ps = ds['pvalues']\n",
    "        else:\n",
    "            for coefs in ds['indiv_split_model_coefs']:\n",
    "                combined_coefs.append(coefs)\n",
    "            combined_coefs = np.asarray(combined_coefs)\n",
    "            combined_coefs = combined_coefs.mean(axis=0)\n",
    "            combined_ps = ds['pvalues']\n",
    "    return combined_coefs, combined_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "for k, v in combined_ds.items(): \n",
    "    for ds in v: \n",
    "        ds['indiv_split_model_coefs'] = []\n",
    "        ds['indiv_split_p_vals'] = []\n",
    "        print(k, ds['data_set'], ds['features'].shape)\n",
    "        ovr_auc = True\n",
    "        ds = get_num_labels(ds)\n",
    "        ds = check_pre_norm(ds)                \n",
    "        y = ds['labels'].values.copy().ravel().astype(int)\n",
    "        X = ds['features'].values.copy()\n",
    "        if X.shape[1] == 0:\n",
    "            ds['train_size'], ds['test_size'], ds['clf'] = 'na','na','na'\n",
    "            ds['auc'] = 0.5\n",
    "            ds['auc_std'] = 0\n",
    "        X = convert_nan_to_val(X, value=0)\n",
    "        X[np.isinf(X)] = 0\n",
    "        X[X<0] = 0\n",
    "        if log and ds['pre_norm'] == 'No':\n",
    "            X[X<1] = 1\n",
    "            X = np.log2(X)\n",
    "        aucs = []\n",
    "        model_feat = []\n",
    "        avg_stat_sig = []\n",
    "        for i in range(30):\n",
    "            auc, std,train_size,test_size,clf,avg_num_stat_feat = fit_model(X,y,ds,model)\n",
    "            aucs.append(auc)\n",
    "            avg_stat_sig.append(avg_num_stat_feat)\n",
    "        aucs = np.asarray(aucs)\n",
    "        if ds['indiv_split_model_coefs'] == [] and ds['indiv_split_p_vals'] == []:\n",
    "            pass\n",
    "        else:\n",
    "            ds['indiv_split_model_coefs'], ds['indiv_split_p_vals'] = avg_indiv_ps_coefs(ds)\n",
    "        ds['auc'] = aucs.mean()\n",
    "        ds['auc_std'] = aucs.std()\n",
    "        ds['avg_stat_sig'] = np.asarray(avg_stat_sig).mean()\n",
    "        ds['train_size'], ds['test_size'], ds['clf'] = train_size, test_size, clf\n",
    "        print(ds['auc'],ds['auc_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "disease_type = {\n",
    "    'acute myocardial infarction': 'cardiovascular', \n",
    "    'cardiovascular': 'cardiovascular',\n",
    "    'coronary heart disease': 'cardiovascular',\n",
    "    'hepatocellular carcinoma': 'cancer',\n",
    "    'Hepatocellular carcinoma': 'cancer',\n",
    "    'Hepatocellular Carcinoma': 'cancer',\n",
    "    'hepatitis b': 'infectious',\n",
    "    'Malaria': 'infectious',\n",
    "    'Malaria (P. vivax)':'infectious',\n",
    "    'non-malaria febrile illness':'infectious',\n",
    "    'scleroderma PAH': 'autoimmune',\n",
    "    'psoriasis':'autoimmune',\n",
    "    'pneumonia': 'infectious',\n",
    "    'Pneumonia - Community acquired': 'infectious',\n",
    "    'copd': 'respiratory',\n",
    "    'COPD': 'respiratory',\n",
    "    'chronic hepatitis B' : 'infectious',\n",
    "    'typhoid': 'infectious',\n",
    "    'typhoid carriage':'infectious',\n",
    "    'lyme': 'infectious',\n",
    "    'common cold - longitudinal':'infectious',\n",
    "    'Lyme disease': 'infectious',\n",
    "    'Alzheimers': 'neurological',\n",
    "    \"Alzheimer's\": 'neurological',\n",
    "    'colorectal cancer': 'cancer',\n",
    "    'Colorectal Cancer': 'cancer',\n",
    "    'depression': 'neurological',\n",
    "    'Depression':'neurological',\n",
    "    'Breast Cancer': 'cancer',\n",
    "    'Breast cancer':'cancer',\n",
    "    'Lung cancer': 'cancer',\n",
    "    'lung cancer': 'cancer',\n",
    "    'Lung Cancer': 'cancer',\n",
    "    'lung cancer - adenocarcinoma': 'cancer',\n",
    "    'lung cancer - non-small-cell lung cancer (adenocarcinoma, etc)': 'cancer',\n",
    "    'Stability of dried blood samples - diabetic men' : 'metabolic',\n",
    "    'Obesity - Non-diabetic and T2 diabetic': 'metabolic',\n",
    "    't2 diabetes': 'metabolic',\n",
    "    't1 diabetes': 'metabolic',\n",
    "    'Diabetes - Type I': 'metabolic',\n",
    "    'Diabetes - healthy v. T2 v. prediabetic': 'metabolic',\n",
    "    'Polycystic Ovarian Syndrome': 'metabolic',\n",
    "    'minimal change disease, focal segmental sclerosis': 'glomerular',\n",
    "    'interstitial cystitis/painful bladder syndrome': 'other',\n",
    "    'prepubertal children with obesity': 'other', #MAYBE CHANGE THIS ONE?\n",
    "    'chronic fatigue syndrome': 'other',\n",
    "    'Chronic fatigue': 'other',\n",
    "    'polycystic ovarian syndrome': 'other',\n",
    "    'scleroderma': 'other',\n",
    "    'Pregnancy': 'other',\n",
    "    'smoker v. nonsmoker':'other',\n",
    "    'Interperson variation':'other',\n",
    "    'short-term and long-term metabolic changes after bariatric surgery':'other',\n",
    "    'high intensity exercise metabolomics':'other',\n",
    "    'Age related metabolomics': 'other',\n",
    "    'Urine sample storage': 'other',\n",
    "    'urine metabolome': 'other',\n",
    "    'Single human time study': 'other'\n",
    "    }\n",
    "\n",
    "def make_summary(u,i,k,j=0):\n",
    "    auc = u['auc']\n",
    "    auc_std = u['auc_std']\n",
    "    analysis = u['data_set']\n",
    "    label = str(l)+str(i)\n",
    "    if u['clf'] == 'na' or u['clf'] == '0 features no model':\n",
    "        model_coef = 0\n",
    "#         if remove_sig == True:\n",
    "#             try:\n",
    "#                 model_coef = np.count_nonzero(u['clf'].coef_)\n",
    "#             except:\n",
    "#                 model_coef = 0\n",
    "#         else:\n",
    "#             if model == 'log_reg':\n",
    "#                 model_coef = np.count_nonzero(u['clf'].coef_)\n",
    "    else:\n",
    "        cutoff = 5e-4\n",
    "        if model == 'plsda':\n",
    "            try:\n",
    "                model_coef = u['clf'].best_estimator_.coef_[:,0]\n",
    "                model_coef = model_coef[np.absolute(model_coef)>cutoff].shape[0]\n",
    "            except:\n",
    "                try:\n",
    "                    model_coef = u['clf'].coef_[:,0]\n",
    "                    model_coef = model_coef[np.absolute(model_coef)>cutoff].shape[0]\n",
    "                except:\n",
    "                    model_coef = 'cannot tell'\n",
    "        else:\n",
    "            try:\n",
    "                model_coef = np.count_nonzero(u['clf'].coef_[j])\n",
    "            except:\n",
    "                try:\n",
    "                    model_coef = np.count_nonzero(u['clf'].best_estimator_.feature_importances_)\n",
    "                except:\n",
    "                    model_coef = 'cannot tell'\n",
    "    s = {'disease': u['disease'], \n",
    "        'number_labels': 2,\n",
    "        'auc':auc,\n",
    "        'auc_std': auc_std,\n",
    "        'samples': u['features'].shape[0],\n",
    "        'model_nonzero_coef': model_coef,\n",
    "        'significant': u['significant'],\n",
    "        'avg_stat_sig_per_model': u['avg_stat_sig'],\n",
    "        'features': u['features'].shape[1],\n",
    "        'train_size': u['train_size'],\n",
    "        'test_size': u['test_size'],\n",
    "        'label': label,\n",
    "        'case': summed_case,\n",
    "        'control': summed_control,\n",
    "        'analysis': analysis,\n",
    "        'disease_type': disease_type[u['disease']],\n",
    "        'study': k}\n",
    "    return s\n",
    "    \n",
    "\n",
    "summary = []\n",
    "for k in combined_ds:\n",
    "    for i, u in enumerate(combined_ds[k]):\n",
    "        if (k == 'ST000062' and u['data_set'] == 'XCMS-Report-annotated-SingleClass-GCTOF.'):\n",
    "            u['data_set'] = 'XCMS-Report-annotated-SingleClass-GCTOF.plasma'\n",
    "        if u['num_labels'] == 2:\n",
    "            control = u['labels']==0\n",
    "            case = u['labels']==1\n",
    "            try:\n",
    "                summed_control = int(control.sum())\n",
    "                summed_case = int(case.sum())\n",
    "            except:\n",
    "                pass\n",
    "            summary.append(make_summary(u,i,k))\n",
    "        else:\n",
    "            for j in range(u['num_labels']):\n",
    "                summary.append(make_summary(u,i,k,j=j,replace=True))                    \n",
    "summary = pd.DataFrame(summary)\n",
    "# summary = summary.set_index('study')\n",
    "# summary['disease_type'] = summary['disease_type'].astype('category')\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the df as a csv:\n",
    "summary.to_csv('./combined_summary-sig_rem_{}_stat_sig_{}_top_sig_{}_{}_30avg_auc_{}_YES_bn_NO_log_NO_standscal_YES_ovo.csv'.format(remove_sig,stat_sig, top_sig, top_sig_num, model))\n",
    "# save dataset object:\n",
    "pickle.dump(combined_ds, open('./combined_data_models-sig_rem_{}_stat_sig_{}_top_sig_{}_{}_30avg_{}_YES_bn_NO_log_NO_standscal_YES_ovo.pkl'.format(remove_sig,stat_sig, top_sig, top_sig_num, model), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the extra metadata onto this data (column, mode, sample type)\n",
    "# metadata = pd.read_csv('./ms_instrument_column_polarity_dataset_names.csv', sep='\\t')\n",
    "metadata = pd.read_csv('./ms_instrument_column_polarity_dataset_names.csv', sep=',').set_index('Accession')\n",
    "summary_w_metadata = summary.merge(metadata, on='analysis')\n",
    "summary_w_metadata = summary_w_metadata.replace(np.nan,'unknown')\n",
    "summary_w_metadata.to_csv('./combined_summary-sig_rem_{}_stat_sig_{}_top_sig_{}_{}_30avg_auc_{}_YES_bn_NO_log_NO_standscal_YES_ovo_YES_meta.csv'.format(remove_sig,stat_sig, top_sig, top_sig_num, model))\n",
    "summary_w_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
