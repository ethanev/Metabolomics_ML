{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import pickle\n",
    "import glob, re\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "root = '/home/ee/Documents/MIT/alm_lab/Revo_healthcare/data/study_data'\n",
    "multi_to_single = False\n",
    "one_to_one = True\n",
    "\n",
    "def get_labels(df, file_label, label):\n",
    "    '''\n",
    "    input: \n",
    "        df: pandas dataframe to ake the labels df out of\n",
    "        file_label: column in the original df you want to to be the INDICIES for the new df\n",
    "        labael: column in the original df that will be the 'label' column\n",
    "    output:\n",
    "        df_label: new dataframe mapping file name to label\n",
    "    '''\n",
    "    df_label = df[[file_label, label]]\n",
    "    df_label.set_index(file_label, inplace=True)\n",
    "    return df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTION TO READ TO METABOLOMICS WORKBENCH DATA - FROM Michael Murphy\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "def read_metadata(fn, label_col, label_key, debug=False):\n",
    "    sample_lines = []\n",
    "    feature_lines = []\n",
    "    metabolite_lines = []\n",
    "    with open(fn,'r') as f:\n",
    "        is_sample_line = False\n",
    "        is_feature_line = False\n",
    "        is_feature_header = False\n",
    "        is_metabolite_line = False\n",
    "        for l in f:\n",
    "            if 'MS_METABOLITE_DATA_START' in l:\n",
    "                is_feature_line = True\n",
    "                is_feature_header = True\n",
    "            elif 'METABOLITES_START' in l:\n",
    "                is_metabolite_line = True\n",
    "            elif '#SUBJECT_SAMPLE_FACTORS' in l:\n",
    "                is_sample_line = True\n",
    "                # FACTORS(NAME:VALUE pairs separated by |)[tab]\n",
    "                sample_lines.append('\\t'.join(l.replace('[tab]','\\t').split('\\t')[1:]))\n",
    "            elif is_feature_line:\n",
    "                if 'MS_METABOLITE_DATA_END' in l:\n",
    "                    is_feature_line = False\n",
    "                else:\n",
    "                    if is_feature_header:\n",
    "                        feature_lines.append(l)\n",
    "                        is_feature_header = False\n",
    "                    else:\n",
    "                        # throw away anything non-numeric after the sample header\n",
    "                        m = re.search(r'^[^\\t]*\\t([^\\t]*)\\t', l).group(1)\n",
    "                        try:\n",
    "                            if len(m) > 0:\n",
    "                                float(m)\n",
    "                            feature_lines.append(l)\n",
    "                        except:\n",
    "                            pass\n",
    "            elif is_metabolite_line:\n",
    "                if 'METABOLITES_END' in l:\n",
    "                    is_metabolite_line = False\n",
    "                else:\n",
    "                    metabolite_lines.append(l)\n",
    "            elif is_sample_line:\n",
    "#                 print(l.strip().split('\\t'))\n",
    "                if '#' in l:\n",
    "                    is_sample_line = False\n",
    "                else:\n",
    "                    # hack fix -- at least one file semi-duplicates the header line, easier to fix in here\n",
    "                    if 'Sample name' not in l:\n",
    "                        sample_lines.append('\\t'.join(l.split('\\t')[1:]))\n",
    "    \n",
    "    # this is basically doing text-to-columns on the sample metadata\n",
    "    samples = pd.read_csv(StringIO(''.join(sample_lines)), sep='\\t').set_index('SAMPLE')\n",
    "    samples = pd.concat([samples.iloc[:,0],\n",
    "                         # FACTORS(NAME:VALUE pairs separated by |)[tab]\n",
    "                         samples.iloc[:,1].astype('str').str.split('[:|]',expand=True),\n",
    "                         # Additional sample data\n",
    "                         samples.iloc[:,2].astype('str').str.split('[=;]',expand=True)], # ***\n",
    "                        axis=1)\n",
    "    # resulting table has field names and values interleaved as columns, this corrects that\n",
    "    cols = [samples.columns[0],] + samples.iloc[0,1::2].str.strip().tolist()\n",
    "    samples = samples.iloc[:,::2]\n",
    "    samples.columns = cols[:len(samples.columns)] # if *** is empty have to prune its label from end\n",
    "\n",
    "    if debug:\n",
    "        display(samples)\n",
    "\n",
    "    labels = pd.DataFrame(samples[label_col].str.strip().apply(lambda x: label_key[x]))\n",
    "\n",
    "    df = pd.read_csv(StringIO(''.join(feature_lines)), sep='\\t', index_col=False)\n",
    "    \n",
    "    # first row of df will contain metabolite identifiers, and we want (samples, features) shape\n",
    "    features = df.iloc[:,1:].T.copy()\n",
    "    # turn any leftover symbols into NaNs -- one file has '\\N' in place of NAs, for whatever reason\n",
    "    features = features.replace(r'(?:.*[^0-9\\.].*)',np.nan,regex=True).astype('float')\n",
    "    # drop any rows of all NaNs\n",
    "    features = features.loc[~np.all(features.isnull(),axis=1)]\n",
    "    \n",
    "    metabolites = df.iloc[:,[0,]].copy()\n",
    "    metabolites.columns = ['metabolite_name',]\n",
    "    \n",
    "    if len(metabolite_lines) > 0:\n",
    "        metabolites = metabolites.merge(pd.read_csv(StringIO(''.join(metabolite_lines)), sep='\\t'))\n",
    "    # thus far I've seen m/z and RT embedded in the metabolite identifier, separated by either @ or _\n",
    "    if metabolites['metabolite_name'].str.contains('[@_]',regex=True).any():\n",
    "        mz_rt = metabolites['metabolite_name'].str.split('[@_]',expand=True)\n",
    "        mz = mz_rt.iloc[:,0].str.strip('*').fillna('')\n",
    "        # the regex just drops anything that isn't a number\n",
    "        mz[~mz.str.match('^(?:[0-9]+(?:\\.[0-9]*)?)$').astype('bool')] = 'nan'\n",
    "        mz = mz.astype('float')\n",
    "        rt = mz_rt.iloc[:,1].str.strip('*').fillna('')\n",
    "        rt[~rt.str.match('^(?:[0-9]+(?:\\.[0-9]*)?)$').astype('bool')] = 'nan'\n",
    "        rt = rt.astype('float')\n",
    "        metabolites['mz'] = pd.Series(mz, index=metabolites.index)\n",
    "        metabolites['rt'] = pd.Series(rt, index=metabolites.index)\n",
    "        # these two fields are sometimes m/z and RT, not always\n",
    "        # metabolites['mz'] = metabolites[['moverz_quant','mz']].max(axis=1)\n",
    "        # metabolites['rt'] = metabolites[['ri','rt']].max(axis=1)\n",
    "    else:\n",
    "        metabolites['mz'] = pd.Series(np.nan * np.ones(metabolites.shape[0]))\n",
    "        metabolites['rt'] = pd.Series(np.nan * np.ones(metabolites.shape[0]))\n",
    "    metabolites = metabolites.set_index('metabolite_name')\n",
    "    \n",
    "    return features, labels, samples, metabolites\n",
    "\n",
    "\n",
    "def label_hist(labels):\n",
    "    num_lab = len(set(labels))\n",
    "    hist = {}\n",
    "    for ele in set(labels):\n",
    "        count = 0\n",
    "        for ele2 in labels:\n",
    "            if ele2 == ele:\n",
    "                count+=1\n",
    "        hist[ele] = count\n",
    "    return hist\n",
    "\n",
    "#### USE :\n",
    "# print(label_hist(label.values.flatten().astype('int').tolist()))\n",
    "#### to get th number of sampels for each class\n",
    "\n",
    "def reduce_multi(labels, data_set, label_key):\n",
    "    '''\n",
    "    for each of the classes make it a one vs. the rest problem. (ie 1 for it, and 0 for lables of the rest)\n",
    "    '''\n",
    "#     try:\n",
    "#         classes = set(labels.values.flatten())\n",
    "#     except:\n",
    "#         classes = set(labels.flatten())\n",
    "    switch_int = len(label_key)+10 #just some larger number to temporarily switch the labels to \n",
    "    label_sets = []\n",
    "    data_sets = []\n",
    "#     for ele in classes:\n",
    "    for k,ele in label_key.items():\n",
    "        data_sets.append(data_set+'_'+str(ele)+'_'+k)\n",
    "        label = labels.copy()\n",
    "        label[label==ele] = switch_int\n",
    "        label[label!=switch_int] = 0\n",
    "        label[label==switch_int] = 1\n",
    "        label_sets.append(label)\n",
    "    return label_sets, data_sets\n",
    "\n",
    "def reduce_to_one_v_one(feat, labels, data_set, label_key):\n",
    "    '''\n",
    "    - make a list of the one-to-one features, labels and names\n",
    "    - for all possible comparisons, filter the labels to only those in the comparison (relabel to 0/1)\n",
    "    - then take those names and filter the features\n",
    "    - use the label key to make the new data set names \n",
    "    '''\n",
    "    seen_combos = []\n",
    "    real_names = []\n",
    "    combos = []\n",
    "    for lab1 in label_key:\n",
    "        for lab2 in label_key:\n",
    "            if lab1 == lab2:\n",
    "                continue\n",
    "            if lab1+'_'+lab2 in seen_combos:\n",
    "                continue\n",
    "            seen_combos.append(lab1+'_'+lab2)\n",
    "            seen_combos.append(lab2+'_'+lab1)\n",
    "            real_names.append(lab1+'_'+lab2)\n",
    "            combos.append([label_key[lab1], label_key[lab2]])\n",
    "    ovo_feat = []\n",
    "    ovo_label = []\n",
    "    ovo_names = []\n",
    "    for combo, name in zip(combos,real_names):\n",
    "        lab = labels[labels.isin(combo)]\n",
    "        lab = lab.dropna()\n",
    "        lab_names = list(lab.index)\n",
    "#         print(feat.index)\n",
    "        try:\n",
    "            fea = feat.loc[lab_names]\n",
    "        except:\n",
    "            lab_names = [str(name) for name in lab_names]\n",
    "            fea = feat.loc[lab_names]\n",
    "#         print(lab.shape, fea.shape, lab)\n",
    "        if combo[1] != 0:\n",
    "            lab[lab == combo[0]] = 0\n",
    "            lab[lab == combo[1]] = 1\n",
    "        else:\n",
    "            lab[lab == combo[0]] = 1\n",
    "#         print(lab.shape, fea.shape, lab)\n",
    "        ovo_feat.append(fea)\n",
    "        ovo_label.append(lab)\n",
    "        ovo_names.append(data_set+'_'+name)\n",
    "    return ovo_feat, ovo_label, ovo_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Patient group\n",
      "SAMPLE               \n",
      "109               1.0\n",
      "11                1.0\n",
      "115               1.0\n",
      "12                1.0\n",
      "122               1.0\n",
      "13                1.0\n",
      "132               1.0\n",
      "133               1.0\n",
      "139               1.0\n",
      "14                1.0\n",
      "152               1.0\n",
      "154               1.0\n",
      "156               1.0\n",
      "157               1.0\n",
      "162               1.0\n",
      "168               1.0\n",
      "173               1.0\n",
      "177               1.0\n",
      "187               1.0\n",
      "193               1.0\n",
      "20                1.0\n",
      "200               1.0\n",
      "202               1.0\n",
      "214               1.0\n",
      "24                1.0\n",
      "250               1.0\n",
      "253               1.0\n",
      "257               1.0\n",
      "263               1.0\n",
      "276               1.0\n",
      "277               1.0\n",
      "278               1.0\n",
      "279               1.0\n",
      "28                1.0\n",
      "281               1.0\n",
      "282               1.0\n",
      "283               1.0\n",
      "284               1.0\n",
      "285               1.0\n",
      "289               1.0\n",
      "291               1.0\n",
      "292               1.0\n",
      "295               1.0\n",
      "300               1.0\n",
      "304               1.0\n",
      "310               1.0\n",
      "312               1.0\n",
      "34                1.0\n",
      "427               1.0\n",
      "447               1.0\n",
      "448               1.0\n",
      "452               1.0\n",
      "454               1.0\n",
      "455               1.0\n",
      "457               1.0\n",
      "469               1.0\n",
      "49                1.0\n",
      "491               1.0\n",
      "5                 1.0\n",
      "53                1.0\n",
      "561               1.0\n",
      "562               1.0\n",
      "575               1.0\n",
      "77                1.0\n",
      "1                 0.0\n",
      "100               0.0\n",
      "107               0.0\n",
      "108               0.0\n",
      "113               0.0\n",
      "118               0.0\n",
      "119               0.0\n",
      "120               0.0\n",
      "131               0.0\n",
      "140               0.0\n",
      "141               0.0\n",
      "159               0.0\n",
      "163               0.0\n",
      "170               0.0\n",
      "179               0.0\n",
      "18                0.0\n",
      "186               0.0\n",
      "192               0.0\n",
      "196               0.0\n",
      "199               0.0\n",
      "2                 0.0\n",
      "205               0.0\n",
      "208               0.0\n",
      "21                0.0\n",
      "210               0.0\n",
      "213               0.0\n",
      "216               0.0\n",
      "237               0.0\n",
      "244               0.0\n",
      "251               0.0\n",
      "252               0.0\n",
      "258               0.0\n",
      "264               0.0\n",
      "265               0.0\n",
      "271               0.0\n",
      "272               0.0\n",
      "288               0.0\n",
      "294               0.0\n",
      "3                 0.0\n",
      "30                0.0\n",
      "37                0.0\n",
      "39                0.0\n",
      "40                0.0\n",
      "406               0.0\n",
      "409               0.0\n",
      "423               0.0\n",
      "43                0.0\n",
      "433               0.0\n",
      "437               0.0\n",
      "442               0.0\n",
      "45                0.0\n",
      "46                0.0\n",
      "466               0.0\n",
      "475               0.0\n",
      "482               0.0\n",
      "483               0.0\n",
      "485               0.0\n",
      "488               0.0\n",
      "489               0.0\n",
      "490               0.0\n",
      "492               0.0\n",
      "498               0.0\n",
      "56                0.0\n",
      "566               0.0\n",
      "57                0.0\n",
      "577               0.0\n",
      "581               0.0\n",
      "586               0.0\n",
      "588               0.0\n",
      "59                0.0\n",
      "6                 0.0\n",
      "61                0.0\n",
      "63                0.0\n",
      "64                0.0\n",
      "65                0.0\n",
      "7                 0.0\n",
      "75                0.0\n",
      "76                0.0\n",
      "78                0.0\n",
      "79                0.0\n",
      "87                0.0\n",
      "88                0.0\n",
      "93                0.0\n",
      "98                0.0\n",
      "        Patient group\n",
      "SAMPLE               \n",
      "1                 0.0\n",
      "100               0.0\n",
      "107               0.0\n",
      "108               0.0\n",
      "113               0.0\n",
      "118               0.0\n",
      "119               0.0\n",
      "120               0.0\n",
      "131               0.0\n",
      "140               0.0\n",
      "141               0.0\n",
      "159               0.0\n",
      "163               0.0\n",
      "170               0.0\n",
      "179               0.0\n",
      "18                0.0\n",
      "186               0.0\n",
      "192               0.0\n",
      "196               0.0\n",
      "199               0.0\n",
      "2                 0.0\n",
      "205               0.0\n",
      "208               0.0\n",
      "21                0.0\n",
      "210               0.0\n",
      "213               0.0\n",
      "216               0.0\n",
      "237               0.0\n",
      "244               0.0\n",
      "251               0.0\n",
      "252               0.0\n",
      "258               0.0\n",
      "264               0.0\n",
      "265               0.0\n",
      "271               0.0\n",
      "272               0.0\n",
      "288               0.0\n",
      "294               0.0\n",
      "3                 0.0\n",
      "30                0.0\n",
      "37                0.0\n",
      "39                0.0\n",
      "40                0.0\n",
      "406               0.0\n",
      "409               0.0\n",
      "423               0.0\n",
      "43                0.0\n",
      "433               0.0\n",
      "437               0.0\n",
      "442               0.0\n",
      "45                0.0\n",
      "46                0.0\n",
      "466               0.0\n",
      "475               0.0\n",
      "482               0.0\n",
      "483               0.0\n",
      "485               0.0\n",
      "488               0.0\n",
      "489               0.0\n",
      "490               0.0\n",
      "492               0.0\n",
      "498               0.0\n",
      "56                0.0\n",
      "566               0.0\n",
      "57                0.0\n",
      "577               0.0\n",
      "581               0.0\n",
      "586               0.0\n",
      "588               0.0\n",
      "59                0.0\n",
      "6                 0.0\n",
      "61                0.0\n",
      "63                0.0\n",
      "64                0.0\n",
      "65                0.0\n",
      "7                 0.0\n",
      "75                0.0\n",
      "76                0.0\n",
      "78                0.0\n",
      "79                0.0\n",
      "87                0.0\n",
      "88                0.0\n",
      "93                0.0\n",
      "98                0.0\n",
      "10                1.0\n",
      "102               1.0\n",
      "106               1.0\n",
      "116               1.0\n",
      "123               1.0\n",
      "136               1.0\n",
      "137               1.0\n",
      "142               1.0\n",
      "149               1.0\n",
      "151               1.0\n",
      "155               1.0\n",
      "160               1.0\n",
      "17                1.0\n",
      "171               1.0\n",
      "172               1.0\n",
      "174               1.0\n",
      "175               1.0\n",
      "178               1.0\n",
      "181               1.0\n",
      "182               1.0\n",
      "183               1.0\n",
      "190               1.0\n",
      "195               1.0\n",
      "220               1.0\n",
      "221               1.0\n",
      "223               1.0\n",
      "227               1.0\n",
      "228               1.0\n",
      "229               1.0\n",
      "232               1.0\n",
      "238               1.0\n",
      "239               1.0\n",
      "241               1.0\n",
      "248               1.0\n",
      "255               1.0\n",
      "266               1.0\n",
      "267               1.0\n",
      "269               1.0\n",
      "29                1.0\n",
      "290               1.0\n",
      "303               1.0\n",
      "308               1.0\n",
      "311               1.0\n",
      "35                1.0\n",
      "4                 1.0\n",
      "402               1.0\n",
      "413               1.0\n",
      "415               1.0\n",
      "446               1.0\n",
      "450               1.0\n",
      "47                1.0\n",
      "470               1.0\n",
      "474               1.0\n",
      "479               1.0\n",
      "48                1.0\n",
      "493               1.0\n",
      "506               1.0\n",
      "507               1.0\n",
      "52                1.0\n",
      "552               1.0\n",
      "556               1.0\n",
      "560               1.0\n",
      "563               1.0\n",
      "569               1.0\n",
      "571               1.0\n",
      "580               1.0\n",
      "582               1.0\n",
      "591               1.0\n",
      "596               1.0\n",
      "598               1.0\n",
      "70                1.0\n",
      "72                1.0\n",
      "74                1.0\n",
      "86                1.0\n",
      "91                1.0\n",
      "99                1.0\n",
      "        Patient group\n",
      "SAMPLE               \n",
      "109               0.0\n",
      "11                0.0\n",
      "115               0.0\n",
      "12                0.0\n",
      "122               0.0\n",
      "13                0.0\n",
      "132               0.0\n",
      "133               0.0\n",
      "139               0.0\n",
      "14                0.0\n",
      "152               0.0\n",
      "154               0.0\n",
      "156               0.0\n",
      "157               0.0\n",
      "162               0.0\n",
      "168               0.0\n",
      "173               0.0\n",
      "177               0.0\n",
      "187               0.0\n",
      "193               0.0\n",
      "20                0.0\n",
      "200               0.0\n",
      "202               0.0\n",
      "214               0.0\n",
      "24                0.0\n",
      "250               0.0\n",
      "253               0.0\n",
      "257               0.0\n",
      "263               0.0\n",
      "276               0.0\n",
      "277               0.0\n",
      "278               0.0\n",
      "279               0.0\n",
      "28                0.0\n",
      "281               0.0\n",
      "282               0.0\n",
      "283               0.0\n",
      "284               0.0\n",
      "285               0.0\n",
      "289               0.0\n",
      "291               0.0\n",
      "292               0.0\n",
      "295               0.0\n",
      "300               0.0\n",
      "304               0.0\n",
      "310               0.0\n",
      "312               0.0\n",
      "34                0.0\n",
      "427               0.0\n",
      "447               0.0\n",
      "448               0.0\n",
      "452               0.0\n",
      "454               0.0\n",
      "455               0.0\n",
      "457               0.0\n",
      "469               0.0\n",
      "49                0.0\n",
      "491               0.0\n",
      "5                 0.0\n",
      "53                0.0\n",
      "561               0.0\n",
      "562               0.0\n",
      "575               0.0\n",
      "77                0.0\n",
      "10                1.0\n",
      "102               1.0\n",
      "106               1.0\n",
      "116               1.0\n",
      "123               1.0\n",
      "136               1.0\n",
      "137               1.0\n",
      "142               1.0\n",
      "149               1.0\n",
      "151               1.0\n",
      "155               1.0\n",
      "160               1.0\n",
      "17                1.0\n",
      "171               1.0\n",
      "172               1.0\n",
      "174               1.0\n",
      "175               1.0\n",
      "178               1.0\n",
      "181               1.0\n",
      "182               1.0\n",
      "183               1.0\n",
      "190               1.0\n",
      "195               1.0\n",
      "220               1.0\n",
      "221               1.0\n",
      "223               1.0\n",
      "227               1.0\n",
      "228               1.0\n",
      "229               1.0\n",
      "232               1.0\n",
      "238               1.0\n",
      "239               1.0\n",
      "241               1.0\n",
      "248               1.0\n",
      "255               1.0\n",
      "266               1.0\n",
      "267               1.0\n",
      "269               1.0\n",
      "29                1.0\n",
      "290               1.0\n",
      "303               1.0\n",
      "308               1.0\n",
      "311               1.0\n",
      "35                1.0\n",
      "4                 1.0\n",
      "402               1.0\n",
      "413               1.0\n",
      "415               1.0\n",
      "446               1.0\n",
      "450               1.0\n",
      "47                1.0\n",
      "470               1.0\n",
      "474               1.0\n",
      "479               1.0\n",
      "48                1.0\n",
      "493               1.0\n",
      "506               1.0\n",
      "507               1.0\n",
      "52                1.0\n",
      "552               1.0\n",
      "556               1.0\n",
      "560               1.0\n",
      "563               1.0\n",
      "569               1.0\n",
      "571               1.0\n",
      "580               1.0\n",
      "582               1.0\n",
      "591               1.0\n",
      "596               1.0\n",
      "598               1.0\n",
      "70                1.0\n",
      "72                1.0\n",
      "74                1.0\n",
      "86                1.0\n",
      "91                1.0\n",
      "99                1.0\n"
     ]
    }
   ],
   "source": [
    "# No reprocessed data\n",
    "# doesnt look like there are duplicates \n",
    "# targeted analysis \n",
    "study = 'ST000284_data'\n",
    "disease = 'Colorectal Cancer'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "f = 'AN000452.txt'\n",
    "\n",
    "label_col = 'Patient group'\n",
    "label_key = {'Healthy': 0, 'CRC': 1, 'Polyp':2} \n",
    "features, labels, samples, metabolites = read_metadata(f, label_col, label_key)\n",
    "feature_names = list(features.index)\n",
    "feature_names = [int(fi) for fi in feature_names]\n",
    "label = labels.loc[feature_names]\n",
    "if multi_to_single:\n",
    "    labels, ds_names = reduce_multi(label, f[:-4], label_key)\n",
    "    for l, n in zip(labels, ds_names):\n",
    "        data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': n,\n",
    "                 'features': features,\n",
    "                 'labels': l,\n",
    "                 'peaks': metabolites,\n",
    "                 'samples': samples.loc[feature_names]}) \n",
    "elif one_to_one:\n",
    "    feat, labels, ds_names = reduce_to_one_v_one(features, label, f[:-4], label_key)\n",
    "    for f, l, n in zip(feat, labels, ds_names):\n",
    "        f_names = list(f.index)\n",
    "        f_names = [int(name) for name in f_names]\n",
    "        s = samples.loc[f_names]\n",
    "        data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': n,\n",
    "                 'features': f,\n",
    "                 'labels': l,\n",
    "                 'peaks': metabolites,\n",
    "                 'samples': s}) \n",
    "else:\n",
    "    data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': f[:-4],\n",
    "                 'features': features,\n",
    "                 'labels': label,\n",
    "                 'peaks': metabolites,\n",
    "                 'samples': samples.loc[feature_names]}) \n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I dont think there are duplicates except for what I note below\n",
    "# labels match the data\n",
    "# 12-7-18: order is same between data sets - can combine\n",
    "study = 'ST000355_data'\n",
    "disease = 'Breast Cancer'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "files = ['AN000580.txt', 'AN000581.txt']\n",
    "\n",
    "label_col = 'Diagnosis'\n",
    "label_key = {'Control': 0, 'Breast cancer': 1} \n",
    "for f in files:\n",
    "    features, labels, samples, metabolites = read_metadata(f, label_col, label_key)\n",
    "    feature_names = list(features.index)\n",
    "    label = labels.loc[feature_names]\n",
    "    seen = []\n",
    "    t_f_mask = []\n",
    "    for i, val in enumerate(list(label.index)):\n",
    "        if val not in seen:\n",
    "            seen.append(val)\n",
    "            if list(label.values)[i] != 0 and list(label.values)[i] != 1:\n",
    "                t_f_mask.append(False)\n",
    "            else:\n",
    "                t_f_mask.append(True)\n",
    "        else:\n",
    "            t_f_mask.append(False)\n",
    "    label = label[t_f_mask]\n",
    "    label_names = list(label.index)\n",
    "    features = features.loc[label_names]\n",
    "    # note: there are three samples seen twice in the data, but its from different stages of cancer (i believe)\n",
    "    data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': f[:-4],\n",
    "                 'features': features,\n",
    "                 'labels': label,\n",
    "                 'peaks': metabolites,\n",
    "                 'samples': samples.loc[feature_names]}) \n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 134 people now, GC data \n",
    "# doesnt look like replicates from what I can tell\n",
    "# labels match to the data\n",
    "# 12-7-18: order is same between data sets - can combine\n",
    "study = 'ST000356_data'\n",
    "disease = 'Breast Cancer'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "files = ['AN000582.txt', 'AN000583.txt']\n",
    "\n",
    "label_col = 'Diagnosis'\n",
    "label_key = {'control': 0, 'breast cancer': 1} \n",
    "for f in files:\n",
    "    features, labels, samples, metabolites = read_metadata(f, label_col, label_key)\n",
    "    feature_names = list(features.index)\n",
    "    label = labels.loc[feature_names]\n",
    "    # note: there are three samples seen twice in the data, but its from different stages of cancer (i believe)\n",
    "    data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': f[:-4],\n",
    "                 'features': features,\n",
    "                 'labels': label,\n",
    "                 'peaks': metabolites,\n",
    "                 'samples': samples.loc[feature_names]}) \n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like 56 people with no duplicates in samples \n",
    "study = 'ST000383_data'\n",
    "disease = 'Obesity - Non-diabetic and T2 diabetic'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "f = 'AN000618.txt'\n",
    "\n",
    "label_col = 'Health Status'\n",
    "label_key = {'diabetic': 1, 'non-diabetic': 0} \n",
    "features, labels, samples, metabolites = read_metadata(f, label_col, label_key)\n",
    "feature_names = list(features.index)\n",
    "label = labels.loc[feature_names]\n",
    "data.append({'study': study[:-5],\n",
    "             'disease': disease,\n",
    "             'data_set': f[:-4],\n",
    "             'features': features,\n",
    "             'labels': label,\n",
    "             'peaks': metabolites,\n",
    "             'samples': samples.loc[feature_names]}) \n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 84 people in study\n",
    "# labels match, no replicates\n",
    "# 12-7-18: order is same between data sets - can combine\n",
    "study = 'ST000450_data'\n",
    "disease = 'Chronic fatigue'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "files = ['AN000705.txt', 'AN000706.txt']\n",
    "\n",
    "label_col = 'Disease'\n",
    "label_key = {'Normal': 0, 'CFS': 1} \n",
    "for f in files:\n",
    "    features, labels, samples, metabolites = read_metadata(f, label_col, label_key)\n",
    "    feature_names = list(features.index)\n",
    "    label = labels.loc[feature_names]\n",
    "    data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': f[:-4],\n",
    "                 'features': features,\n",
    "                 'labels': label,\n",
    "                 'peaks': metabolites,\n",
    "                 'samples': samples.loc[feature_names]}) \n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:14: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "# 3 replicates for each\n",
    "# compared dried blood spots vs serum \n",
    "# 12-7-18: order is same between data sets - can combine\n",
    "study = 'ST000608_data'\n",
    "disease = 'Stability of dried blood samples - diabetic men'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "files = ['AN000929.txt', 'AN000930.txt', 'AN000931.txt']\n",
    "\n",
    "label_col = 'Factor'\n",
    "label_key = {'control': 0, 'case': 1} \n",
    "for f in files:\n",
    "    features, labels, samples, metabolites = read_metadata(f, label_col, label_key)\n",
    "    features = features[samples['Replicate']=='1']\n",
    "    feature_names = list(features.index)\n",
    "    label = labels.loc[feature_names]\n",
    "    data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': f[:-4],\n",
    "                 'features': features,\n",
    "                 'labels': label,\n",
    "                 'peaks': metabolites,\n",
    "                 'samples': samples.loc[feature_names]}) \n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# some randomly have replicates \n",
    "# looks like labels match up to data\n",
    "study = 'ST000888_data'\n",
    "disease = 'Lyme disease'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "f = 'AN001450.txt'\n",
    "\n",
    "label_col = 'Disease Status'\n",
    "# label_key = {'Early Disseminated Lyme': 1, 'Heathy (Non-endemic) control': 0, \n",
    "#              'Early Localized Lyme':1, 'Healthy Endemic Control':0, \n",
    "#              'Early Lyme': 1, 'Heatlhy Controls - Endemic':0,\n",
    "#              'Healthy Non-endemic control':0, 'Mononucleosis':0, \n",
    "#              'Fibromyalgia':0, 'Lyme C6+ Baseline':1, 'Lyme C6+ Baseline /Atyp':1, 'Severe Periodontitis':0, 'Syphilis':0} \n",
    "\n",
    "# note following what the paper did which was classify lyme vs NOT lyme. \n",
    "label_key = {'Early Disseminated Lyme': 1, 'Heathy (Non-endemic) control': 0, \n",
    "             'Early Localized Lyme':1, 'Healthy Endemic Control':0, \n",
    "             'Early Lyme': 1, 'Heatlhy Controls - Endemic':0,\n",
    "             'Healthy Non-endemic control':0, 'Mononucleosis':0, \n",
    "             'Fibromyalgia':0, 'Lyme C6+ Baseline':1, 'Lyme C6+ Baseline /Atyp':1, 'Severe Periodontitis':0, 'Syphilis':0} \n",
    "features, labels, samples, metabolites = read_metadata(f, label_col, label_key)\n",
    "feature_names = list(features.index)\n",
    "keep_files = []\n",
    "seen = []\n",
    "toss = []\n",
    "for ele in feature_names:\n",
    "    ind = ele.find('Run')\n",
    "    part_file = ele[:ind]\n",
    "    if part_file not in seen:\n",
    "        seen.append(part_file)\n",
    "        keep_files.append(ele)\n",
    "    else:\n",
    "        toss.append(ele)\n",
    "features = features.loc[keep_files]\n",
    "label = labels.loc[keep_files]\n",
    "label = label.dropna()\n",
    "\n",
    "features = features.loc[list(label.index)]\n",
    "data.append({'study': study[:-5],\n",
    "             'disease': disease,\n",
    "             'data_set': f[:-4],\n",
    "             'features': features,\n",
    "             'labels': label,\n",
    "             'peaks': metabolites,\n",
    "             'samples': samples.loc[keep_files]}) \n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = 'ST000918_data'\n",
    "disease = 'Breast cancer'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "f = 'AN001503.txt'\n",
    "\n",
    "label_col = 'Disease Status'\n",
    "label_key = {'Breast cancer patient': 1, 'Control patient': 0} \n",
    "features, labels, samples, metabolites = read_metadata(f, label_col, label_key)\n",
    "feature_names = list(features.index)\n",
    "label = labels.loc[feature_names]\n",
    "data.append({'study': study[:-5],\n",
    "             'disease': disease,\n",
    "             'data_set': f[:-4],\n",
    "             'features': features,\n",
    "             'labels': label,\n",
    "             'peaks': metabolites,\n",
    "             'samples': samples.loc[feature_names]}) \n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont think there are any duplicates \n",
    "# This is just the targeted oxylipin assay. \n",
    "study = 'MTBLS253_data' \n",
    "disease = 'chronic hepatitis B'\n",
    "\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "df_lc = pd.read_csv('a_oxylipin_mass_spectrometry.txt',sep='\\t')\n",
    "df_s = pd.read_csv('s_mtbls253.txt', sep='\\t')\n",
    "df_lc = df_s.merge(df_lc, on='Sample Name').set_index('Sample Name')\n",
    "\n",
    "labels = df_lc['Factor Value[control]']\n",
    "lab_to_int = {True:0, False:1}\n",
    "labels = labels.replace(lab_to_int)\n",
    "\n",
    "#get author data:\n",
    "features = pd.read_csv('m_oxylipin_analyses_of_chronic_hepatitis_b_metabolite_profiling_mass_spectrometry_v2_maf.tsv', sep='\\t')\n",
    "file_names = list(features.iloc[:,10:])\n",
    "feat = features.iloc[:,10:].T.astype('float')\n",
    "new_files = []\n",
    "for fi in file_names:\n",
    "    fi = int(fi[8:12])\n",
    "    new_files.append(fi)\n",
    "df_l = labels.loc[new_files]\n",
    "data.append({'study':study[:-5],\n",
    "             'disease': disease,\n",
    "             'data_set': 'm_oxylipin_chronic_hep_b',\n",
    "             'features': feat,\n",
    "             'labels': df_l,\n",
    "             'peaks': features,\n",
    "             'samples': df_lc})\n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12-7-18: order is same between data sets - can combine\n",
    "study = 'MTBLS279_data' \n",
    "disease = 'chronic hepatitis B'\n",
    "\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "\n",
    "df_p = pd.read_csv('a_POS_lipid_analyses_of_chronic_hepatitis_b_mass_spectrometry.txt',sep='\\t')\n",
    "df_n = pd.read_csv('a_NEG_lipid_analyses_of_chronic_hepatitis_b_mass_spectrometry.txt',sep='\\t')\n",
    "df_s = pd.read_csv('s_mtbls279.txt', sep='\\t')\n",
    "df_p = df_s.merge(df_p, on='Sample Name').set_index('Sample Name')\n",
    "df_n = df_s.merge(df_n, on='Sample Name').set_index('Sample Name')\n",
    "\n",
    "labels_p = df_p['Factor Value[control]']\n",
    "labels_n = df_n['Factor Value[control]']\n",
    "lab_to_int = {True:0, False:1}\n",
    "labels_p = labels_p.replace(lab_to_int)\n",
    "labels_n = labels_n.replace(lab_to_int)\n",
    "\n",
    "#get author data:\n",
    "files = ['m_POS_lipid_analyses_of_chronic_hepatitis_b_mass_spectrometry_v2_maf.tsv',\n",
    "         'm_NEG_lipid_analyses_of_chronic_hepatitis_b_mass_spectrometry_v2_maf.tsv']\n",
    "for f in files:\n",
    "    if 'POS' in f:\n",
    "        labels = labels_p\n",
    "        df = df_p\n",
    "    else:\n",
    "        labels = labels_n\n",
    "        df = df_n\n",
    "    features = pd.read_csv(f, sep='\\t')\n",
    "    file_names = list(features.iloc[:,21:])\n",
    "    feat = features.iloc[:,21:].T.astype('float')\n",
    "    file_names = [int(fi) for fi in file_names]\n",
    "    df_l = labels.loc[file_names]\n",
    "    data.append({'study':study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': 'm_chronic_hep_b_'+f[2:5],\n",
    "                 'features': feat,\n",
    "                 'labels': df_l,\n",
    "                 'peaks': features,\n",
    "                 'samples': df})\n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))\n",
    "# NOTE: since 280 is also jst targeted but for amines (i think) I wont be analyzing since thats not the point of this work\n",
    "# we care more about untargeted studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ee/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n"
     ]
    }
   ],
   "source": [
    "# this study would be super useful for looking at confounders since their final models had many!\n",
    "# 12-7-18: order is same between data sets - can combine\n",
    "study = 'MTBLS358_data'\n",
    "disease = 'COPD'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "\n",
    "df_cer = pd.read_csv('a_CER_mass_spectrometry.txt', sep='\\t')\n",
    "df_eico = pd.read_csv('a_EICO_mass_spectrometry.txt', sep='\\t')\n",
    "df_tag = pd.read_csv('a_TAG_mass_spectrometry.txt', sep='\\t')\n",
    "df_shot = pd.read_csv('a_SHOT_mass_spectrometry.txt', sep='\\t')\n",
    "# df_shot_n = pd.read_csv('a_SHOT_mass_spectrometry-neg.txt', sep='\\t')\n",
    "# df_shot_p = pd.read_csv('a_SHOT_mass_spectrometry-pos.txt', sep='\\t')\n",
    "\n",
    "df_s = pd.read_csv('s_Study.txt', sep='\\t')\n",
    "\n",
    "df_cer = df_s.merge(df_cer, on='Sample Name').set_index('Sample Name')\n",
    "df_eico = df_s.merge(df_eico, on='Sample Name').set_index('Sample Name')\n",
    "df_tag = df_s.merge(df_tag, on='Sample Name').set_index('Sample Name')\n",
    "df_shot = df_s.merge(df_shot, on='Sample Name').set_index('Sample Name')\n",
    "# df_shot_n = df_s.merge(df_shot_n, on='Sample Name').set_index('Sample Name')\n",
    "# df_shot_p = df_s.merge(df_shot_p, on='Sample Name').set_index('Sample Name')\n",
    "\n",
    "dfs = [df_cer, df_eico, df_shot, df_tag]\n",
    "files = ['m_CER_mass_spectrometry_v4.maf', 'm_EICO_mass_spectrometry_v4.maf',\n",
    "         'm_SHOT_mass_spectrometry_v4.maf', 'm_TAG_mass_spectrometry_v4.maf']\n",
    "# Factor Value[Study Group] is the label\n",
    "to_replace = {'COPD':1, 'FS':2, 'CS':3, 'NS':0} \n",
    "# ns: never smoke, fs: former smoker, cs: current smoker, also the COPD are all smokers\n",
    "for f, df in zip(files,dfs):\n",
    "    features = pd.read_csv(f, sep='\\t')\n",
    "    file_names = list(features.iloc[:,21:])\n",
    "    feat = features.iloc[:,21:].T.astype('float')\n",
    "    labels = df['Factor Value[Study Group]']\n",
    "    df_l = labels.loc[file_names]\n",
    "    df_l = df_l[df_l.notnull()]\n",
    "    df_l = df_l.replace(to_replace)\n",
    "    new_files = list(df_l.index)\n",
    "    feat = feat.loc[new_files]\n",
    "    if multi_to_single:\n",
    "        labels_new, ds_names = reduce_multi(df_l, f[:-4], to_replace)\n",
    "        for l, n in zip(labels_new, ds_names):\n",
    "            data.append({'study': study[:-5],\n",
    "                         'disease': disease,\n",
    "                         'data_set': n,\n",
    "                         'features': feat,\n",
    "                         'labels': l,\n",
    "                         'peaks': features[['mass_to_charge','retention_time']],\n",
    "                         'samples': df}) \n",
    "    elif one_to_one:\n",
    "        feat, labels, ds_names = reduce_to_one_v_one(feat, df_l, f[:-4], to_replace)\n",
    "        for f, l, n in zip(feat, labels, ds_names):\n",
    "            f_names = list(f.index)\n",
    "#             f_names = [int(name) for name in f_names]\n",
    "            s = df.loc[f_names]\n",
    "            data.append({'study': study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': n,\n",
    "                     'features': f,\n",
    "                     'labels': l,\n",
    "                     'peaks': features[['mass_to_charge','retention_time']],\n",
    "                     'samples': s}) \n",
    "    else:\n",
    "        data.append({'study':study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': f[:-4],\n",
    "                     'features': feat,\n",
    "                     'labels': df_l,\n",
    "                     'peaks': features[['mass_to_charge','retention_time']],\n",
    "                     'samples': df})   \n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = 'MTBLS579_data'\n",
    "disease = 'typhoid carriage'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "\n",
    "df = pd.read_csv('a_typhoid_carriage_metabolite_profiling_mass_spectrometry.txt', sep='\\t')\n",
    "df_s = pd.read_csv('s_Typhoid carriage.txt', sep='\\t')\n",
    "df = df_s.merge(df, on='Sample Name').set_index('Sample Name')\n",
    "to_replace = {'S. Paratyphi A carriage':1, 'S. Typhi carriage':1, 'Non-carriage control':0, 'Quality control':np.nan}\n",
    "labels = df['Factor Value[Carriage status]'].replace(to_replace)\n",
    "labels = labels[labels.notnull()]\n",
    "file_names = list(labels.index)\n",
    "\n",
    "file_name = 'm_typhoid_carriage_metabolite_profiling_mass_spectrometry_v2_maf.tsv'\n",
    "features = pd.read_csv(file_name, sep='\\t')\n",
    "feat = features.iloc[:,21:].T.astype('float')\n",
    "feat = feat.loc[file_names]\n",
    "data.append({'study':study[:-5],\n",
    "             'disease': disease,\n",
    "             'data_set': file_name[:-4],\n",
    "             'features': feat,\n",
    "             'labels': labels,\n",
    "             'peaks': features[['mass_to_charge','retention_time']],\n",
    "             'samples': df})  \n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studies with raw data but I have not fully finished XCMS runs on... or are from XCMSonline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was an xcmsonline study\n",
    "# looks like 97 people from the authors\n",
    "# looks like the labels match to the samples\n",
    "# NOTE: for the reprocessed data I did two reprocessings with different parameters\n",
    "study = 'ST000062_data'\n",
    "disease = 'Depression'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "f = 'AN000100.txt'\n",
    "\n",
    "label_col = 'Source'\n",
    "label_key = {'Group 1 - Score 0': 0, 'Group 2 - Score 50': 1} \n",
    "features, labels, samples, metabolites = read_metadata(f, label_col, label_key)\n",
    "\n",
    "files = ['XCMS-Report-annotated-SingleClass-GCTOF.xlsx'] # 'XCMS-Report-annotated-SingleClass.xlsx',\n",
    "for f in files:\n",
    "    features = pd.read_excel(f) \n",
    "    feat = features.iloc[:,10:-3].T #.loc[list(labels.index)]\n",
    "#     inds = list(feat.index)\n",
    "    labels = labels.loc[list(feat.index)]\n",
    "    labels = labels.dropna(how='all')\n",
    "    samples = samples.loc[list(labels.index)]\n",
    "    feat = feat.loc[list(labels.index)]\n",
    "    data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': f[:-4],\n",
    "                 'features': feat,\n",
    "                 'labels': labels,\n",
    "                 'peaks': features[['mzmed', 'mzmin', 'mzmax', 'rtmed', 'rtmin', 'rtmax', 'npeaks', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                 'samples': samples})     \n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was an xcmsonline study\n",
    "# looks like 97 people from the authors\n",
    "# looks like the labels match to the samples\n",
    "# NOTE: for the reprocessed data I did two reprocessings with different parameters\n",
    "study = 'ST000063_data'\n",
    "disease = 'Depression'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "f = 'AN000101.txt'\n",
    "\n",
    "label_col = 'Source'\n",
    "label_key = {'Group 1 - Score 0': 0, 'Group 2 - Score 50': 1} \n",
    "features, labels, samples, metabolites = read_metadata(f, label_col, label_key)\n",
    "\n",
    "files = ['XCMS-Report-annotated-SingleClass-GCTOF.xlsx'] #'XCMS-Report-annotated-SingleClass.xlsx',\n",
    "for f in files:\n",
    "    features = pd.read_excel(f) \n",
    "    feat = features.iloc[:,10:-3].T #.loc[list(labels.index)]\n",
    "#     inds = list(feat.index)\n",
    "    labels = labels.loc[list(feat.index)]\n",
    "    labels = labels.dropna(how='all')\n",
    "    samples = samples.loc[list(labels.index)]\n",
    "    feat = feat.loc[list(labels.index)]\n",
    "    data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': f[:-4],\n",
    "                 'features': feat,\n",
    "                 'labels': labels,\n",
    "                 'peaks': features[['mzmed', 'mzmin', 'mzmax', 'rtmed', 'rtmin', 'rtmax', 'npeaks', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                 'samples': samples})    \n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# in the author data: dont think there are duplicates (< the number authors stated)\n",
    "# got the labels to match to the results\n",
    "# 12-7-18: for the author data the positive and negative can now be combined! \n",
    "study = 'MTBLS352_data'\n",
    "disease = \"Diabetes - healthy v. T2 v. prediabetic\"\n",
    "os.chdir(os.path.join(root,study))\n",
    "\n",
    "data = []\n",
    "df_s = pd.read_csv('s_MTBLS352.txt', sep='\\t').set_index('Sample Name')\n",
    "\n",
    "lab_to_int = {'T2D patients':1, 'normal glucose tolerant':0, 'prediabetes':2}\n",
    "lab_to_int_short = {'T2D':1, 'NGT':0, 'Pre-DM':2}\n",
    "labels_real = df_s['Factor Value[Group]'].replace(lab_to_int)\n",
    "labels_real = labels_real[labels_real.notnull()]\n",
    "\n",
    "files = ['DEMO_neg-norm-metaboAnalystInput.csv', 'DEMO_pos-norm-metaboAnalystInput.csv']\n",
    "for f in files:\n",
    "    features = pd.read_csv(f)\n",
    "    feat = features.iloc[:,1:].T.astype('float')\n",
    "    peaks = features.iloc[:,0]\n",
    "    file_names = list(feat.index)\n",
    "    label = [p.split('_')[0] for p in file_names]\n",
    "    names = list(feat.index)\n",
    "    df_n = pd.DataFrame(names, columns=['Sample Names'])\n",
    "    df_l = pd.DataFrame(label, columns=['label']).replace(lab_to_int_short)\n",
    "    df_l = pd.concat([df_n, df_l], axis=1)\n",
    "    df_l = df_l.set_index('Sample Names')\n",
    "    df_l = df_l[df_l.label != 'QC']\n",
    "    keep_files = list(df_l.index)\n",
    "    feat = feat.loc[keep_files]\n",
    "    if multi_to_single:\n",
    "        labels_new, ds_names = reduce_multi(df_l, f[:-4], lab_to_int_short)\n",
    "        for l, n in zip(labels_new, ds_names):\n",
    "            data.append({'study': study[:-5],\n",
    "                         'disease': disease,\n",
    "                         'data_set': n,\n",
    "                         'features': feat,\n",
    "                         'labels': l,\n",
    "                         'peaks': peaks,\n",
    "                         'samples': df_s}) \n",
    "    elif one_to_one:\n",
    "        feat, labels, ds_names = reduce_to_one_v_one(feat, df_l, f[:-4], lab_to_int_short)\n",
    "        for f, l, n in zip(feat, labels, ds_names):\n",
    "            f_names = list(f.index)\n",
    "#             f_names = [int(name) for name in f_names]\n",
    "#             s = df_s.loc[f_names]\n",
    "            data.append({'study': study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': n,\n",
    "                     'features': f,\n",
    "                     'labels': l,\n",
    "                     'peaks': peaks,\n",
    "                     'samples': df_s}) \n",
    "    else:\n",
    "        data.append({'study': study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': f[:-4],\n",
    "                     'features': feat,\n",
    "                     'labels': df_l,\n",
    "                     'peaks': peaks,\n",
    "                     'samples': df_s})  \n",
    "# this next part is to get only the shared elements so you can combine datasets\n",
    "\n",
    "if one_to_one:\n",
    "    #0/3, 1/4, 2/5 map up\n",
    "    combos = [(0,3), (1,4), (2,5)]\n",
    "    for combo in combos:\n",
    "        labels_2 = list(data[combo[1]]['features'].index)\n",
    "        labels_1 = list(data[combo[0]]['features'].index)\n",
    "        shared = [value for value in labels_1 if value in labels_2]\n",
    "        data[combo[1]]['features'] = data[combo[1]]['features'].loc[shared]\n",
    "        data[combo[1]]['labels'] = data[combo[1]]['labels'].loc[shared]\n",
    "        data[combo[0]]['features'] = data[combo[0]]['features'].loc[shared]\n",
    "        data[combo[0]]['labels'] = data[combo[0]]['labels'].loc[shared]\n",
    "if multi_to_single:\n",
    "    labels_2 = list(data[3]['features'].index)\n",
    "    labels_1 = list(data[0]['features'].index)\n",
    "    shared = [value for value in labels_1 if value in labels_2]\n",
    "    for i in range(len(data)):\n",
    "        data[i]['features'] = data[i]['features'].loc[shared]\n",
    "        data[i]['labels'] = data[i]['labels'].loc[shared]\n",
    "#     print(data[i]['features'].shape, data[i]['labels'].shape)\n",
    "\n",
    "\n",
    "# get my data: \n",
    "dirs = ['neg'] #pos once done...\n",
    "for folder in dirs:\n",
    "    data_file = 'IPO_aligned_MTBLS352_' + folder + '.csv'\n",
    "    open_file = os.path.join(folder,data_file)\n",
    "    features = pd.read_csv(open_file) \n",
    "    file_names_old = list(features.iloc[:,9:-3])    \n",
    "    feat = features.iloc[:,9:-3].T.astype('float')\n",
    "    file_names = [fi[:-9].replace('.','-') for fi in file_names_old]\n",
    "    new_index = dict(zip(file_names_old,file_names))\n",
    "    feat = feat.rename(new_index)\n",
    "    df_l = labels_real.loc[file_names]\n",
    "    if multi_to_single:\n",
    "        labels_new, ds_names = reduce_multi(df_l, data_file[:-4], lab_to_int_short)\n",
    "        for l, n in zip(labels_new, ds_names):\n",
    "            data.append({'study': study[:-5],\n",
    "                         'disease': disease,\n",
    "                         'data_set': n,\n",
    "                         'features': feat,\n",
    "                         'labels': l,\n",
    "                         'peaks': features[['mz', 'rt', 'X1','isotopes', 'adduct', 'pcgroup']],\n",
    "                         'samples': df_s}) \n",
    "    elif one_to_one:\n",
    "        feat, labels, ds_names = reduce_to_one_v_one(feat, df_l, data_file[:-4], lab_to_int_short)\n",
    "        for f, l, n in zip(feat, labels, ds_names):\n",
    "            f_names = list(f.index)\n",
    "#             f_names = [int(name) for name in f_names]\n",
    "#             s = df_s.loc[f_names]\n",
    "            data.append({'study': study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': n,\n",
    "                     'features': f,\n",
    "                     'labels': l,\n",
    "                     'peaks': features[['mz', 'rt', 'X1','isotopes', 'adduct', 'pcgroup']],\n",
    "                     'samples': df_s}) \n",
    "    else:\n",
    "        data.append({'study': study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': data_file[:-4],\n",
    "                     'features': feat,\n",
    "                     'labels': df_l,\n",
    "                     'peaks': features[['mz', 'rt', 'X1','isotopes', 'adduct', 'pcgroup']],\n",
    "                     'samples': df_s})\n",
    "\n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note two clear batches here! \n",
    "# correct number of samples - no duplicates - 90 people\n",
    "# 12-7-18: looks like the order of the samples matches between pos and neg\n",
    "study = 'MTBLS408_data'\n",
    "disease = \"psoriasis\"\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "\n",
    "df_s = pd.read_csv('s_psoriasis.txt', sep='\\t').set_index('Sample Name')\n",
    "df_l = df_s['Source Name']\n",
    "to_replace = {'healthy':0, 'disease':1}\n",
    "# Authors give no data...\n",
    "\n",
    "#ok get my data:\n",
    "dirs = ['neg', 'pos']\n",
    "for folder in dirs:\n",
    "    data_file = 'IPO_aligned_MTBLS408_' + folder + '.csv'\n",
    "    open_file = os.path.join(folder,data_file)\n",
    "    features = pd.read_csv(open_file) \n",
    "    file_names = list(features.iloc[:,9:-3])    \n",
    "    feat = features.iloc[:,9:-3].T.astype('float')\n",
    "    file_names = [fi[:-5] for fi in file_names]\n",
    "    labels = df_l.loc[file_names]\n",
    "    labels = labels.replace(to_replace)\n",
    "    data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': data_file[:-4],\n",
    "                 'features': feat,\n",
    "                 'labels': labels,\n",
    "                 'peaks': features[['mz', 'rt', 'X1','isotopes', 'adduct', 'pcgroup']],\n",
    "                 'samples': df_s.loc[file_names]})\n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ee/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:70: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "/home/ee/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:72: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "/home/ee/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:153: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XCMS-Report-annotated-SingleClass.04jun12_CN_MCI\n",
      "XCMS-Report-annotated-SingleClass.04jun12_CN_AD\n",
      "XCMS-Report-annotated-SingleClass.04jun12_MCI_AD\n",
      "XCMS-Report-annotated-SingleClass.11jun12_CN_MCI\n",
      "XCMS-Report-annotated-SingleClass.11jun12_CN_AD\n",
      "XCMS-Report-annotated-SingleClass.11jun12_MCI_AD\n",
      "XCMS-Report-annotated-SingleClass.27jun12_CN_MCI\n",
      "XCMS-Report-annotated-SingleClass.27jun12_CN_AD\n",
      "XCMS-Report-annotated-SingleClass.27jun12_MCI_AD\n"
     ]
    }
   ],
   "source": [
    "# this study is part xcmsonline and part my processing\n",
    "# random replicates and names \n",
    "# think I finally got names and labels to match\n",
    "# 12-7-18: seems like everything matches in term sof order across datasets...probably can combine!\n",
    "study = 'ST000046_data'\n",
    "disease = \"Alzheimer's\"\n",
    "os.chdir(os.path.join(root,study))\n",
    "files = ['AN000076.txt', 'AN000077.txt', 'AN000078.txt', 'AN000079.txt']\n",
    "data = []\n",
    "label_col = 'Cognitive Status'\n",
    "label_key = {'CN': 0, 'MCI': 2, 'AD':1}\n",
    "for f in files:\n",
    "    features, labels, samples, metabolites = read_metadata(f, label_col, label_key)\n",
    "    if multi_to_single:\n",
    "        labels_new, ds_names = reduce_multi(labels, f[:-4], label_key)\n",
    "        for l, n in zip(labels_new, ds_names):\n",
    "            data.append({'study': study[:-5],\n",
    "                         'disease': disease,\n",
    "                         'data_set': n,\n",
    "                         'features': features,\n",
    "                         'labels': l,\n",
    "                         'peaks': metabolites,\n",
    "                         'samples': samples})\n",
    "    elif one_to_one:\n",
    "        feat, labels_list, ds_names = reduce_to_one_v_one(features, labels, f[:-4], label_key)\n",
    "        for f, l, n in zip(feat, labels_list, ds_names):\n",
    "            f_names = list(f.index)\n",
    "#             f_names = [int(name) for name in f_names]\n",
    "            s = samples.loc[f_names]\n",
    "#             print(f.shape, l.shape, s.shape)\n",
    "            data.append({'study': study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': n,\n",
    "                     'features': f,\n",
    "                     'labels': l,\n",
    "                     'peaks': metabolites,\n",
    "                     'samples': s}) \n",
    "    else:\n",
    "        data.append({'study': study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': f[:-4],\n",
    "                     'features': features,\n",
    "                     'labels': labels,\n",
    "                     'peaks': metabolites,\n",
    "                     'samples': samples}) \n",
    "    \n",
    "dirs = ['20120606_data', '20120613_data', \n",
    "        '20120618_data', '20120620_data', '20120625_data']\n",
    "chroms = ['neg_hilic', 'neg_hilic', 'pos_c18', 'neg_c18', 'pos_c18']\n",
    "maps = ['06jun12.csv', '13jun12.csv', '18jun12.csv', '20jun12.csv', '25jun12.csv']\n",
    "\n",
    "lab_index = list(labels.index)\n",
    "ind_rename = {f:f[7:] for f in lab_index}\n",
    "labels = labels.rename(index=ind_rename)\n",
    "\n",
    "for folder, chrom, name_to_pat_map in zip(dirs, chroms, maps):\n",
    "    data_file = 'IPO_aligned_ST000046_' + folder[:-5] + '_' + chrom + '.csv'\n",
    "    open_file = os.path.join(folder,data_file)\n",
    "    features = pd.read_csv(open_file) \n",
    "    file_names = list(features.iloc[:,8:-3])\n",
    "    m = pd.read_csv(name_to_pat_map).set_index('filename')\n",
    "    m_ind = list(m.index)\n",
    "    m_ind = {name:name[8:10] for name in m_ind}\n",
    "    m = m.rename(m_ind)\n",
    "    m = m.drop_duplicates() # still has a single NaN from a QC or blank due to how dropping works \n",
    "    # map file numbers to patient numbers\n",
    "    file_numb = []\n",
    "    for fi in file_names:\n",
    "        file_numb.append(fi[9:11])\n",
    "    pat_num = m['Patient #'].loc[file_numb].values.astype('int').astype('str')\n",
    "    pat_num = [str(int(n)) for n in pat_num]\n",
    "    label = labels.loc[pat_num]\n",
    "    feat = features.iloc[:,8:-3].T.astype('float')\n",
    "#     print(list(feat.index), label.index) # NEEDD TO REMOVED DUPLICATES\n",
    "\n",
    "    # get the mask for to dereplicate the data....\n",
    "    seen = []\n",
    "    t_f_mask = []\n",
    "    for ele in file_names:\n",
    "        if ele[:11] not in seen:\n",
    "            seen.append(ele[:11])\n",
    "            t_f_mask.append(True)\n",
    "        else:\n",
    "            t_f_mask.append(False)\n",
    "    feat = feat[t_f_mask]\n",
    "    label = label[t_f_mask]\n",
    "    feat = feat[label.notnull().values]\n",
    "    label = label[label.notnull().values]\n",
    "    index_rename = dict(zip(list(label.index),list(feat.index)))\n",
    "    label = label.rename(index_rename)\n",
    "    if multi_to_single:\n",
    "        labels_new, ds_names = reduce_multi(label, data_file[:-4], label_key)\n",
    "        for l, n in zip(labels_new, ds_names):\n",
    "            data.append({'study': study[:-5],\n",
    "                         'disease': disease,\n",
    "                         'data_set': n,\n",
    "                         'features': feat,\n",
    "                         'labels': l,\n",
    "                         'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'npeaks', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                         'samples': m}) \n",
    "    elif one_to_one:\n",
    "        feat_list, labels_list, ds_names = reduce_to_one_v_one(feat, label, data_file[:-4], label_key)\n",
    "        for f, l, n in zip(feat_list, labels_list, ds_names):\n",
    "            f_names = list(f.index)\n",
    "#             f_names = [int(name) for name in f_names]\n",
    "            f_names = [ele[9:11] for ele in f_names]\n",
    "            s = m.loc[f_names]\n",
    "            data.append({'study': study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': n,\n",
    "                     'features': f,\n",
    "                     'labels': l,\n",
    "                     'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'npeaks', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                     'samples': s}) \n",
    "    else:\n",
    "        data.append({'study': study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': data_file[:-4],\n",
    "                     'features': feat,\n",
    "                     'labels': label,\n",
    "                     'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'npeaks', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                     'samples': m}) \n",
    "    \n",
    "# There are the XCMSonline processed files\n",
    "dirs = ['20120604_data', '20120611_data', '20120627_data']\n",
    "maps = ['04jun12.csv', '11jun12.csv', '27jun12.csv']\n",
    "for folder, name_to_pat_map in zip(dirs, maps):\n",
    "    data_file = 'XCMS-Report-annotated-SingleClass.xlsx'\n",
    "    open_file = os.path.join(folder,data_file)\n",
    "    features = pd.read_excel(open_file) \n",
    "    file_names = list(features.iloc[:,10:-3])\n",
    "    m = pd.read_csv(name_to_pat_map).set_index('filename')\n",
    "    m_ind = list(m.index)\n",
    "    m_ind_short = [name[8:10] for name in m_ind]\n",
    "    m_ind_d = {name:name[8:10] for name in m_ind}\n",
    "    t_f_mask = []\n",
    "    seen = []\n",
    "    for ind in list(m_ind_short):\n",
    "        if ind not in seen:\n",
    "            seen.append(ind)\n",
    "            t_f_mask.append(True)\n",
    "        else:\n",
    "            t_f_mask.append(False)\n",
    "    m = m.rename(m_ind_d)\n",
    "    m = m[t_f_mask]\n",
    "    \n",
    "    # map file numbers to patient numbers\n",
    "    file_numb = []\n",
    "    for fi in file_names:\n",
    "        file_numb.append(fi[8:10])\n",
    "    pat_num = m['Patient #'].loc[file_numb].values.astype('int').astype('str')\n",
    "    pat_num = [str(int(n)) for n in pat_num]\n",
    "    label = labels.loc[pat_num]\n",
    "    feat = features.iloc[:,10:-3].T.astype('float')\n",
    "#     # get the mask for to dereplicate the data....\n",
    "    seen = []\n",
    "    t_f_mask = []\n",
    "    file_names = list(label.index)\n",
    "    for ele in file_names:\n",
    "        if ele not in seen:\n",
    "#         if ele[:10] not in seen:\n",
    "#             seen.append(ele[:10])\n",
    "            seen.append(ele)\n",
    "            t_f_mask.append(True)\n",
    "        else:\n",
    "            t_f_mask.append(False)\n",
    "    feat = feat[t_f_mask]\n",
    "    label = label[t_f_mask]\n",
    "    feat = feat[label.notnull().values]\n",
    "    label = label[label.notnull().values]\n",
    "    index_rename = dict(zip(list(label.index),list(feat.index)))\n",
    "    label = label.rename(index_rename)\n",
    "    if multi_to_single:\n",
    "        labels_new, ds_names = reduce_multi(label, data_file[:-4]+name_to_pat_map[:-4], label_key)\n",
    "        for l, n in zip(labels_new, ds_names):\n",
    "            data.append({'study': study[:-5],\n",
    "                         'disease': disease,\n",
    "                         'data_set': n,\n",
    "                         'features': feat,\n",
    "                         'labels': l,\n",
    "                         'peaks': features[['mzmed', 'mzmin', 'mzmax', 'rtmed', 'rtmin', 'rtmax', 'npeaks', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                         'samples': m}) \n",
    "    elif one_to_one:\n",
    "        feat_list, labels_list, ds_names = reduce_to_one_v_one(feat, label, data_file[:-4]+name_to_pat_map[:-4], label_key)\n",
    "        for f, l, n in zip(feat_list, labels_list, ds_names):\n",
    "            f_names = list(f.index)\n",
    "#             f_names = [int(name) for name in f_names]\n",
    "            f_names = [ele[8:10] for ele in f_names]\n",
    "            s = m.loc[f_names]\n",
    "            print(n)\n",
    "            data.append({'study': study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': n,\n",
    "                     'features': f,\n",
    "                     'labels': l,\n",
    "                     'peaks': features[['mzmed', 'mzmin', 'mzmax', 'rtmed', 'rtmin', 'rtmax', 'npeaks', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                     'samples': s}) \n",
    "    else:\n",
    "        data.append({'study': study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': data_file[:-4]+name_to_pat_map[:-4],\n",
    "                     'features': feat,\n",
    "                     'labels': label,\n",
    "                     'peaks': features[['mzmed', 'mzmin', 'mzmax', 'rtmed', 'rtmin', 'rtmax', 'npeaks', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                     'samples': m}) \n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STUDIES WITH RAW DATA THAT I PROCESSED!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no technical replicates. \n",
    "# label order appears to match the feature order\n",
    "# NO CLUE HOW TO MAP FILE NAMES FROM PLASMA TO URINE - CANNOT COMBINE\n",
    "study = 'Feng_data'\n",
    "disease = 'coronary heart disease'\n",
    "os.chdir(os.path.join(root,study))\n",
    "files = ['srep22525-s2.xls', 'srep22525-s4.xls']\n",
    "analysis_type = ['plasma', 'urine']\n",
    "data = []\n",
    "label_dict = {}\n",
    "for f, t in zip(files,analysis_type):\n",
    "    df = pd.read_excel(f, skiprows=1) # NOTE THIS IS JUST THE PLASMA DATA\n",
    "    if t == 'plasma':\n",
    "        key = 'CD'\n",
    "    else:\n",
    "        key = 'ZSL'\n",
    "    labels = pd.DataFrame(df.columns[1:].str.contains(key), index=df.iloc[:,1:].T.index)\n",
    "    samples = pd.DataFrame(index=df.iloc[:,1:].T.index)\n",
    "    label_dict[t] = [labels, samples]\n",
    "    data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': t+'all_author',\n",
    "                 'features': df.iloc[:,1:].T.astype('float'),\n",
    "                 'labels': labels,\n",
    "                 'peaks': df.iloc[:,[0,]],\n",
    "                 'samples': samples})\n",
    "\n",
    "dirs = ['serum', 'urine']\n",
    "for folder in dirs:\n",
    "    if 'serum' in folder:\n",
    "        labels, samples = label_dict['plasma']\n",
    "    else:\n",
    "        labels, samples = label_dict['urine']\n",
    "    for f in os.listdir(folder):\n",
    "        if f[-3:] == 'csv':\n",
    "            analysis = folder +'_'+f[:-4]\n",
    "            open_file = os.path.join(folder,f)\n",
    "            features = pd.read_csv(open_file) \n",
    "            file_names = list(features.iloc[:,9:-3])  \n",
    "            file_names = [f[:-6]+f[-8:-6] for f in file_names]\n",
    "            label = labels.loc[file_names]\n",
    "            data.append({'study': study[:-5],\n",
    "                         'disease': disease,\n",
    "                         'data_set': analysis,\n",
    "                         'features': features.iloc[:,9:-3].T.astype('float'),\n",
    "                         'labels': label,\n",
    "                         'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                         'samples': samples.loc[file_names]})      #using the last samples df as well\n",
    "            \n",
    "dirs = ['serum_onebatch', 'urine_onebatch']   \n",
    "for folder in dirs:\n",
    "    if 'serum' in folder:\n",
    "        labels, samples = label_dict['plasma']\n",
    "    else:\n",
    "        labels, samples = label_dict['urine']\n",
    "    for f in os.listdir(folder):\n",
    "        if f[-3:] == 'csv':\n",
    "            analysis = folder +'_'+f[:-4]\n",
    "            open_file = os.path.join(folder,f)\n",
    "            features = pd.read_csv(open_file) \n",
    "            file_names = list(features.iloc[:,9:-3])  \n",
    "            file_names = [f[:-6]+f[-8:-6] for f in file_names]\n",
    "            file_names = [fi for fi in file_names if 'QC' not in fi]\n",
    "            label = labels.loc[file_names]\n",
    "            file_names_2 = [f[:-2]+'.mzXML' for f in file_names]\n",
    "            feat = features.iloc[:,9:-3].T.astype('float')\n",
    "            feat = feat.loc[file_names_2]\n",
    "            data.append({'study': study[:-5],\n",
    "                         'disease': disease,\n",
    "                         'data_set': analysis,\n",
    "                         'features': feat,\n",
    "                         'labels': label,\n",
    "                         'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                         'samples': samples.loc[file_names]})  \n",
    "\n",
    "pickle.dump(data, open('FengEtAl.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also some people with same subject name have different sex....\n",
    "# LOOK AT PAPER BEFORE DEALING ANYMORE WITH THIS\n",
    "# looks like the label order matches the sample order in the features \n",
    "# 12-7-18: looks like the order is good and you can combine features! \n",
    "study = 'ST000763_data'\n",
    "disease = 'scleroderma PAH'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "files = ['AN001201.txt', 'AN001202.txt'] #, 'AN001203.txt', 'AN001204.txt']\n",
    "# this is pos-Qtof, neg-Qtof, pos-3tof, neg-3tof\n",
    "# 1201 and 1202 are the untargeted samples, 1203 and 1204 are the msms and match to the pos and neg folder\n",
    "label_col = 'Group'\n",
    "label_key = {'Healthy': 0, 'PAH': 4,'Normal Pressures':2, 'Borderline Pressures': 3, 'LowRisk':1} \n",
    "# 1) healthy controls, 2) patients with scleroderma at low risk for pulmonary hypertension, \n",
    "# 3) pateints with scleroderma at high risk for pulmonary hypertension who underwent a catheterization \n",
    "# and were found to have normal pressures, borderline elevated pressures or pulmonary arterial hypertensino (PAH).\n",
    "for f in files:\n",
    "    features, labels, samples, metabolites = read_metadata(f, label_col, label_key)\n",
    "    if multi_to_single:\n",
    "        labels_new, ds_names = reduce_multi(labels, f[:-4], label_key)\n",
    "        for l, n in zip(labels_new, ds_names):\n",
    "            data.append({'study': study[:-5],\n",
    "                         'disease': disease,\n",
    "                         'data_set': n,\n",
    "                         'features': features,\n",
    "                         'labels': l,\n",
    "                         'peaks': metabolites,\n",
    "                         'samples': samples}) \n",
    "    elif one_to_one:\n",
    "        feat_list, labels_list, ds_names = reduce_to_one_v_one(features, labels, f[:-4], label_key)\n",
    "        for f, l, n in zip(feat_list, labels_list, ds_names):\n",
    "            f_names = list(f.index)\n",
    "#             f_names = [int(name) for name in f_names]\n",
    "            s = samples.loc[f_names]\n",
    "            data.append({'study': study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': n,\n",
    "                     'features': f,\n",
    "                     'labels': l,\n",
    "                     'peaks': metabolites,\n",
    "                     'samples': s}) \n",
    "    else:\n",
    "        data.append({'study': study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': f[:-4],\n",
    "                     'features': features,\n",
    "                     'labels': labels,\n",
    "                     'peaks': metabolites,\n",
    "                     'samples': samples})   \n",
    "# get my data:\n",
    "dirs = ['untargeted']\n",
    "# dirs = ['neg', 'pos', 'untargeted']\n",
    "for folder in dirs:\n",
    "    for fi in os.listdir(folder):\n",
    "        if fi[-3:] == 'csv':\n",
    "            open_file = os.path.join(folder,fi)\n",
    "            features = pd.read_csv(open_file) \n",
    "            file_names = list(features.iloc[:,9:-3])  \n",
    "            if folder == 'untargeted':\n",
    "                new_ind = {f:f[-16:-7] for f in file_names}\n",
    "                file_names = [f[-16:-7] for f in file_names]\n",
    "            else:\n",
    "                new_ind = {f:f[31:-5] for f in file_names}\n",
    "                file_names = [f[31:-5] for f in file_names]\n",
    "            # this next line only works because all the above files make 'labels' dfs that are the same, so just using the last:\n",
    "            label = labels.loc[file_names]\n",
    "            features_filt = features.iloc[:,9:-3].T.astype('float')\n",
    "            features_filt = features_filt.rename(index=new_ind)\n",
    "            if multi_to_single:\n",
    "                labels_new, ds_names = reduce_multi(label, fi[:-4], label_key)\n",
    "                for l, n in zip(labels_new, ds_names):\n",
    "                    data.append({'study': study[:-5],\n",
    "                                 'disease': disease,\n",
    "                                 'data_set': n,\n",
    "                                 'features': features_filt,\n",
    "                                 'labels': l,\n",
    "                                 'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                                 'samples': samples.loc[file_names]}) \n",
    "            elif one_to_one:\n",
    "                feat_list, labels_list, ds_names = reduce_to_one_v_one(features_filt, label, fi[:-4], label_key)\n",
    "                for f, l, n in zip(feat_list, labels_list, ds_names):\n",
    "                    f_names = list(f.index)\n",
    "                    s = samples.loc[f_names]\n",
    "                    data.append({'study': study[:-5],\n",
    "                             'disease': disease,\n",
    "                             'data_set': n,\n",
    "                             'features': f,\n",
    "                             'labels': l,\n",
    "                             'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                             'samples': s})\n",
    "            else:\n",
    "                data.append({'study': study[:-5],\n",
    "                             'disease': disease,\n",
    "                             'data_set': fi[:-4],\n",
    "                             'features': features_filt,\n",
    "                             'labels': label,\n",
    "                             'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                             'samples': samples.loc[file_names]})      #using the last samples df as well\n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2723: DtypeWarning: Columns (850) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#  data has triplicates\n",
    "# 12-7-18: looks like you can probably combine the datasets!\n",
    "study = 'ST000578_data'\n",
    "disease = 'Malaria (P. vivax)'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "files = ['AN000888.txt', 'AN000889.txt']\n",
    "label_col = 'Current Malaria Infection'\n",
    "label_key = {'P.Vivax': 1, 'None': 0}\n",
    "columns = ['C18', 'AE']\n",
    "label_dir = {}\n",
    "sample_dir = {}\n",
    "for f, col in zip(files, columns):\n",
    "    features, labels, samples, metabolites = read_metadata(f, label_col, label_key)\n",
    "    labels.index = labels.index.astype('string')\n",
    "    samples.index = samples.index.astype('string')\n",
    "    labels = labels[samples['Gender'] != 'N/A']\n",
    "    samples = samples[samples['Gender'] != 'N/A']\n",
    "    file_names = list(labels.index)\n",
    "    features = features.loc[file_names]\n",
    "    label_dir[col] = labels\n",
    "    sample_dir[col] = samples\n",
    "    data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': f[:-4],\n",
    "                 'features': features,\n",
    "                 'labels': labels,\n",
    "                 'peaks': metabolites,\n",
    "                 'samples': samples})\n",
    "    \n",
    "metadata = pd.read_csv('metadata.csv').set_index('Instrument.File.Name')\n",
    "metadata = metadata.dropna(subset=['Aliquot.Id'])\n",
    "metadata['Aliquot.Id'] = metadata['Aliquot.Id'].astype('int').astype('string')\n",
    "dirs = ['AE', 'C18']\n",
    "for folder in dirs:\n",
    "    #VT_140317_103.mzML map this to the 2008...s, 2008603 (this is index in label dir folder)\n",
    "    # ok - map names from features to IDs in the matadata, take these IDs to get the labels from label_dir[folder].loc[names]\n",
    "    data_file = 'IPO_aligned_ST000578_' + folder + '.csv'\n",
    "    open_file = os.path.join(folder,data_file)\n",
    "    features = pd.read_csv(open_file) \n",
    "    file_names = list(features.iloc[:,9:-3:3])  # using the slicing to drop replicates...could probably keep and drop later as well\n",
    "    file_names = [f[:-5] for f in file_names]\n",
    "    file_names = metadata['Aliquot.Id'].loc[file_names]\n",
    "    file_names = file_names.drop_duplicates()\n",
    "    file_names = file_names.dropna()\n",
    "    mapped_file_names = list(file_names)\n",
    "    labels = label_dir[folder].loc[mapped_file_names]\n",
    "    labels = labels.dropna()\n",
    "    files_for_feats = list(labels.index)\n",
    "    new_files = []\n",
    "    for fi in files_for_feats:\n",
    "        new_files.append(list(file_names[file_names == fi].index)[0])\n",
    "    \n",
    "    files_for_feats = [f+'.mzML' for f in new_files]\n",
    "    feat = features.iloc[:,9:-3:3].T.astype('float')\n",
    "    feat = feat.loc[files_for_feats]\n",
    "    data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': data_file[:-4],\n",
    "                 'features': feat,\n",
    "                 'labels': labels,\n",
    "                 'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                 'samples': sample_dir[folder]}) \n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all good, looks like only 299 samples and none are duplicates and labels look to be in correct order\n",
    "study = 'ST000396_data'\n",
    "disease = 'lung cancer - non-small-cell lung cancer (adenocarcinoma, etc)'\n",
    "os.chdir(os.path.join(root,study))\n",
    "f = 'AN000633.txt'\n",
    "data = []\n",
    "label_col = 'Diagnosis'\n",
    "label_key = {'-': 0, 'Adenocarcinoma': 1, 'Other NSCLC': 1, 'Squamous cell':1}\n",
    "features, labels, samples, metabolites = read_metadata(f, label_col, label_key)\n",
    "data.append({'study': study[:-5],\n",
    "             'disease': disease,\n",
    "             'data_set': f[:-4],\n",
    "             'features': features,\n",
    "             'labels': labels,\n",
    "             'peaks': metabolites,\n",
    "             'samples': samples}) \n",
    "\n",
    "f = 'IPO_aligned_ST000396.csv'\n",
    "features = pd.read_csv(f)\n",
    "file_names = list(features.iloc[:,9:-3])\n",
    "file_names = [fi[1:-7] for fi in file_names]\n",
    "# first get labels for all samples based on labels given in AN file by authors\n",
    "label = labels.loc[file_names]\n",
    "data.append({'study': study[:-5],\n",
    "             'disease': disease,\n",
    "             'data_set': f[:-4],\n",
    "             'features': features.iloc[:,9:-3].T.astype('float'),\n",
    "             'labels': label,\n",
    "             'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1', 'isotopes', 'adduct', 'pcgroup']],\n",
    "             'samples': samples}) \n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looks like no replicates! labels are in the correct order with the samples\n",
    "# 12-7-18: looks like ordering is good so can mix multiple datasets!\n",
    "study = 'ST000392_data'\n",
    "disease = 'Lung cancer'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "f = 'AN000628.txt'\n",
    "label_col = 'Disease State'\n",
    "label_key = {'control': 0, 'cancer': 1, '-': 'NaN'}\n",
    "features, labels, samples, metabolites = read_metadata(f, label_col, label_key)\n",
    "labels = labels[labels['Disease State']!='NaN'] # remove QCs\n",
    "file_names = list(labels.index)\n",
    "features = features.loc[file_names] #filter the QCs out of the features\n",
    "# now split the plasma and serum . first get file names \n",
    "plasma_names = list(samples[samples['Organ']=='Plasma '].index)\n",
    "serum_names = list(samples[samples['Organ']=='Serum '].index)\n",
    "# get labels for plasma and serum\n",
    "labels_p = labels.loc[plasma_names]\n",
    "labels_s = labels.loc[serum_names]\n",
    "# split features:\n",
    "features_p = features.loc[plasma_names]\n",
    "features_s = features.loc[serum_names]\n",
    "data.append({'study': study[:-5],\n",
    "             'disease': disease,\n",
    "             'data_set': f[:-4]+'_plasma',\n",
    "             'features': features_p,\n",
    "             'labels': labels_p,\n",
    "             'peaks': metabolites,\n",
    "             'samples': samples.loc[plasma_names]}) \n",
    "data.append({'study': study[:-5],\n",
    "             'disease': disease,\n",
    "             'data_set': f[:-4]+'_serum',\n",
    "             'features': features_s,\n",
    "             'labels': labels_s,\n",
    "             'peaks': metabolites,\n",
    "             'samples': samples.loc[serum_names]})\n",
    "\n",
    "dirs = ['plasma_data', 'serum_data']\n",
    "for folder in dirs:\n",
    "    files = os.listdir(folder)\n",
    "    for fi in files:\n",
    "        if 'IPO_aligned' in fi:\n",
    "            data_file = fi\n",
    "        else:\n",
    "            continue\n",
    "        open_file = os.path.join(folder,data_file)\n",
    "        features = pd.read_csv(open_file)   \n",
    "        file_names = list(features.iloc[:,9:-3])\n",
    "        file_names = [f[1:-7] for f in file_names]\n",
    "        # first get labels for all samples based on labels given in AN file by authors\n",
    "        label = labels.loc[file_names]\n",
    "        data.append({'study': study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': data_file[:-4],\n",
    "                     'features': features.iloc[:,9:-3].T.astype('float'),\n",
    "                     'labels': label,\n",
    "                     'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                     'samples': samples}) \n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like no replicates, fixed label order.\n",
    "study = 'ST000389_data'\n",
    "disease = 'Lung cancer'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "label_col = 'Group'\n",
    "label_key = {'Benign': 0, 'Cancer': 1}\n",
    "f = 'AN000625.txt'\n",
    "features, labels, samples, metabolites = read_metadata(f, label_col, label_key)\n",
    "labels = labels.loc[features.index]\n",
    "data.append({'study': study[:-5],\n",
    "             'disease': disease,\n",
    "             'data_set': f[:-4]+'_GC',\n",
    "             'features': features,\n",
    "             'labels': labels,\n",
    "             'peaks': metabolites,\n",
    "             'samples': samples}) \n",
    "# get my features\n",
    "dirs = ['gc']\n",
    "for folder in dirs:\n",
    "    data_file = 'IPO_aligned_ST000388_GC.csv'\n",
    "    open_file = os.path.join(folder,data_file)\n",
    "    features = pd.read_csv(open_file)\n",
    "    # turns out this datalist has like 40ish more samples that lack labels, filtering these\n",
    "    file_names = list(features.iloc[:,9:-3])\n",
    "    file_names = [f[1:-7] for f in file_names]\n",
    "    # first get labels for all samples based on labels given in AN file by authors:\n",
    "    labels = labels.loc[file_names]\n",
    "    # now find what doesnt have a label and remove these\n",
    "    mask = labels.isnull()\n",
    "    labels = labels[mask['Group']==False]\n",
    "    # now get the feature data and do the same\n",
    "    feat_data = features.iloc[:,9:-3].T.astype('float')\n",
    "    feat_names = list(feat_data.index)\n",
    "    rename_inx = {feat_name:feat_name[1:-7] for feat_name in feat_names} #need to make a old name:new name mapping dict\n",
    "    feat_data = feat_data.rename(index=rename_inx) # renames indicies\n",
    "    feat_data = feat_data[mask['Group']==False] # now mask based on samples with vs without labels\n",
    "    data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': data_file[:-4],\n",
    "                 'features': feat_data,\n",
    "                 'labels': labels,\n",
    "                 'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                 'samples': samples})\n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed data order, and size and no replicates\n",
    "# author data 94 samples, I have 94 though...I think this is related to 388\n",
    "study = 'ST000388_data'\n",
    "disease = 'Lung cancer'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "label_col = 'Group'\n",
    "label_key = {'Benign': 0, 'Cancer': 1}\n",
    "f = 'AN000624.txt'\n",
    "features, labels, samples, metabolites = read_metadata(f, label_col, label_key)\n",
    "labels = labels.loc[features.index]\n",
    "data.append({'study': study[:-5],\n",
    "             'disease': disease,\n",
    "             'data_set': f[:-4]+'_LC',\n",
    "             'features': features,\n",
    "             'labels': labels,\n",
    "             'peaks': metabolites,\n",
    "             'samples': samples}) \n",
    "# get my features\n",
    "dirs = ['lc']\n",
    "for folder in dirs:\n",
    "    data_file = 'IPO_aligned_ST000388_LC.csv'\n",
    "    open_file = os.path.join(folder,data_file)\n",
    "    features = pd.read_csv(open_file)\n",
    "    file_names = list(features.iloc[:,9:-3])\n",
    "    file_names = [f.replace('_HILIC_Pos_', '_')[:-5] for f in file_names]\n",
    "    labels = labels.loc[file_names]\n",
    "    labels = labels.dropna()   \n",
    "    \n",
    "    feat_files = [f.replace('_','_HILIC_Pos_')+'.mzML' for f in list(labels.index)]\n",
    "    feat = features.iloc[:,9:-3].T.astype('float')\n",
    "    feat = feat.loc[feat_files]\n",
    "    data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': data_file[:-4],\n",
    "                 'features': feat,\n",
    "                 'labels': labels,\n",
    "                 'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                 'samples': samples})\n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n",
      "['140813amssa25_1', '140814amssa37_1', '140814amssa14_1', '140814amssa35_1', '140813amssa35_1', '140813amssa13_1', '140813amssa42_1', '140814amssa24_1', '140813amssa38_1', '140813amssa47_1', '140813amssa03_1', '140814amssa50_1', '140813amssa19_1', '140814amssa07_1', '140813amssa11_1', '140813amssa49_1', '140814amssa39_1', '140814amssa23_1', '140813amssa16_1', '140814amssa27_1', '140813amssa10_1', '140814amssa11_1', '140813amssa01_1', '140814amssa49_1', '140813amssa46_1', '140814amssa19_1', '140813amssa40_1', '140814amssa08_1', '140813amssa05_1', '140813amssa37_1', '140814amssa05_1', '140814amssa21_1', '140814amssa10_1', '140813amssa07_1', '140813amssa09_1', '140814amssa22_1', '140813amssa29_1', '140814amssa16_1', '140814amssa12_1', '140814amssa09_1', '140813amssa02_1', '140813amssa18_1', '140814amssa31_1', '140813amssa39_1', '140814amssa38_1', '140814amssa01_1', '140813amssa23_1', '140813amssa15_1', '140814amssa02_1', '140813amssa31_1', '140814amssa15_1', '140813amssa41_1', '140813amssa17_1', '140813amssa43_1', '140813amssa14_1', '140814amssa42_1', '140813amssa36_1', '140814amssa25_1', '140814amssa26_1', '140813amssa48_1', '140814amssa41_1', '140814amssa20_1', '140813amssa33_1', '140814amssa47_1', '140813amssa21_1', '140813amssa44_1', '140813amssa28_1', '140813amssa22_1', '140814amssa32_1', '140814amssa45_1', '140814amssa30_1', '140814amssa33_1', '140814amssa03_1', '140813amssa24_1', '140813amssa27_1', '140814amssa28_1', '140814amssa34_1', '140813amssa26_1', '140813amssa12_1', '140814amssa13_1', '140813amssa20_1', '140814amssa06_1', '140813amssa08_1', '140813amssa45_1', '140813amssa50_1', '140814amssa29_1', '140814amssa44_1', '140814amssa46_1', '140813amssa32_1', '140813amssa04_1', '140814amssa48_1', '140814amssa17_1']\n",
      "94\n",
      "['Inj076_SA007_KimUrine.d', 'Inj057_SA008_KimUrine.d', 'Inj009_SA009_KimUrine.d', 'Inj006_SA010_KimUrine.d', 'Inj068_SA011_KimUrine.d', 'Inj004_SA012_KimUrine.d', 'Inj007_SA013_KimUrine.d', 'Inj049_SA014_KimUrine.d', 'Inj090_SA015_KimUrine.d', 'Inj034_SA016_KimUrine.d', 'Inj077_SA017_KimUrine.d', 'Inj020_SA018_KimUrine.d', 'Inj051_SA019_KimUrine.d', 'Inj066_SA020_KimUrine.d', 'Inj056_SA021_KimUrine.d', 'Inj021_SA022_KimUrine.d', 'Inj036_SA023_KimUrine.d', 'Inj046_SA024_KimUrine.d', 'Inj028_SA025_KimUrine.d', 'Inj069_SA026_KimUrine.d', 'Inj011_SA027_KimUrine.d', 'Inj084_SA028_KimUrine.d', 'Inj094_SA029_KimUrine.d', 'Inj052_SA030_KimUrine.d', 'Inj055_SA031_KimUrine.d', 'Inj058_SA032_KimUrine.d', 'Inj053_SA033_KimUrine.d', 'Inj086_SA034_KimUrine.d', 'Inj085_SA035_KimUrine.d', 'Inj092_SA036_KimUrine.d', 'Inj012_SA037_KimUrine.d', 'Inj061_SA038_KimUrine.d', 'Inj038_SA039_KimUrine.d', 'Inj033_SA040_KimUrine.d', 'Inj019_SA041_KimUrine.d', 'Inj010_SA042_KimUrine.d', 'Inj022_SA043_KimUrine.d', 'Inj079_SA044_KimUrine.d', 'Inj027_SA045_KimUrine.d', 'Inj014_SA046_KimUrine.d', 'Inj048_SA047_KimUrine.d', 'Inj054_SA048_KimUrine.d', 'Inj003_SA049_KimUrine.d', 'Inj060_SA050_KimUrine.d', 'Inj071_SA051_KimUrine.d', 'Inj016_SA052_KimUrine.d', 'Inj065_SA053_KimUrine.d', 'Inj035_SA054_KimUrine.d', 'Inj013_SA055_KimUrine.d', 'Inj030_SA056_KimUrine.d', 'Inj088_SA057_KimUrine.d', 'Inj083_SA058_KimUrine.d', 'Inj018_SA059_KimUrine.d', 'Inj082_SA060_KimUrine.d', 'Inj089_SA061_KimUrine.d', 'Inj025_SA062_KimUrine.d', 'Inj078_SA063_KimUrine.d', 'Inj045_SA064_KimUrine.d', 'Inj032_SA065_KimUrine.d', 'Inj064_SA066_KimUrine.d', 'Inj063_SA067_KimUrine.d', 'Inj037_SA068_KimUrine.d', 'Inj017_SA069_KimUrine.d', 'Inj070_SA070_KimUrine.d', 'Inj039_SA071_KimUrine.d', 'Inj002_SA072_KimUrine.d', 'Inj074_SA073_KimUrine.d', 'Inj081_SA074_KimUrine.d', 'Inj047_SA075_KimUrine.d', 'Inj072_SA076_KimUrine.d', 'Inj059_SA077_KimUrine.d', 'Inj023_SA078_KimUrine.d', 'Inj044_SA079_KimUrine.d', 'Inj091_SA080_KimUrine.d', 'Inj073_SA081_KimUrine.d', 'Inj062_SA082_KimUrine.d', 'Inj005_SA083_KimUrine.d', 'Inj015_SA084_KimUrine.d', 'Inj026_SA085_KimUrine.d', 'Inj024_SA086_KimUrine.d', 'Inj087_SA087_KimUrine.d', 'Inj075_SA088_KimUrine.d', 'Inj031_SA089_KimUrine.d', 'Inj029_SA090_KimUrine.d', 'Inj093_SA091_KimUrine.d', 'Inj080_SA092_KimUrine.d', 'Inj067_SA093_KimUrine.d', 'Inj040_SA094_KimUrine.d', 'Inj041_SA095_KimUrine.d', 'Inj008_SA096_KimUrine.d', 'Inj050_SA097_KimUrine.d', 'Inj042_SA098_KimUrine.d', 'Inj001_SA099_KimUrine.d', 'Inj043_SA100_KimUrine.d']\n",
      "94\n",
      "['Inj076_SA007_KimUrine.d', 'Inj057_SA008_KimUrine.d', 'Inj009_SA009_KimUrine.d', 'Inj006_SA010_KimUrine.d', 'Inj068_SA011_KimUrine.d', 'Inj004_SA012_KimUrine.d', 'Inj007_SA013_KimUrine.d', 'Inj049_SA014_KimUrine.d', 'Inj090_SA015_KimUrine.d', 'Inj034_SA016_KimUrine.d', 'Inj077_SA017_KimUrine.d', 'Inj020_SA018_KimUrine.d', 'Inj051_SA019_KimUrine.d', 'Inj066_SA020_KimUrine.d', 'Inj056_SA021_KimUrine.d', 'Inj021_SA022_KimUrine.d', 'Inj036_SA023_KimUrine.d', 'Inj046_SA024_KimUrine.d', 'Inj028_SA025_KimUrine.d', 'Inj069_SA026_KimUrine.d', 'Inj011_SA027_KimUrine.d', 'Inj084_SA028_KimUrine.d', 'Inj094_SA029_KimUrine.d', 'Inj052_SA030_KimUrine.d', 'Inj055_SA031_KimUrine.d', 'Inj058_SA032_KimUrine.d', 'Inj053_SA033_KimUrine.d', 'Inj086_SA034_KimUrine.d', 'Inj085_SA035_KimUrine.d', 'Inj092_SA036_KimUrine.d', 'Inj012_SA037_KimUrine.d', 'Inj061_SA038_KimUrine.d', 'Inj038_SA039_KimUrine.d', 'Inj033_SA040_KimUrine.d', 'Inj019_SA041_KimUrine.d', 'Inj010_SA042_KimUrine.d', 'Inj022_SA043_KimUrine.d', 'Inj079_SA044_KimUrine.d', 'Inj027_SA045_KimUrine.d', 'Inj014_SA046_KimUrine.d', 'Inj048_SA047_KimUrine.d', 'Inj054_SA048_KimUrine.d', 'Inj003_SA049_KimUrine.d', 'Inj060_SA050_KimUrine.d', 'Inj071_SA051_KimUrine.d', 'Inj016_SA052_KimUrine.d', 'Inj065_SA053_KimUrine.d', 'Inj035_SA054_KimUrine.d', 'Inj013_SA055_KimUrine.d', 'Inj030_SA056_KimUrine.d', 'Inj088_SA057_KimUrine.d', 'Inj083_SA058_KimUrine.d', 'Inj018_SA059_KimUrine.d', 'Inj082_SA060_KimUrine.d', 'Inj089_SA061_KimUrine.d', 'Inj025_SA062_KimUrine.d', 'Inj078_SA063_KimUrine.d', 'Inj045_SA064_KimUrine.d', 'Inj032_SA065_KimUrine.d', 'Inj064_SA066_KimUrine.d', 'Inj063_SA067_KimUrine.d', 'Inj037_SA068_KimUrine.d', 'Inj017_SA069_KimUrine.d', 'Inj070_SA070_KimUrine.d', 'Inj039_SA071_KimUrine.d', 'Inj002_SA072_KimUrine.d', 'Inj074_SA073_KimUrine.d', 'Inj081_SA074_KimUrine.d', 'Inj047_SA075_KimUrine.d', 'Inj072_SA076_KimUrine.d', 'Inj059_SA077_KimUrine.d', 'Inj023_SA078_KimUrine.d', 'Inj044_SA079_KimUrine.d', 'Inj091_SA080_KimUrine.d', 'Inj073_SA081_KimUrine.d', 'Inj062_SA082_KimUrine.d', 'Inj005_SA083_KimUrine.d', 'Inj015_SA084_KimUrine.d', 'Inj026_SA085_KimUrine.d', 'Inj024_SA086_KimUrine.d', 'Inj087_SA087_KimUrine.d', 'Inj075_SA088_KimUrine.d', 'Inj031_SA089_KimUrine.d', 'Inj029_SA090_KimUrine.d', 'Inj093_SA091_KimUrine.d', 'Inj080_SA092_KimUrine.d', 'Inj067_SA093_KimUrine.d', 'Inj040_SA094_KimUrine.d', 'Inj041_SA095_KimUrine.d', 'Inj008_SA096_KimUrine.d', 'Inj050_SA097_KimUrine.d', 'Inj042_SA098_KimUrine.d', 'Inj001_SA099_KimUrine.d', 'Inj043_SA100_KimUrine.d']\n"
     ]
    }
   ],
   "source": [
    "# order seems fine - CHECK for duplicates\n",
    "# 92 samples\n",
    "study = 'ST000381_data'\n",
    "disease = 'interstitial cystitis/painful bladder syndrome'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "# ok the IPO_aligned matches with AN000015, AN16 and 17 are for the nonexistant LC data\n",
    "# YOU CANNOT DL the LC data sadly\n",
    "files = ['AN000615.txt', 'AN000616.txt', 'AN000617.txt'] \n",
    "label_col = ['Factor3', 'SubCondition', 'SubCondition']\n",
    "label_key = {'n/a': 0, 'MODEST': 1, 'INTERMEDIATE':1, 'SEVERE':1}\n",
    "\n",
    "for f, col in zip(files,label_col):\n",
    "    features, labels, samples, metabolites = read_metadata(f, col, label_key, debug=False)\n",
    "    print(len(list(features.index)))\n",
    "    print(list(features.index))\n",
    "    data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': f[:-4],\n",
    "                 'features': features,\n",
    "                 'labels': labels,\n",
    "                 'peaks': metabolites,\n",
    "                 'samples': samples})   \n",
    "# ONLY FOR DATA WITHOUT LC SAMPLES FROM AUTHORS... it grabs metadata from the first\n",
    "# element of data above which is the GC data\n",
    "# THIS DATA MATCHES WITH AN000615 IN TERMS OF NUMBER OF SAMPLES\n",
    "gc_labels = data[0]['labels']\n",
    "f = 'IPO_aligned_ST000381_pos.csv'\n",
    "features = pd.read_csv(f)\n",
    "file_names = list(features.iloc[:,9:-3])\n",
    "file_names = [fi[1:-7] for fi in file_names]\n",
    "labels = gc_labels.loc[file_names]\n",
    "data.append({'study': study[:-5],\n",
    "             'disease': disease,\n",
    "             'data_set': f[:-4],\n",
    "             'features': features.iloc[:,9:-3].T.astype('float'),\n",
    "             'labels': labels,\n",
    "             'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1', 'isotopes', 'adduct', 'pcgroup']],\n",
    "             'samples': data[0]['samples']})\n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order of labels appears to match order of samples also doesnt look like there are any duplicates\n",
    "# 12-8-18: mapping datafiles accross the datasets!!!! \n",
    "\n",
    "# this has all ST000386, 369 and 386\n",
    "study = 'ST000385_data'\n",
    "disease = 'lung cancer - adenocarcinoma'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "# this is for ADC 1 and ADC 2 for both serum and plasma\n",
    "files = ['AN000603.txt', 'AN000620.txt'] #files starting with 14... is ADC 1 is 620\n",
    "# The AN files HAVE BOTH PLASMA AND SERUM IN THEM 602 ~ 620 \n",
    "# NOTE I THINK AN000602 is off by one! as is 621....wtf using the two others \n",
    "label_col = ['Factor3', 'Health State']\n",
    "plas_v_serum = ['Factor1', 'Organ']\n",
    "label_key = {'Healthy': 0, 'Adenocarcinoma': 1,'Adenocarcinoma ': 1,  'NA':'NA', 'Adenosquamous':1, 'Adenocarcnoma':1}\n",
    "df_m_adc1 = pd.read_csv('metadata_adc1.csv')\n",
    "df_m_adc1.set_index('Sample name', inplace=True)\n",
    "df_m_adc2 = pd.read_csv('metadata_adc2.csv')\n",
    "df_m_adc2.set_index('Sample name', inplace=True)\n",
    "final_file_names = []\n",
    "\n",
    "for f, col, col2 in zip(files,label_col,plas_v_serum):\n",
    "    features, labels, samples, metabolites = read_metadata(f, col, label_key, debug=False)\n",
    "    #get labels to be in correct order and split into plasma and serum seperate\n",
    "    plasma_files = list(samples.index[samples[col2] == 'Plasma ']) #note the extra space here\n",
    "    serum_files = list(samples.index[samples[col2] == 'Serum '])\n",
    "    serum_features = features.loc[serum_files]\n",
    "    plasma_features = features.loc[plasma_files]\n",
    "    labels_s = labels.loc[serum_files]\n",
    "    labels_p = labels.loc[plasma_files]\n",
    "    #get a mask for where the non-QC / pool files are - plasma first:\n",
    "    na_t_f_samples_p = labels_p !='NA' \n",
    "    na_samples_p = labels_p[na_t_f_samples_p[col] == False].index\n",
    "    labels_p = labels_p[na_t_f_samples_p[col] == True]\n",
    "    plasma_features = plasma_features[na_t_f_samples_p[col]==True]\n",
    "    p_files = list(plasma_features.index)\n",
    "    # now for the serum samples\n",
    "    na_t_f_samples_s = labels_s !='NA' \n",
    "    na_samples_s = labels_s[na_t_f_samples_s[col] == False].index\n",
    "    labels_s = labels_s[na_t_f_samples_s[col] == True]\n",
    "    serum_features = serum_features[na_t_f_samples_s[col]==True]\n",
    "    s_files = list(serum_features.index)\n",
    "    #### MAPPING SAMPLES ACROSS DATASETS:\n",
    "    samples_s_serum = samples.loc[s_files]['SUBJECT(optional)'].values\n",
    "    samples_patient_serum = [s[:-6] for s in samples_s_serum]\n",
    "    samples_label_serum = list(samples.loc[s_files].index)\n",
    "    samples_s_plasma = samples.loc[p_files]['SUBJECT(optional)'].values\n",
    "    samples_patient_plasma = [s[:-7] for s in samples_s_plasma]\n",
    "    samples_label_plasma = list(samples.loc[p_files].index)\n",
    "    remove_p = []\n",
    "    remove_s = []\n",
    "    for p in samples_patient_plasma:\n",
    "        if p not in samples_patient_serum:\n",
    "            remove_p.append(p)\n",
    "    for s in samples_patient_serum:\n",
    "        if s not in samples_patient_plasma:\n",
    "            remove_s.append(s)\n",
    "    for lab, pat in zip(samples_label_serum, samples_patient_serum):\n",
    "        if pat in remove_s:\n",
    "            samples_label_serum.remove(lab)\n",
    "    for lab, pat in zip(samples_label_plasma, samples_patient_plasma):\n",
    "        if pat in remove_p:\n",
    "            samples_label_plasma.remove(lab)\n",
    "    final_file_names.append(samples_label_plasma)\n",
    "    final_file_names.append(samples_label_serum)\n",
    "    \n",
    "    plasma_features = plasma_features.loc[samples_label_plasma]\n",
    "    serum_features = serum_features.loc[samples_label_serum]\n",
    "    labels_p = labels_p.loc[samples_label_plasma]\n",
    "    labels_s = labels_s.loc[samples_label_serum]\n",
    "    samples_p = samples.loc[samples_label_plasma]\n",
    "    samples_s = samples.loc[samples_label_serum]\n",
    "    \n",
    "    # metabolites are time_mz seperated, when there are 2+, after the 1st it starts with + or _\n",
    "    #features are already samples X features    \n",
    "#     print(labels_p, labels_s)\n",
    "    data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': f[:-4]+'_plasma',\n",
    "                 'features': plasma_features,\n",
    "                 'labels': labels_p,\n",
    "                 'peaks': metabolites,\n",
    "                 'samples': samples_p})\n",
    "    data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': f[:-4]+'_serum',\n",
    "                 'features': serum_features,\n",
    "                 'labels': labels_s,\n",
    "                 'peaks': metabolites,\n",
    "                 'samples': samples_s})\n",
    "    \n",
    "dirs = ['adc2_plasma', 'adc2_serum', 'adc1_plasma', 'adc1_serum']\n",
    "data_temp = []\n",
    "mapping_labels_across_ds = []\n",
    "for folder, f_names in zip(dirs,final_file_names):\n",
    "    df = df_m_adc1 if 'adc1' in folder else df_m_adc2\n",
    "    data_file = 'IPO_aligned_ST000385_' + folder + '.csv'\n",
    "    open_file = os.path.join(folder,data_file)\n",
    "    features = pd.read_csv(open_file)\n",
    "    feat = features.iloc[:,9:-3].T.astype('float')\n",
    "    ff_names_indata = ['X'+f+'.mzData' for f in f_names] # use this to index the labels, features and samples....\n",
    "    feat = feat.loc[ff_names_indata]\n",
    "    labels = df.loc[f_names]['Health State']\n",
    "    label = {'Healthy':0, 'Healthy ':0, 'Adenocarcinoma':1,'Adenocarcinoma ': 1, 'Adenosquamous':1,'Adenosquamous ':1, 'Adenocarcnoma':1}\n",
    "    labels = labels.replace(label)\n",
    "    data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': data_file[:-4],\n",
    "                 'features': feat,\n",
    "                 'labels': labels,\n",
    "                 'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                 'samples': df.loc[f_names]})\n",
    "\n",
    "files = ['IPO_aligned_ST000385_onebatch_plasma.csv', 'IPO_aligned_ST000385_onebatch_serum.csv']\n",
    "combined = pd.concat([df_m_adc1,df_m_adc2])\n",
    "\n",
    "lists = []\n",
    "to_delete = ['X130729dlvsa21_1.mzData', 'X130729dlvsa50_2.mzData', 'X130730dlvsa15_1.mzData',\n",
    "             'X130730dlvsa32_1.mzData', 'X130730dlvsa37_3.mzData']\n",
    "for fi in files:\n",
    "    features = pd.read_csv(fi)\n",
    "    feat = features.iloc[:,9:-3].T.astype('float')\n",
    "    feat_names = list(feat.index)\n",
    "    for ele in to_delete:\n",
    "        try:\n",
    "            feat_names.remove(ele)\n",
    "        except:\n",
    "            pass\n",
    "    feat = feat.loc[feat_names]\n",
    "    ff_names_indata = [f[1:-7] for f in feat_names] \n",
    "    lists.append(ff_names_indata)\n",
    "    labels = combined.loc[ff_names_indata]['Health State']\n",
    "    label = {'Healthy':0, 'Healthy ':0, 'Adenocarcinoma':1,'Adenocarcinoma ': 1, 'Adenosquamous':1,'Adenosquamous ':1, 'Adenocarcnoma':1}\n",
    "    labels = labels.replace(label)\n",
    "    data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': fi[:-4],\n",
    "                 'features': feat,\n",
    "                 'labels': labels,\n",
    "                 'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                 'samples': combined.loc[ff_names_indata]})\n",
    "# looking through what needs to be removed in order to combine datasets....\n",
    "# 1st: 130729dlvsa21_1, 130729dlvsa50_2, 130730dlvsa32_1, 130730dlvsa37_3\n",
    "# 2nd: 130730dlvsa15_1\n",
    "\n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 samples even though the study AN file says 31\n",
    "# no duplicate files and file order seems correct\n",
    "# 12-7-18: I think you can go ahead and combine datasets\n",
    "study = 'ST000329_data'\n",
    "disease = 'minimal change disease, focal segmental sclerosis'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "\n",
    "files = ['AN000525.txt', 'AN000526.txt']\n",
    "label_col = 'Sample type'\n",
    "label_key = {'MCD': 1, 'FSGS': 2, 'Control': 0}\n",
    "df_s = pd.read_csv('metadata.csv')\n",
    "df_s.set_index('Sample name', inplace=True)\n",
    "\n",
    "for f in files:\n",
    "    features, labels, samples, metabolites = read_metadata(f, label_col, label_key, debug=False)\n",
    "    # metabolites are time_mz seperated, when there are 2+, after the 1st it starts with + or _\n",
    "    #features are already samples X features \n",
    "    if multi_to_single:\n",
    "        labels_new, ds_names = reduce_multi(labels, f[:-4], label_key)\n",
    "        for l, n in zip(labels_new, ds_names):\n",
    "            data.append({'study': study[:-5],\n",
    "                         'disease': disease,\n",
    "                         'data_set': n,\n",
    "                         'features': features,\n",
    "                         'labels': l,\n",
    "                         'peaks': metabolites,\n",
    "                         'samples': samples}) \n",
    "    elif one_to_one:\n",
    "        feat_list, labels_list, ds_names = reduce_to_one_v_one(features, labels, f[:-4], label_key)\n",
    "        for f, l, n in zip(feat_list, labels_list, ds_names):\n",
    "            f_names = list(f.index)\n",
    "            f_names = [int(ele) for ele in f_names]\n",
    "            s = samples.loc[f_names]\n",
    "            data.append({'study': study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': n,\n",
    "                     'features': f,\n",
    "                     'labels': l,\n",
    "                     'peaks': metabolites,\n",
    "                     'samples': s})\n",
    "    else:\n",
    "        data.append({'study': study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': f[:-4],\n",
    "                     'features': features,\n",
    "                     'labels': labels,\n",
    "                     'peaks': metabolites,\n",
    "                     'samples': samples})\n",
    "\n",
    "dirs = ['pos', 'neg']\n",
    "for folder in dirs:\n",
    "    data_file = 'IPO_aligned_ST000329_' + folder + '.csv'\n",
    "    open_file = os.path.join(folder,data_file)\n",
    "    features = pd.read_csv(open_file)\n",
    "    if folder == 'pos':\n",
    "        file_names = list(features.iloc[:,9:-3])\n",
    "        feat = features.iloc[:,9:-3].T.astype('float')\n",
    "        p = features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1', 'isotopes', 'adduct', 'pcgroup']]\n",
    "    else:\n",
    "        file_names = list(features.iloc[:,9:])\n",
    "        feat = features.iloc[:,9:].T.astype('float')  \n",
    "        p = features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1']]\n",
    "    file_names = [f[:-5]+'.d' for f in file_names]\n",
    "    labels = df_s.loc[file_names]\n",
    "    labels = labels['Treatment'].replace(label_key)\n",
    "    rename_labels = dict(zip(list(labels.index),list(feat.index)))\n",
    "    labels = labels.rename(rename_labels)\n",
    "    if multi_to_single:\n",
    "        labels_new, ds_names = reduce_multi(labels, data_file[:-4], label_key)\n",
    "        for l, n in zip(labels_new, ds_names):\n",
    "            data.append({'study': study[:-5],\n",
    "                         'disease': disease,\n",
    "                         'data_set': n,\n",
    "                         'features': feat,\n",
    "                         'labels': l,\n",
    "                         'peaks': p,\n",
    "                         'samples': df_s})\n",
    "    elif one_to_one:\n",
    "        feat_list, labels_list, ds_names = reduce_to_one_v_one(feat, labels, data_file[:-4], label_key)\n",
    "        for f, l, n in zip(feat_list, labels_list, ds_names):\n",
    "            f_names = list(f.index)\n",
    "            f_names = [ele[:-4]+'d' for ele in f_names]\n",
    "#             f_names = [int(ele) for ele in f_names]\n",
    "            s = df_s.loc[f_names]\n",
    "            data.append({'study': study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': n,\n",
    "                     'features': f,\n",
    "                     'labels': l,\n",
    "                     'peaks': p,\n",
    "                     'samples': s})\n",
    "    else:\n",
    "        data.append({'study': study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': data_file[:-4],\n",
    "                     'features': feat,\n",
    "                     'labels': labels,\n",
    "                     'peaks': p,\n",
    "                     'samples': df_s})\n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = 'ST000865_data'\n",
    "disease = 'Hepatocellular Carcinoma'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "\n",
    "f = 'AN001390.txt'\n",
    "label_col = 'Patient group'\n",
    "label_key = {'CIRR': 0, 'HCC': 1, '-':np.nan, 'Pool HCC': np.nan, 'Pool CIRR':np.nan, 'POSSIBLE CASE':np.nan}\n",
    "\n",
    "features, labels, samples, metabolites = read_metadata(f, label_col, label_key, debug=False)\n",
    "labels = labels.loc[list(features.index)]\n",
    "good_files = list(labels.index)\n",
    "data.append({'study': study[:-5],\n",
    "             'disease': disease,\n",
    "             'data_set': f[:-4]+'all_author',\n",
    "             'features': features,\n",
    "             'labels': labels,\n",
    "             'peaks': metabolites,\n",
    "             'samples': samples})\n",
    "\n",
    "folders = ['batch2_raw', 'batch3_raw', 'onebatch']\n",
    "for folder in folders:\n",
    "    data_file = 'IPO_aligned_ST000865_' + folder + '.csv'\n",
    "    open_file = os.path.join(folder,data_file)\n",
    "    features = pd.read_csv(open_file)\n",
    "    file_names = list(features.iloc[:,9:-3])\n",
    "    file_names = [name[1:-7] for name in file_names if name[1:-7] in good_files]\n",
    "    \n",
    "    feat = features.iloc[:,9:-3].T.astype('float')\n",
    "    peaks = features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1', 'isotopes', 'adduct', 'pcgroup']]\n",
    "    label = labels.loc[file_names]\n",
    "    sample = samples.loc[file_names]\n",
    "    file_names = ['X'+name+'.mzData' for name in file_names]\n",
    "    feat = feat.loc[file_names]\n",
    "    data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': data_file[:-4],\n",
    "                 'features': feat,\n",
    "                 'labels': label,\n",
    "                 'peaks': peaks,\n",
    "                 'samples': sample})\n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 63 samples, however all have 3 replicates - removed all but first using pd df slicing every 3\n",
    "# order appears to match. \n",
    "# NOTE: NOT SURE WHAT THE 'ND' data is...I think its 'No Diabetes' and they jst got saline\n",
    "# 12-7-18: can probably go ahead and combine the datasets...\n",
    "study = 'ST000045_data'\n",
    "disease = 't1 diabetes'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "\n",
    "label_col = 'Treatment'\n",
    "# label_key = {'Saline Infusion': 0, 'Insulin Withdrawal': 1}\n",
    "modes = ['pos', 'neg', 'pos', 'neg']\n",
    "files = ['AN000072', 'AN000074', 'AN000073', 'AN000075'] #these match: 72 is 02feb, 74 = 11feb, 73 is 11mar, 75 had mar17\n",
    "# look like these files only use 2/3 of the samples....groupA isnt used (sammples 15-21)\n",
    "# took the data from the PMID file and will use this instead as the data\n",
    "files = ['a_2feb.csv', 'a_11feb.csv', 'a_11mar.csv', 'a_17mar.csv']\n",
    "# author features: get from PMID file, its a .xlsx so use read_excel, the first column is mz@rt \n",
    "df_s = pd.read_csv('s_data.csv')\n",
    "# get dataframe for easy file label look up. \n",
    "df_l = df_s[['Filename', 'Treatment']]\n",
    "df_l.set_index('Filename', inplace=True)\n",
    "df_l = df_l.replace({'ND': 0, 'II': 1, 'IW': 2})\n",
    "label_key = {'ND': 0, 'II': 1, 'IW': 2}\n",
    "df_s = df_s.set_index('Filename')\n",
    "\n",
    "replicate_keep = 'r001' #also can choose r002 or r003, HOWEVER then you will need to change iloc when getting the features\n",
    "\n",
    "for f in files:\n",
    "#     features, labels, samples, metabolites = read_metadata(fn, label_col, label_key)\n",
    "    features = pd.read_csv(f)\n",
    "    file_names = list(features.iloc[:,3:-8])\n",
    "    n_file_names = []\n",
    "    for f in file_names:\n",
    "        if f[-2:] == '_1':\n",
    "            n_file_names.append(f[:-2]+'.d')\n",
    "        else:\n",
    "            n_file_names.append(f+'.d')\n",
    "    n_file_names = [f for f in n_file_names if replicate_keep in f]\n",
    "    labels = df_l.loc[n_file_names]\n",
    "    feat = features.iloc[:,3:-8:3].T.astype('float')\n",
    "    feat_ind_replace = dict(zip(list(feat.index),list(labels.index)))\n",
    "    feat = feat.rename(feat_ind_replace)\n",
    "    if multi_to_single:\n",
    "        labels_new, ds_names = reduce_multi(labels, f[:-4], label_key)\n",
    "        for l, n in zip(labels_new, ds_names):\n",
    "            data.append({'study': study[:-5],\n",
    "                         'disease': disease,\n",
    "                         'data_set': n,\n",
    "                         'features': feat,\n",
    "                         'labels': l,\n",
    "                         'peaks': features[['Mass', 'Retention Time', 'Ionization mode']],\n",
    "                         'samples': df_s})\n",
    "    elif one_to_one:\n",
    "        feat_list, labels_list, ds_names = reduce_to_one_v_one(feat, labels, f[:-4], label_key)\n",
    "        for f, l, n in zip(feat_list, labels_list, ds_names):\n",
    "            f_names = list(f.index)\n",
    "            s = df_s.loc[f_names]\n",
    "            data.append({'study': study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': n,\n",
    "                     'features': f,\n",
    "                     'labels': l,\n",
    "                     'peaks': features[['Mass', 'Retention Time', 'Ionization mode']],\n",
    "                     'samples': s})\n",
    "    else:\n",
    "        data.append({'study': study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': f[:-4],\n",
    "                     'features': features.iloc[:,3:-8:3].T.astype('float'),\n",
    "                     'labels': labels,\n",
    "                     'peaks': features[['Mass', 'Retention Time', 'Ionization mode']],\n",
    "                     'samples': df_s})\n",
    "\n",
    "#get my data:\n",
    "dirs = ['2Feb', '11Feb', '11Mar', '17Mar']\n",
    "for folder, mode in zip(dirs, modes):\n",
    "    data_file = 'IPO_aligned_ST000045_' + folder.lower() + '_' + mode + '.csv'\n",
    "    open_file = os.path.join(folder,data_file)\n",
    "    features = pd.read_csv(open_file)\n",
    "    # 8:-3 - No X1 feature column, use npeaks instead\n",
    "    # also note, the file names in my csv files are:  X02Feb10.01.r001.mzML.1 so if matching to a label will need to slice\n",
    "    file_names = list(features)[8:-3]\n",
    "    file_names = [f[1:-7].replace('.','-')+'.d' for f in file_names]\n",
    "    file_names = [f.replace('r002', 'r001') for f in file_names if 'r002' in f]\n",
    "    # THIS IS NOT DONE YET, NEED TO MAKE SAMPLES TO LABELS! \n",
    "    feat = features.iloc[:,8:-3:3].T.astype('float')\n",
    "    labels = df_l.loc[file_names]\n",
    "    feat_ind_rename = dict(zip(list(feat.index),list(labels.index)))\n",
    "    feat = feat.rename(feat_ind_rename)\n",
    "    if multi_to_single:\n",
    "        labels_new, ds_names = reduce_multi(df_l.loc[file_names], data_file[:-4], label_key)\n",
    "        for l, n in zip(labels_new, ds_names):\n",
    "            data.append({'study': study[:-5],\n",
    "                         'disease': disease,\n",
    "                         'data_set': n,\n",
    "                         'features': features.iloc[:,8:-3:3].T.astype('float'),\n",
    "                         'labels': l,\n",
    "                         'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'npeaks', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                         'samples': df_s})\n",
    "    elif one_to_one:\n",
    "        feat_list, labels_list, ds_names = reduce_to_one_v_one(feat, labels, data_file[:-4], label_key)\n",
    "        for f, l, n in zip(feat_list, labels_list, ds_names):\n",
    "            f_names = list(f.index)\n",
    "            s = df_s.loc[f_names]\n",
    "            data.append({'study': study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': n,\n",
    "                     'features': f,\n",
    "                     'labels': l,\n",
    "                     'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'npeaks', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                     'samples': s})\n",
    "    else:\n",
    "        data.append({'study':study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': data_file[:-4],\n",
    "                     'features': features.iloc[:,8:-3:3].T.astype('float'),\n",
    "                     'labels': df_l.loc[file_names],\n",
    "                     'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'npeaks', 'isotopes', 'adduct', 'pcgroup']],\n",
    "                     'samples': df_s})\n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 108 samples in my data...authors mention ~113...? \n",
    "# no replicates, order looks good for my processed data\n",
    "# 12-7-18: looks like order is good, combine datasets\n",
    "study = 'MTBLS364_data' \n",
    "disease = 'smoker v. nonsmoker'\n",
    "\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "df_n_hil = pd.read_csv('a_mtbls364_HILIC_NEG_mass_spectrometry.txt',sep='\\t')\n",
    "df_p_hil = pd.read_csv('a_mtbls364_HILIC_POS_mass_spectrometry.txt',sep='\\t')\n",
    "df_n_lc = pd.read_csv('a_mtbls364_RP_NEG_mass_spectrometry.txt',sep='\\t')\n",
    "df_p_lc = pd.read_csv('a_mtbls364_RP_POS_mass_spectrometry.txt',sep='\\t')\n",
    "df_s = pd.read_csv('s_BoEfRTP2_Serum.txt', sep='\\t')\n",
    "df_n_hil = df_s.merge(df_n_hil, on='Sample Name').set_index('Sample Name')\n",
    "df_p_hil = df_s.merge(df_p_hil, on='Sample Name').set_index('Sample Name')\n",
    "df_n_lc = df_s.merge(df_n_lc, on='Sample Name').set_index('Sample Name')\n",
    "df_p_lc = df_s.merge(df_p_lc, on='Sample Name').set_index('Sample Name')\n",
    "dfs = [df_n_hil, df_p_hil, df_n_lc, df_p_lc]\n",
    "\n",
    "# label: Factor Value[smoking status]\n",
    "# get author files:\n",
    "# files = ['m_mtbls364_HILIC_NEG_mass_spectrometry_v2_maf.tsv', 'm_mtbls364_HILIC_POS_mass_spectrometry_v2_maf.tsv',\n",
    "#          'm_mtbls364_RP_NEG_mass_spectrometry_v2_maf.tsv', 'm_mtbls364_RP_POS_mass_spectrometry_v2_maf.tsv']\n",
    "##### TURNS OUT NO AUTHOR DATA AGAIN....\n",
    "# for f, df in zip(files, dfs):\n",
    "#     labels = get_labels(df, 'Raw Spectral Data File', 'Factor Value[smoking status]')\n",
    "#     features = pd.read_csv(f, sep='\\t')\n",
    "#     sample_names = list(features.iloc[:,21:])\n",
    "#     if ext == '.cdf':\n",
    "#         sample_names = [s.zfill(3)+'_1'+ext for s in sample_names]\n",
    "#     else:\n",
    "#         sample_names = [s+ext for s in sample_names]\n",
    "#     df_l = labels.loc[sample_names]\n",
    "#     data.append({'study':study[:-5],\n",
    "#                  'disease': disease,\n",
    "#                  'data_set': f[:-4],\n",
    "#                  'features': features.iloc[:,21:].T.astype('float'),\n",
    "#                  'labels': df_l,\n",
    "#                  'peaks': features[['mass_to_charge', 'retention_time']],\n",
    "#                  'samples': df})\n",
    "\n",
    "# get my data:\n",
    "lab_to_int = {'Smoker':1, 'Never Smoker': 0}\n",
    "dirs = ['hil_neg', 'hil_pos', 'lip_neg', 'lip_pos']\n",
    "for folder, df in zip(dirs,dfs):\n",
    "    labels = get_labels(df, 'Raw Spectral Data File', 'Factor Value[smoking status]')\n",
    "    data_file = 'IPO_aligned_MTBLS364_' + folder + '.csv'\n",
    "    open_file = os.path.join(folder,data_file)\n",
    "    features = pd.read_csv(open_file)\n",
    "    file_names = list(features.iloc[:,9:-3])\n",
    "    file_names = [f[:-5]+'.raw' for f in file_names]\n",
    "    df_l = labels.loc[file_names]\n",
    "    df_l = df_l.replace(lab_to_int)\n",
    "    data.append({'study':study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': data_file[:-4],\n",
    "                 'features': features.iloc[:,9:-3].T.astype('float'),\n",
    "                 'labels': df_l,\n",
    "                 'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1' ,'isotopes', 'adduct', 'pcgroup']],\n",
    "                 'samples': df})\n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 239 people in study, no replicates\n",
    "# 12-7-18: looks like order matches between ds, can combine datasets!\n",
    "study = 'MTBLS354_data'\n",
    "disease = 'Pneumonia - Community acquired'\n",
    "\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "df_s = pd.read_csv('s_CAP.txt', sep='\\t').set_index('Sample Name')\n",
    "\n",
    "files = ['m_cap_metabolite_profiling_mass_spectrometry_v2_maf.tsv',\n",
    "         'm_cap_metabolite_profiling_mass_spectrometry-1_v2_maf.tsv']\n",
    "lab_to_int = {'non-community acquired pneumonia':0, 'community acquired pneumonia':1}\n",
    "for f in files:\n",
    "    features = pd.read_csv(f, sep='\\t')\n",
    "    file_names = list(features.iloc[:,21:].T.index)\n",
    "    labels = df_s['Factor Value[disease state]'].loc[file_names]\n",
    "    labels = labels.replace(lab_to_int)\n",
    "    data.append({'study':study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': f[:-4],\n",
    "                 'features': features.iloc[:,21:].T.astype('float'),\n",
    "                 'labels': labels,\n",
    "                 'peaks': features[['mass_to_charge', 'retention_time']],\n",
    "                 'samples': df_s.loc[file_names]})\n",
    "#get my data:\n",
    "folders = ['neg', 'pos']\n",
    "for folder in folders:\n",
    "    data_file = 'IPO_aligned_MTBLS354_' + folder + '.csv'\n",
    "    open_file = os.path.join(folder,data_file)\n",
    "    features = pd.read_csv(open_file)\n",
    "    file_names = list(features.iloc[:,9:-3])\n",
    "    file_names = [f[:-9].replace('.','-') for f in file_names]\n",
    "    labels = df_s['Factor Value[disease state]'].loc[file_names]\n",
    "    labels = labels.replace(lab_to_int)\n",
    "    data.append({'study':study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': data_file[:-4],\n",
    "                 'features': features.iloc[:,9:-3].T.astype('float'),\n",
    "                 'labels': labels,\n",
    "                 'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1' ,'isotopes', 'adduct', 'pcgroup']],\n",
    "                 'samples': df_s.loc[file_names]})  \n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2723: DtypeWarning: Columns (10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# NEED TO UPDATE WITH THE NEWER FILE AND THE CDF WHEN IT RUNS.....\n",
    "# order of labels maps to the orders of features, no duplicates.\n",
    "# 12-8-18: looks like you can now combine datasets! \n",
    "study = 'MTBLS315_data' \n",
    "disease = 'non-malaria febrile illness'\n",
    "\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "df_n_uplc = pd.read_csv('a_UPLC_NEG_nmfi_and_bsi_diagnosis.txt',sep='\\t')\n",
    "df_p_uplc = pd.read_csv('a_UPLC_POS_nmfi_and_bsi_diagnosis.txt',sep='\\t')\n",
    "df_p_gc = pd.read_csv('a_GC_nmfi_and_bsi_diagnosis.txt',sep='\\t')\n",
    "df_p_lc = pd.read_csv('a_LC_nmfi_and_bsi_diagnosis.txt',sep='\\t')\n",
    "df_s = pd.read_csv('s_NMFI and BSI diagnosis.txt', sep='\\t')\n",
    "df_n_uplc = df_s.merge(df_n_uplc, on='Sample Name').set_index('Sample Name')\n",
    "df_p_uplc = df_s.merge(df_p_uplc, on='Sample Name').set_index('Sample Name')\n",
    "df_p_gc = df_s.merge(df_p_gc, on='Sample Name').set_index('Sample Name')\n",
    "df_p_lc = df_s.merge(df_p_lc, on='Sample Name').set_index('Sample Name')\n",
    "dfs = [df_p_gc, df_p_lc, df_n_uplc, df_p_uplc] #note this order needs to make the order of the 'files' or 'dirs' below for zip to work\n",
    "extensions = ['.cdf', '.mzXML', '.mzML', '.mzML'] # same here!\n",
    "names = ['GC', 'LC', 'UPLC_N', 'UPLC_P']\n",
    "# label: Factor Value[patient group]\n",
    "to_be_df = {}\n",
    "for df,name in zip(dfs,names):\n",
    "    to_be_df[name] = df['Raw Spectral Data File']\n",
    "df_mapping_lab_sample = pd.DataFrame(to_be_df) # use this to get all the names consistent\n",
    "\n",
    "files = ['m_GC_nmfi_and_bsi_diagnosis_v2_maf.tsv', 'm_LC_nmfi_and_bsi_diagnosis_v2_maf.tsv',\n",
    "         'm_UPLC_NEG_nmfi_and_bsi_diagnosis_v2_maf.tsv', 'm_UPLC_POS_nmfi_and_bsi_diagnosis_v2_maf.tsv']\n",
    "lab_to_int = {'malaria': 1, 'non-malarial febrile illness':0, 'bacterial bloodstream infection':0} # NOTE THIS DROPPING ON A LABEL\n",
    "for f, df, ext, name in zip(files,dfs, extensions, names):\n",
    "    features = pd.read_csv(f, sep='\\t')\n",
    "    f_names = df_mapping_lab_sample[name].values\n",
    "    if name == 'GC':\n",
    "        f_names_feat = [str(int(fi[:-6])) for fi in f_names]\n",
    "        f_names_lab = f_names\n",
    "    elif name == 'LC':\n",
    "        f_names_feat = [fi[:-6] for fi in f_names]\n",
    "        f_names_lab = f_names\n",
    "    else:\n",
    "        f_names_feat = [fi[:-5] for fi in f_names]\n",
    "        f_names_lab = f_names\n",
    "    labels = get_labels(df, 'Raw Spectral Data File', 'Factor Value[patient group]')\n",
    "    df_l = labels.loc[f_names_lab]\n",
    "    df_l = df_l.replace(lab_to_int)\n",
    "    feat = features.iloc[:,21:].T.astype('float')\n",
    "    feat = feat.loc[f_names_feat]\n",
    "#     sample_names = list(features.iloc[:,21:])\n",
    "#     if ext == '.cdf':\n",
    "#         sample_names = [s.zfill(3)+'_1'+ext for s in sample_names]\n",
    "#     else:\n",
    "#         sample_names = [s+ext for s in sample_names]\n",
    "    data.append({'study':study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': f[:-4],\n",
    "                 'features': feat,\n",
    "                 'labels': df_l,\n",
    "                 'peaks': features[['mass_to_charge', 'retention_time']],\n",
    "                 'samples': df})\n",
    "# will need to change this! \n",
    "dirs = ['mzData', 'mzXML', 'n_mzML', 'p_mzML']\n",
    "for folder, df, ext, name in zip(dirs,dfs,extensions, names):\n",
    "    labels = get_labels(df, 'Raw Spectral Data File', 'Factor Value[patient group]')\n",
    "    data_file = 'IPO_aligned_MTBLS315_' + folder + '.csv'\n",
    "    open_file = os.path.join(folder,data_file)\n",
    "    features = pd.read_csv(open_file)\n",
    "    feat = features.iloc[:,9:-3].T.astype('float')\n",
    "    \n",
    "    f_names = df_mapping_lab_sample[name].values\n",
    "    if name == 'GC':\n",
    "        f_names_feat = ['X'+f[:-3]+'mzData' for f in f_names]\n",
    "        f_names_lab = f_names\n",
    "    else:\n",
    "        f_names_feat = ['X'+f.replace('-','.') for f in f_names]\n",
    "        f_names_lab = f_names\n",
    "        \n",
    "#     file_names = list(features.iloc[:,9:-3])\n",
    "#     if ext == '.cdf':\n",
    "#         file_names = [f[1:-7]+ext for f in file_names]\n",
    "#     elif ext == '.mzXML':\n",
    "#         file_names = [f[1:].replace('.','-')[:-6]+ext for f in file_names]\n",
    "#     else:\n",
    "#         file_names = [f[1:] for f in file_names]\n",
    "    df_l = labels.loc[f_names_lab]\n",
    "    df_l = df_l.replace(lab_to_int)\n",
    "    feat = feat.loc[f_names_feat]\n",
    "    data.append({'study':study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': data_file[:-4],\n",
    "                 'features': feat,\n",
    "                 'labels': df_l,\n",
    "                 'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1' ,'isotopes', 'adduct', 'pcgroup']],\n",
    "                 'samples': df})\n",
    "\n",
    "# files = ['XCMS-Report-annotated-SingleClass-CDF.xlsx', 'XCMS-Report-annotated-SingleClass-n_mzML.xlsx']\n",
    "# dfs = [df_p_gc, df_n_uplc]\n",
    "# extensions = ['.cdf', '.mzML']\n",
    "# first = True\n",
    "# for fi, df, ext in zip(files, dfs, extensions):\n",
    "#     labels = get_labels(df, 'Raw Spectral Data File', 'Factor Value[patient group]')\n",
    "#     features = pd.read_excel(fi)\n",
    "#     feat = features.iloc[:,10:-3].T.astype('float')\n",
    "#     file_names = list(features.iloc[:,10:-3])\n",
    "#     if ext == '.cdf':\n",
    "#         file_names = [f+ext for f in file_names]\n",
    "#     else:\n",
    "#         file_names = [f+ext for f in file_names]\n",
    "#     df_l = labels.loc[file_names]\n",
    "#     df_l = df_l.replace(lab_to_int)\n",
    "#     if first:\n",
    "#         file_to_sample = df['Raw Spectral Data File'].reset_index().set_index('Raw Spectral Data File')\n",
    "        \n",
    "    \n",
    "#     data.append({'study':study[:-5],\n",
    "#                  'disease': disease,\n",
    "#                  'data_set': fi[:-4],\n",
    "#                  'features': feat,\n",
    "#                  'labels': df_l,\n",
    "#                  'peaks': features[['mzmed', 'mzmin', 'mzmax', 'rtmed', 'rtmin', 'rtmax', 'npeaks' ,'isotopes', 'adduct', 'pcgroup']],\n",
    "#                  'samples': df})\n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no replicates, labels map to the data\n",
    "# 12-7-18: looks like you can combine datasets!\n",
    "study = 'MTBLS266_data'\n",
    "disease = 'Age related metabolomics'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "df_n = pd.read_csv('a_mtbls266_NEG_mass_spectrometry.txt',sep='\\t')\n",
    "df_p = pd.read_csv('a_mtbls266_POS_mass_spectrometry.txt',sep='\\t')\n",
    "df_s = pd.read_csv('s_MTBLS266.txt', sep='\\t')\n",
    "df_n = df_s.merge(df_n, on='Sample Name').set_index('Raw Spectral Data File')\n",
    "df_p = df_s.merge(df_p, on='Sample Name').set_index('Raw Spectral Data File')\n",
    "dfs = [df_n, df_p]\n",
    "\n",
    "# get author data:\n",
    "files =  ['m_mtbls266_NEG_mass_spectrometry_v2_maf.tsv', 'm_mtbls266_POS_mass_spectrometry_v2_maf.tsv']\n",
    "for f, df in zip(files,dfs):\n",
    "    features = pd.read_csv(f, sep='\\t')\n",
    "    feat = features.iloc[:,21:].T.astype('float')\n",
    "    sample_names = list(features.iloc[:,21:])\n",
    "    sample_names = [name+'.mzML' for name in sample_names]\n",
    "    labels = df['Factor Value[Age group]'].loc[sample_names]\n",
    "    to_replace = {'youth':0, 'elder':1}\n",
    "    labels = labels.replace(to_replace)\n",
    "    data.append({'study':study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': f[:-4],\n",
    "                 'features': feat,\n",
    "                 'labels': labels,\n",
    "                 'peaks': features[['mass_to_charge', 'retention_time']],\n",
    "                 'samples': df.loc[sample_names]})\n",
    "# get my data:\n",
    "dirs = ['MTBLS266_neg', 'MTBLS266_pos']\n",
    "for folder, df in zip(dirs,dfs):\n",
    "    data_file = 'IPO_aligned_' + folder + '.csv'\n",
    "    open_file = os.path.join(folder,data_file)\n",
    "    features = pd.read_csv(open_file)\n",
    "    feat = features.iloc[:,9:-3].T.astype('float')\n",
    "    file_names = list(features.iloc[:,9:-3])\n",
    "    labels = df['Factor Value[Age group]'].loc[file_names]\n",
    "    to_replace = {'youth':0, 'elder':1}\n",
    "    labels = labels.replace(to_replace)\n",
    "    data.append({'study':study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': data_file[:-4],\n",
    "                 'features': feat,\n",
    "                 'labels': labels,\n",
    "                 'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1' ,'isotopes', 'adduct', 'pcgroup']],\n",
    "                 'samples': df.loc[file_names]})\n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no replicates, labels map to the data\n",
    "# not really a classification problem - its a 24 hr time course\n",
    "# 12-7-18: looks like you can combine datasets!\n",
    "study = 'MTBLS264_data'\n",
    "disease = 'Age related metabolomics'\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "df_n = pd.read_csv('a_mtbls264_NEG_mass_spectrometry.txt',sep='\\t')\n",
    "df_p = pd.read_csv('a_mtbls264_POS_mass_spectrometry.txt',sep='\\t')\n",
    "df_s = pd.read_csv('s_MTBLS264.txt', sep='\\t')\n",
    "df_n = df_s.merge(df_n, on='Sample Name').set_index('Raw Spectral Data File')\n",
    "df_p = df_s.merge(df_p, on='Sample Name').set_index('Raw Spectral Data File')\n",
    "dfs = [df_n, df_p]\n",
    "\n",
    "# package author data:\n",
    "files = ['m_mtbls264_NEG_mass_spectrometry_v2_maf.tsv', 'm_mtbls264_POS_mass_spectrometry_v2_maf.tsv']\n",
    "for f, df in zip(files,dfs):\n",
    "    features = pd.read_csv(f, sep='\\t')\n",
    "    sample_names = list(features.iloc[:,21:])\n",
    "    for samp_type in ['blood', 'RBC', 'plasma']:\n",
    "        names = [ele for ele in sample_names if samp_type in ele]\n",
    "        feat = features.iloc[:,21:].T.astype('float').loc[names]\n",
    "        names = [ele+'.mzML' for ele in names]\n",
    "        labels = df['Factor Value[timepoint]'].loc[names]\n",
    "        data.append({'study':study[:-5],\n",
    "                     'disease': disease,\n",
    "                     'data_set': f[:-4]+'_'+samp_type,\n",
    "                     'features': feat,\n",
    "                     'labels': labels,\n",
    "                     'peaks': features[['mass_to_charge', 'retention_time']],\n",
    "                     'samples': df.loc[names]})\n",
    "# get my data:\n",
    "dirs = ['MTBLS264_blood_neg', 'MTBLS264_blood_pos', 'MTBLS264_plasma_neg', \n",
    "        'MTBLS264_plasma_pos', 'MTBLS264_rbc_neg', 'MTBLS264_rbc_pos']\n",
    "dfs = [df_n, df_p, df_n, df_p, df_n, df_p]\n",
    "for folder, df in zip(dirs,dfs):\n",
    "    data_file = 'IPO_aligned_' + folder + '.csv'\n",
    "    open_file = os.path.join(folder,data_file)\n",
    "    features = pd.read_csv(open_file)\n",
    "    file_names = list(features.iloc[:,9:-3])\n",
    "    labels = df['Factor Value[timepoint]'].loc[file_names]\n",
    "    data.append({'study':study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': data_file[:-4],\n",
    "                 'features': features.iloc[:,9:-3].T.astype('float'),\n",
    "                 'labels': labels,\n",
    "                 'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1' ,'isotopes', 'adduct', 'pcgroup']],\n",
    "                 'samples': df.loc[file_names]})\n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the author data has no features present\n",
    "# the labels match with the files and there are not replicates \n",
    "# 12-7-18: looks like you can combine datasets\n",
    "study = 'MTBLS105_data'\n",
    "disease = 'Hepatocellular carcinoma'\n",
    "\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "df_s = pd.read_csv('s_mtbls105.txt', sep='\\t').set_index('Sample Name')\n",
    "\n",
    "# get author data: TURNS OUT ITS EMPTY\n",
    "# files = ['m_mtbls105_GC_Q_mass_spectrometry_v2_maf.tsv', \n",
    "#          'm_mtbls105_GC_SIM_mass_spectrometry_v2_maf.tsv',\n",
    "#          'm_mtbls105_GC_TOF_mass_spectrometry_v2_maf.tsv']\n",
    "# for f in files:\n",
    "#     features = pd.read_csv(f, sep='\\t')\n",
    "#     file_names = list(features.iloc[:,21:].T.index)\n",
    "#     labels = df_s['Factor Value[Disease]'].loc[file_names]\n",
    "#     print(labels.shape, features.iloc[:,21:].T.shape)\n",
    "#     data.append({'study':study[:-5],\n",
    "#                  'disease': disease,\n",
    "#                  'data_set': f[:-4],\n",
    "#                  'features': features.iloc[:,21:].T.astype('float'),\n",
    "#                  'labels': labels,\n",
    "#                  'peaks': features[['mass_to_charge', 'retention_time']],\n",
    "#                  'samples': df_s.loc[file_names]})\n",
    "\n",
    "# Get my files\n",
    "lab_to_int = {'hepatocellular carcinoma': 1, 'cirrhosis of liver': 0}\n",
    "folders = ['qMS', 'SIM-MS'] #TOFMS didnt work for the IPO run\n",
    "for folder in folders:\n",
    "    data_file = 'IPO_aligned_MTBLS105_' + folder + '.csv'\n",
    "    open_file = os.path.join(folder,data_file)\n",
    "    features = pd.read_csv(open_file)\n",
    "    file_names = list(features.iloc[:,9:-3])\n",
    "    file_names = [f[:-7].replace('.','-') for f in file_names]\n",
    "    labels = df_s['Factor Value[Disease]'].loc[file_names]\n",
    "    labels = labels.replace(lab_to_int)\n",
    "    data.append({'study':study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': data_file[:-4],\n",
    "                 'features': features.iloc[:,9:-3].T.astype('float'),\n",
    "                 'labels': labels,\n",
    "                 'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1' ,'isotopes', 'adduct', 'pcgroup']],\n",
    "                 'samples': df_s.loc[file_names]})     \n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No replicates it appears and the labels map to the features order\n",
    "# removing the samples from the operation time and just using the baseline samples to try to predict the outcome. \n",
    "study = 'MTBLS92_data'\n",
    "disease = 'Breast Cancer'\n",
    "\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "df_s = pd.read_csv('s_BreastCancerLipidome.txt',sep='\\t')\n",
    "df_p = pd.read_csv('a_breastcancerlipidome_metabolite_profiling_mass_spectrometry.txt',sep='\\t')\n",
    "df_p = df_s.merge(df_p, on='Sample Name').set_index('Sample Name')\n",
    "\n",
    "#get my IPO-xcms data\n",
    "directory = 'MTBLS92_out'\n",
    "data_file = 'IPO_aligned_MTBLS92.csv'\n",
    "open_file = os.path.join(directory, data_file)\n",
    "features = pd.read_csv(open_file)\n",
    "\n",
    "# map the data names to labels\n",
    "matching = {'A':{}, 'B':{}}\n",
    "names_to_keep = []\n",
    "for ind, row in df_p.iterrows():\n",
    "    dataset = row['Source Name'][6]\n",
    "    #OLDmaking BL = 0 and OP = 1\n",
    "    # NEW: making this a prediction of who responds to the treatment and who does not\n",
    "    if row['Factor Value[Chemotherapy response]'] == 'no pCR':\n",
    "#     if row['Factor Value[Timepoint]'] == 'BL':\n",
    "        value = 0 \n",
    "        names_to_keep.append(row['Source Name'])\n",
    "    elif row['Factor Value[Chemotherapy response]'] == 'pCR':\n",
    "        value = 1\n",
    "        names_to_keep.append(row['Source Name'])\n",
    "    else:\n",
    "        continue \n",
    "    matching[dataset][row['Source Name'][8:]] = (value, row['Source Name'])\n",
    "names_to_keep = [ele[6:] for ele in names_to_keep]  \n",
    "df_p = df_p.loc[names_to_keep]\n",
    "\n",
    "# go through the names of the files that index the features and map to the name for the labels   \n",
    "data_names = list(features.iloc[:,9:-3].T.index)\n",
    "feat = features.iloc[:,9:-3].T.astype('float')\n",
    "info = []\n",
    "names = []\n",
    "for ele in data_names:\n",
    "    dataset = ele[1:4]\n",
    "    if dataset == '229':\n",
    "        dataset = 'B'\n",
    "    else: \n",
    "        dataset = 'A'\n",
    "    try:\n",
    "        row_data = matching[dataset][ele[9:-7]] \n",
    "        names.append(ele)\n",
    "        info.append(row_data)\n",
    "    except:\n",
    "        pass\n",
    "feat = feat.loc[names]\n",
    "headers = ['label','sample_name']\n",
    "# NOTE the following data from is not JUST the labels but also the sample name (A/B and number from the authors)\n",
    "df_l = pd.DataFrame.from_records(info, columns=headers, index=names)\n",
    "data.append({'study':study[:-5],\n",
    "             'disease': disease,\n",
    "             'data_set': data_file[:-4],\n",
    "             'features': feat,\n",
    "             'labels': df_l['label'],\n",
    "             'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1' ,'isotopes', 'adduct', 'pcgroup']],\n",
    "             'samples': df_p})\n",
    "\n",
    "\n",
    "# get author data:\n",
    "df_m = pd.read_csv('m_breastcancerlipidome_metabolite_profiling_mass_spectrometry_v2_maf.tsv',sep='\\t')\n",
    "data_names = list(df_m.iloc[:,21:].T.index)\n",
    "info = []\n",
    "to_keep = []\n",
    "for ele in data_names:\n",
    "    dataset = ele[0]\n",
    "    try:\n",
    "        row_data = matching[dataset][ele[2:]] \n",
    "        info.append(row_data)\n",
    "        to_keep.append(ele)\n",
    "    except:\n",
    "        pass\n",
    "headers = ['label','sample_name']\n",
    "# NOTE the following data from is not JUST the labels but also the sample name (A/B and number from the authors)\n",
    "df_l_a = pd.DataFrame.from_records(info, columns=headers, index=to_keep)\n",
    "feat = df_m.iloc[:,21:].T.astype('float').loc[to_keep]\n",
    "data.append({'study':study[:-5],\n",
    "             'disease': disease,\n",
    "             'data_set': 'Author_data',\n",
    "             'features': feat,\n",
    "             'labels': df_l_a['label'],\n",
    "             'peaks': df_m[['mass_to_charge', 'retention_time']],\n",
    "             'samples': df_p})\n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n",
      "127\n"
     ]
    }
   ],
   "source": [
    "# NOTE number of pos / neg do not match what my spreadsheet says and no mention of 'converters' in my data its just 0 or 1\n",
    "\n",
    "# looks like there is an A and B for each run \n",
    "# post removing the B files, looks like 127 samples in total\n",
    "# labels match with the feature order, 72 positive samples \n",
    "# 12-7-18: looks liek you can combine datasets!\n",
    "study = 'MTBLS72_data'\n",
    "disease = 'Alzheimers'\n",
    "#doesnt look like the authors even put up feature tables for the NEG and POS sample...only the targeted biocrates assay...\n",
    "# so previous analysis was only with their ~10 targets metabolites\n",
    "\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "\n",
    "### we dont get the same numbers for pos/neg data as expected from paper...made sure not due to incorrect processing\n",
    "### some samples may not have been uploaded\n",
    "\n",
    "df_s = pd.read_csv('s_Plasma_AD_Lipidomics.txt',sep='\\t')\n",
    "df_n = pd.read_csv('a_NEG_plasma_ad_lipidomics_mass_spectrometry.txt',sep='\\t')\n",
    "df_p = pd.read_csv('a_POS_plasma_ad_lipidomics_mass_spectrometry.txt',sep='\\t')\n",
    "df_p = df_s.merge(df_p, on='Sample Name').set_index('Sample Name')\n",
    "df_n = df_s.merge(df_n, on='Sample Name').set_index('Sample Name')\n",
    "##### or\n",
    "# df_n = pd.read_csv('a_NEG_plasma_ad_lipidomics_mass_spectrometry.txt',sep='\\t').set_index('Sample Name')\n",
    "# df_p = pd.read_csv('a_POS_plasma_ad_lipidomics_mass_spectrometry.txt',sep='\\t').set_index('Sample Name')\n",
    "# names_n = list(df_n.index)\n",
    "# names_p = list(df_p.index)\n",
    "# df_s_n = df_s.loc[names_n]\n",
    "# df_s_p = df_s.loc[names_p]\n",
    "# df_n = pd.concat([df_n, df_s_n], axis=1)\n",
    "# df_p = pd.concat([df_p, df_s_p], axis=1)\n",
    "\n",
    "dfs = [df_n, df_p]\n",
    "removed_dfs = []\n",
    "\n",
    "for df in dfs:\n",
    "    df = df.set_index('Source Name')\n",
    "    files_to_keep = [f for f in list(df.index) if 'B' not in f] # tossing all the B labeled files...\n",
    "    df = df.loc[files_to_keep]\n",
    "    removed_dfs.append(df)\n",
    "\n",
    "### get my data since really no author data except the KIT data and not untargeted metabs\n",
    "dirs = ['neg', 'pos']\n",
    "for folder, df in zip(dirs,removed_dfs):\n",
    "    n = list(df.index)\n",
    "    n = [ele.split('_')[-1][:-1] for ele in n]\n",
    "    n = len(set(n))\n",
    "    data_file = 'IPO_aligned_MTBLS72_' + folder + '.csv'\n",
    "    open_file = os.path.join(folder,data_file)\n",
    "    features = pd.read_csv(open_file)\n",
    "    \n",
    "    #get labels \n",
    "    mapping = {}\n",
    "    for ind, row in df_s.iterrows():\n",
    "        mapping[row['Sample Name']] = row['Factor Value[Cognitive Status]']\n",
    "    file_names = list(features.iloc[:,9:-3])\n",
    "    file_to_label = []\n",
    "    for f in file_names:\n",
    "        file_to_label.append(mapping[f[1:-5]])\n",
    "    header_to_label = {'label':file_to_label}\n",
    "    df_l = pd.DataFrame(header_to_label, index=file_names)\n",
    "    df_l[df_l['label'] == 'Normal Control'] = 0\n",
    "    df_l[df_l['label'] != 0] = 1\n",
    "    # no remove the duplicated file determined from above \n",
    "    derep_files = list(df['MS Assay Name'])\n",
    "    derep_files = ['X'+f+'.mzML' for f in derep_files]\n",
    "    f = features.iloc[:,9:-3].T.astype('float')\n",
    "    f = f.loc[derep_files]\n",
    "    labels = df_l['label']\n",
    "    labels = labels.loc[derep_files]\n",
    "    data.append({'study':study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': data_file[:-4],\n",
    "                 'features': f,\n",
    "                 'labels': labels,\n",
    "                 'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1' ,'isotopes', 'adduct', 'pcgroup']],\n",
    "                 'samples': df})\n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I dont think there are replicates - 1005 samples and the order matches\n",
    "# 12-7-18: looks like you can combine datasets!\n",
    "study = 'MTBLS28_data'\n",
    "disease = 'Lung cancer'\n",
    "\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "df_s = pd.read_csv('s_mtbls28.txt', sep='\\t').set_index('Sample Name')\n",
    "\n",
    "lab_to_int = {'Control':0,'Case':1}\n",
    "# get author data:\n",
    "files = ['m_mtbls28_NEG_v2_maf.tsv', 'm_mtbls28_POS_v2_maf.tsv']\n",
    "for f in files:\n",
    "    features = pd.read_csv(f, sep='\\t')\n",
    "    file_names = list(features.iloc[:,21:].T.index)\n",
    "    labels = df_s['Factor Value[Sample Type]'].loc[file_names]\n",
    "    labels = labels.replace(lab_to_int)\n",
    "    data.append({'study':study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': f[:-4],\n",
    "                 'features': features.iloc[:,21:].T.astype('float'),\n",
    "                 'labels': labels,\n",
    "                 'peaks': features[['mass_to_charge', 'retention_time']],\n",
    "                 'samples': df_s.loc[file_names]})\n",
    "\n",
    "# Get my IPO aligned files:\n",
    "folders = ['neg', 'pos']\n",
    "for folder in folders:\n",
    "    data_file = 'IPO_aligned_MTBLS28_' + folder + '.csv'\n",
    "    open_file = os.path.join(folder,data_file)\n",
    "    features = pd.read_csv(open_file)\n",
    "    file_names = list(features.iloc[:,9:-3])\n",
    "    file_names = [f[1:-7] for f in file_names]\n",
    "    labels = df_s['Factor Value[Sample Type]'].loc[file_names]\n",
    "    labels = labels.replace(lab_to_int)\n",
    "    data.append({'study':study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': data_file[:-4],\n",
    "                 'features': features.iloc[:,9:-3].T.astype('float'),\n",
    "                 'labels': labels,\n",
    "                 'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1' ,'isotopes', 'adduct', 'pcgroup']],\n",
    "                 'samples': df_s.loc[file_names]})  \n",
    "    \n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dereplicated the F and R samples. \n",
    "# labels appear to map to the samples features \n",
    "# 12-7-18: can combine the POS / NEG of expt 1F, expt1R, expt2F and expt2R...maybe combine the Expt1/2F or expt1/2R IF 1 and 2 are actually different\n",
    "study = 'MTBLS19_data'\n",
    "disease = 'hepatocellular carcinoma'\n",
    "\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "\n",
    "#read in the study data:\n",
    "df_s = pd.read_csv('s_MTBLS19.txt',sep='\\t')\n",
    "df_n = pd.read_csv('a_neg_MTBLS19_metabolite profiling_mass spectrometry.txt',sep='\\t')\n",
    "df_p = pd.read_csv('a_pos_MTBLS19_metabolite profiling_mass spectrometry.txt',sep='\\t')\n",
    "df_p = df_s.merge(df_p, on='Sample Name').set_index('Sample Name')\n",
    "df_n = df_s.merge(df_n, on='Sample Name').set_index('Sample Name')\n",
    "a_d_files = ['Exp1F_POS.xlsx', 'Exp1R_POS.xlsx', 'Exp2F_POS.xlsx', 'Exp2R_POS.xlsx',\n",
    "             'Exp1F_NEG.xlsx', 'Exp1R_NEG.xlsx', 'Exp2F_NEG.xlsx', 'Exp2R_NEG.xlsx']\n",
    "# note 1F and 1R are the same samples but with order of run in LCMS reversed...\n",
    "for f in a_d_files:\n",
    "    df_m = pd.read_excel(f)\n",
    "    data.append({'study': study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': f[:-5],\n",
    "                 'features': df_m.iloc[1:,2:].T.astype('float'),\n",
    "                 'labels': df_m.iloc[[0,],2:].T - 1,\n",
    "                 'peaks': df_m[['mz','rt']].iloc[1:],\n",
    "                 'samples': df_p if 'POS' in f else df_n})\n",
    "#These next files has identified metabolites but not the assocaited data:\n",
    "# df_m_n = pd.read_csv('m_neg_MTBLS19_metabolite profiling_mass spectrometry_v2_maf.tsv', sep='\\t')\n",
    "# df_m_p = pd.read_csv('m_pos_MTBLS19_metabolite profiling_mass spectrometry_v2_maf.tsv', sep='\\t')\n",
    "\n",
    "#Get IPO processed data:\n",
    "ipo_d_files = ['IPO_aligned_MTBLS19_neg_exp1.csv', 'IPO_aligned_MTBLS19_neg_exp2.csv',\n",
    "               'IPO_aligned_MTBLS19_pos_exp1.csv', 'IPO_aligned_MTBLS19_pos_exp2.csv']\n",
    "for f in ipo_d_files:\n",
    "    df_m = pd.read_csv(f)\n",
    "    runs = ['F', 'R']\n",
    "    for run in runs:\n",
    "        mask_str = 'Exp.'+run\n",
    "        mask = list(df_m.columns[df_m.columns.str.contains(mask_str).T])\n",
    "        masked_d = df_m[mask]\n",
    "        data.append({'study': study,\n",
    "                     'disease': disease,\n",
    "                     'data_set': f[:-4]+'_'+run,\n",
    "                     'features': masked_d.iloc[:,:].T.astype('float'),\n",
    "                     'labels': pd.DataFrame(masked_d.columns[:].str.contains('HCC'), index=masked_d.iloc[:,:].T.index),\n",
    "                     'peaks': df_m[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1', 'isotopes', 'adduct', 'pcgroup']].iloc[:],\n",
    "                     'samples': df_p if 'pos' in f else df_n})\n",
    "# get single batch datasets\n",
    "ipo_d_files = ['IPO_aligned_MTBLS19_neg_all_F.csv', 'IPO_aligned_MTBLS19_neg_all_R.csv',\n",
    "               'IPO_aligned_MTBLS19_pos_all_F.csv', 'IPO_aligned_MTBLS19_pos_all_R.csv']\n",
    "for f in ipo_d_files:\n",
    "    df_m = pd.read_csv(f)\n",
    "    features = df_m.iloc[:,9:-3]\n",
    "    data.append({'study': study,\n",
    "                 'disease': disease,\n",
    "                 'data_set': f[:-4]+'_'+run,\n",
    "                 'features': features.T.astype('float'),\n",
    "                 'labels': pd.DataFrame(features.columns[:].str.contains('HCC'), index=features.iloc[:,:].T.index),\n",
    "                 'peaks': df_m[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1', 'isotopes', 'adduct', 'pcgroup']].iloc[:],\n",
    "                 'samples': df_p if 'pos' in f else df_n})\n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPO_aligned_MTBLS17_neg_exp1\n",
      "IPO_aligned_MTBLS17_neg_exp2\n",
      "IPO_aligned_MTBLS17_neg_exp3\n",
      "IPO_aligned_MTBLS17_pos_exp1\n",
      "IPO_aligned_MTBLS17_pos_exp3\n",
      "IPO_aligned_MTBLS17_negall_12_bw\n",
      "IPO_aligned_MTBLS17_negall_15_ppm\n",
      "IPO_aligned_MTBLS17_negall_1e-1_mzwid\n",
      "IPO_aligned_MTBLS17_negall_1e-4_mzdiff\n",
      "IPO_aligned_MTBLS17_negall_2000_noise\n",
      "IPO_aligned_MTBLS17_negall_25e-3_mzwid\n",
      "IPO_aligned_MTBLS17_negall_3e-2_mzdiff\n",
      "IPO_aligned_MTBLS17_negall_3e-3_mzdiff\n",
      "IPO_aligned_MTBLS17_negall_3_10_peakwidth\n",
      "IPO_aligned_MTBLS17_negall_3_30_peakwidth\n",
      "IPO_aligned_MTBLS17_negall_3_90_peakwidth\n",
      "IPO_aligned_MTBLS17_negall_4_bw\n",
      "IPO_aligned_MTBLS17_negall_5e-1_profstep\n",
      "IPO_aligned_MTBLS17_negall_88e-2_bw\n",
      "IPO_aligned_MTBLS17_negall_guess-opt_-1e-4_mzdiff\n",
      "IPO_aligned_MTBLS17_negall_guess-opt_1e-3_mzdiff\n",
      "IPO_aligned_MTBLS17_neg_onebatch\n",
      "IPO_aligned_MTBLS17_pos_onebatch\n"
     ]
    }
   ],
   "source": [
    "# looks like there are a and b for each file - removed the b file\n",
    "# labels appear to map to the file order in the features \n",
    "# 12-7-18: can combine the pos/neg! \n",
    "study = 'MTBLS17_data'\n",
    "disease = 'hepatocellular carcinoma'\n",
    "\n",
    "os.chdir(os.path.join(root,study))\n",
    "data = []\n",
    "df_s = pd.read_csv('s_live_mtbls17.txt',sep='\\t')\n",
    "df_p = pd.read_csv('a_live_mtbls17pos_metabolite profiling_mass spectrometry.txt',sep='\\t')\n",
    "df_n = pd.read_csv('a_live_mtbls17neg_metabolite profiling_mass spectrometry.txt',sep='\\t')\n",
    "df_p = df_s.merge(df_p, on='Sample Name').set_index('Sample Name')\n",
    "df_n = df_s.merge(df_n, on='Sample Name').set_index('Sample Name')\n",
    "\n",
    "# get my IPO-xcms features\n",
    "dirs = ['neg_exp1', 'neg_exp2', 'neg_exp3', 'pos_exp1', 'pos_exp2', 'pos_exp3', 'neg_onebatch', 'pos_onebatch']\n",
    "# dirs = ['neg_onebatch', 'pos_onebatch']\n",
    "for folder in dirs:\n",
    "    dir_list = os.listdir(folder)\n",
    "    for fi in dir_list:\n",
    "        if 'IPO_aligned' in fi:\n",
    "            data_file = fi\n",
    "        else:\n",
    "            continue\n",
    "#         data_file = 'IPO_aligned_MTBLS17_' + folder + '.csv'\n",
    "        open_file = os.path.join(folder,data_file)\n",
    "        features = pd.read_csv(open_file)\n",
    "        if list(features)[-1] == 'pcgroup': #added this because pos_exp2 didnt get isotope/adduct picking done...\n",
    "            f = features.iloc[:,9:-3].T.astype('float')\n",
    "            label = pd.DataFrame(features.columns[9:-3].str.contains('HCC'), index=features.iloc[:,9:-3].T.index)\n",
    "            keep_index = [fname for fname in list(f.index) if 'a_' in fname]\n",
    "            f = f.loc[keep_index]\n",
    "            label = label.loc[keep_index]\n",
    "            data.append({'study':study[:-5],\n",
    "                         'disease': disease,\n",
    "                         'data_set': data_file[:-4],\n",
    "                         'features': f,\n",
    "                         'labels': label,\n",
    "                         'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1' ,'isotopes', 'adduct', 'pcgroup']],\n",
    "                         'samples': df_p if 'pos' in folder else df_n})\n",
    "        else:\n",
    "            f = features.iloc[:,9:].T.astype('float')\n",
    "            label = pd.DataFrame(features.columns[9:].str.contains('HCC'), index=features.iloc[:,9:].T.index)\n",
    "            keep_index = [fname for fname in list(f.index) if 'a_' in fname]\n",
    "            f = f.loc[keep_index]\n",
    "            label = label.loc[keep_index]\n",
    "            data.append({'study':study[:-5],\n",
    "                         'disease': disease,\n",
    "                         'data_set': data_file[:-4],\n",
    "                         'features': f,\n",
    "                         'labels': label,\n",
    "                         'peaks': features[['mz', 'mzmin', 'mzmax', 'rt', 'rtmin', 'rtmax', 'X1']],\n",
    "                         'samples': df_p if 'pos' in folder else df_n})       \n",
    "# get the author features\n",
    "files = ['Peaklist_EXP1_POS.xlsx','Peaklist_EXP2_POS.xlsx','Peaklist_EXP3_POS.xlsx',\n",
    "         'Peaklist_EXP1_NEG.xlsx','Peaklist_EXP2_NEG.xlsx','Peaklist_EXP3_NEG.xlsx']\n",
    "for f in files:\n",
    "    features = pd.read_excel(f)\n",
    "    feat = features.iloc[:,8:-2].T.astype('float')\n",
    "    label = pd.DataFrame(features.columns[8:-2].str.contains('HCC'), index=features.iloc[:,8:-2].T.index)\n",
    "    keep_index = [fname for fname in list(feat.index) if 'a_' in fname]\n",
    "    feat = feat.loc[keep_index]\n",
    "    label = label.loc[keep_index]\n",
    "    data.append({'study':study[:-5],\n",
    "                 'disease': disease,\n",
    "                 'data_set': f[:-5],\n",
    "                 'features': feat,\n",
    "                 'labels': label,\n",
    "                 'peaks': features[['mz','mono_mz','rt','FC','TT_pv','TT_qv','WX_pv','WX_qv','isotopes','adduct']],\n",
    "                 'samples': df_p if 'pos' in folder else df_n})\n",
    "pickle.dump(data, open('%s.pkl'%study, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
